{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ded82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading content:  Suga (rapper)\n",
      "loading content:  k-means clustering\n",
      "loading content:  Pakistan\n",
      "loading content:  k-means clustering\n",
      "loading content:  k-means clustering\n",
      "loading content:  k-means clustering\n",
      "examine content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Min Yoon-gi (Korean: 민윤기; born March 9, 1993), known professionally by his stage names Suga (stylized in all caps) and Agust D, is a South Korean rapper, songwriter and record producer. Managed by Big Hit Music, he debuted as a member of the South Korean boy band BTS in 2013. In 2016, he released his first solo mixtape, Agust D. In 2018, he re-released the mixtape for digital purchase and streaming. The reissue reached number three on Billboard\\'s World Albums Chart. In 2020, he released his second solo mixtape, D-2. Commercially, the mixtape peaked at number 11 on the US Billboard 200, number seven on the UK Albums Chart, and number two on Australia\\'s ARIA Album Chart. The Korea Music Copyright Association attributes over 100 songs to Suga as a songwriter and producer, including Suran\\'s \"Wine\" which peaked at number two on the Gaon Music Chart and won best Soul/R&B track of the year at the 2017 Melon Music Awards.\\n\\n\\n== Early life and education ==\\nMin Yoon-gi was born on March 9, 1993, in Daegu, South Korea. The younger of two sons, he attended Taejeon Elementary School, Gwaneum Middle School, and Apgujeong High School.In March 2019, after graduating from the Global Cyber University with a degree in Broadcasting and Entertainment, he enrolled at Hanyang Cyber University for the Master of Business Administration program in Advertising and Media.\\n\\n\\n== Career ==\\n\\n\\n=== 1993–2010: Beginnings ===\\nSuga became interested in rap after hearing \"Ragga Muffin\" by Stony Skunk, stating that it was different from anything he had ever heard before. After hearing Epik High, he decided to become a rapper.By age 13, he began to write music lyrics and learned about MIDI. He worked a part-time job at a record studio by age 17. From then on, he began composing and arranging music, rapping, and performing. Before being signed, he was active under the name Gloss as an underground rapper. As part of the hip hop crew D-Town in 2010, he produced \"518-062\", a song commemorating the Gwangju Uprising.\\n\\n\\n=== 2013–present: BTS ===\\n\\nOriginally joining the company as a music producer, Suga trained under Big Hit Entertainment for three years alongside members J-Hope and RM. He made his debut as a member of BTS on Mnet\\'s M Countdown with the track \"No More Dream\" from their debut single album 2 Cool 4 Skool. He has produced and written lyrics for a variety of tracks on all of BTS\\' albums.\\nFor BTS\\' third Korean-language extended play (EP) The Most Beautiful Moment in Life, Pt. 1, Suga released a solo intro entitled \"Intro: The Most Beautiful Moment in Life\". The rap track\\'s lyrics itself discussed the fears of reaching adulthood at the end of one\\'s adolescent years. It released on April 17, 2015, and featured an animated music video. Pt. 1\\'s follow up EP, The Most Beautiful Moment in Life, Pt. 2, featured another intro performed by Suga, called \"Intro: Never Mind\", specifically recounting Suga\\'s teenage years. The song released on November 15, 2015, and additionally served as the intro for BTS\\' 2016 compilation album The Most Beautiful Moment in Life: Young Forever. Suga did not perform another introductory track for BTS until 2020, where he released \"Interlude: Shadow\" as part of the album roll out for Map of the Soul: 7. The interlude is a rap song that references \"Intro: O!RUL8,2?\" from the 2013 EP of the same name, and discusses BTS\\' fame, comparing its reality to the celebrity O!RUL8,2? dreamed about. The song released on January 10, along with a music video also referencing the struggles of fame. Tamar Herman of Billboard described it as a \"evocative, yet brash track\" and noted how the song\\'s sound changes midway through to show \"this dichotomy between the relationship of how fame and audiences watching him affects his idea of self\".In addition to performing intros for BTS, Suga released two solo tracks under the group\\'s name. The first, a song entitled \"First Love\", appeared on BTS\\' 2016 studio album Wings, is an autobiographical rap track reminiscent of a monologue. On the 2018 compilation album Love Yourself: Answer, Suga released the song \"Trivia: Seesaw\", which discussed the up-and-down nature of falling in love. That same year, Suga was awarded the fifth-class Hwagwan Order of Cultural Merit as a member of BTS by the President of South Korea along with other members of the group.In July 2021, he was appointed Special Presidential Envoy for Future Generations and Culture by President Moon Jae-in, along with the other members of BTS, to help \"lead the global agenda for future generations, such as sustainable growth\" and \"expand South Korea\\'s diplomatic efforts and global standing\" in the international community.\\n\\n\\n=== 2016–present: Solo work ===\\nSuga released a free self-titled mixtape via SoundCloud on August 15, 2016. He decided against releasing the project as a commercial studio album, describing it as the \"feeling of being trapped in some sort of framework.\" On the record, he discussed matters such as his struggles with depression and social phobia. Fuse TV rated it one of the top 20 mixtapes of 2016. The following year, in 2017, Suga composed the song \"Wine\" for singer Suran, whom he had previously worked with for a single on his mixtape. At Suga\\'s studio, Suran heard a rough draft of \"Wine\" and asked Suga for the song. The record peaked at number two on the Gaon Digital Chart in South Korea and won best Soul/R&B track of the year at the Melon Music Awards on December 2, 2017. Suga also received the \"Hot Trend Award\" for his work on the track. Suga later re-released his mixtape for digital purchase and streaming in February 2018. The reissue reached number three on Billboard\\'s World Albums Chart, number five on the Heatseekers Albums chart, and number 74 on the Top Album Sales chart, in the United States. It also caused Suga\\'s solo alias, Agust D, to reach number 46 on the Emerging Artists chart for the week of March 3.In January 2019, Suga provided a rap feature on Lee So-ra\\'s single \"Song Request\". The track was co-written by Suga and Tablo of Epik High, who also produced the track. The single debuted at number three on the Gaon Digital Chart and at number two on Billboard\\'s World Digital Song Sales chart, with 3,000 downloads during the song\\'s two-day charting period. Suga later produced a track for Epik High\\'s Sleepless in  extended play, titled \"Eternal Sunshine\", in February. He co-wrote and produced the digital single, \"We Don\\'t Talk Together\", for singer Heize, which she released on July 7. In December, American singer-songwriter Halsey released the song \"Suga\\'s Interlude\", from her third studio album Manic, which both featured and was produced by Suga.On May 6, 2020, IU released the digital single \"Eight\" featuring and produced by Suga. The song debuted at number one on both the Gaon Digital Chart and the World Digital Song Sales chart. Suga released his second mixtape, D-2 (the continuation to Agust D), together with the music video for its lead single \"Daechwita\" on May 22, which peaked at number 76 on the US Billboard Hot 100 chart. The mixtape debuted at number 11 on the Billboard 200 and became the highest-charting album by a Korean soloist in the US. It is also the first Korean solo release to reach the top 10 in the United Kingdom, opening at number seven on the UK albums chart.In 2021, Suga recomposed Samsung\\'s signature ringtone, \"Over The Horizon\". The track was unveiled on August 11, as part of Samsung\\'s \"Unpacked 2021\" event. Suga later produced the single \"You\" for Japanese singer ØMI, which was released on October 15. In December, Suga featured on the single \"Girl of My Dreams\" for American rapper Juice Wrld\\'s posthumous album Fighting Demons. The song debuted at number 29 on the Billboard Hot 100, earning Suga his second entry on the chart, as a solo artist. On April 25, 2022, Suga was revealed as the producer of the lead single \"That That\" from Psy\\'s album Psy 9th. He also co-wrote and featured on the single, and appeared in the music video alongside Psy.\\n\\n\\n== Name ==\\n\\nThe stage name Suga (슈가) is derived from the first syllables of the term shooting guard (슈팅 가드), the position he played in basketball as a student. He adopted the alias Agust D in 2016 for his mixtape, which is derived from the initials DT, short for his birthplace, Daegu Town, and \"Suga\" spelled backwards.\\n\\n\\n== Artistry ==\\nSuga writes, composes, arranges, mixes, and masters his own material. Over 100 registered songs are credited to him by the Korea Music Copyright Association. He plays piano and produces mainly hip hop and R&B music. His lyrics involve themes that are \"full of dreams and hope,\" conceived with the intent of his music becoming \"many people\\'s strength.\" He cites Stony Skunk and Epik High as his inspirations to pursue hip hop music. Particularly, he credits the former\\'s reggae-hip hop hybrid album Ragga Muffin (2005) and its title track for igniting his interest in the genre.Jeff Benjamin of Fuse said that Suga\\'s mixtape \"showcases the star\\'s ear for hot productions, hardcore rap style, and how he can make his vulnerabilities a strength.\" Other critics stated that Suga\\'s \"storytelling execution in the music he creates tears down the barrier of censoring and sugarcoating\".In January 2018, Suga was promoted to a full member of the Korea Music Copyright Association.\\n\\n\\n=== Reception ===\\nIn a survey conducted by Gallup Korea, Suga was ranked the 13th most preferred idol of the year for 2017. In the 2018 and 2019 surveys, he ranked 7th and 9th, respectively.\\n\\n\\n== Personal life ==\\nIn 2018, he purchased a US$3 million apartment located in South Korea and as of 2019, he lives in Hannam-dong, Seoul, South Korea.\\n\\n\\n=== Health ===\\nIn December 2013, Suga was diagnosed with appendicitis, and underwent surgery at Severance Hospital in Sinchon on the 9th. He was discharged on the 17th, but was hospitalized again on the 26th due to inflammation in the surgical site. Consequently, he was unable to attend any year-end music festivals.In December 2016, Suga suffered an ear injury after tripping over a door threshold. Following doctor recommendations, he took a one-week break from performing and participating in choreography (including at year-end festivals) to ensure the wound healed properly.In November 2020, Suga underwent surgery to repair a torn labrum (reverse Bankart tear) in his left shoulder, and announced that he would be taking a break from subsequent promotional activities in order to fully recover. He resumed activities in January 2021, beginning with a performance at the 35th Golden Disc Awards.\\n\\n\\n== Other ventures ==\\n\\n\\n=== Philanthropy ===\\nIn 2014, Suga made a promise to buy his fans meat should he find success as a musical artist. Four years later, on his 25th birthday, he donated beef to 39 orphanages in the name of \"ARMY\", BTS\\' fanbase. For his 26th birthday, in 2019, he donated KR₩100 million (US$88,000) and 329 BT21 Shooky dolls to the Korea Pediatric Cancer Foundation. On February 27, 2020, Suga donated ₩100 million to the Hope Bridge National Disaster Relief Association to help prevention and relief efforts in his hometown of Daegu, one of the cities hardest hit by the coronavirus outbreak in South Korea at the time. On March 9, 2021, he donated ₩100 million to Daegu\\'s Keimyung University Dongsan Hospital, to support child cancer patients who cannot access treatment due to financial difficulties.On March 9, 2022, in celebration of his 29th birthday, Suga donated ₩100 million to the Hope Bridge Disaster Relief Association to help the victims of the massive wildfires along the country\\'s eastern coastal area.\\n\\n\\n=== Activism ===\\nSuga has spoken openly about mental health and equality for the LGBTQ+ community.\\n\\n\\n== Discography ==\\n\\n\\n=== Mixtapes ===\\n\\n\\n=== Charted songs ===\\n\\n\\n=== Other songs ===\\n\\n\\n=== As producer ===\\n\\n\\n== Filmography ==\\n\\n\\n=== Music videos ===\\n\\n\\n=== Trailers and short films ===\\n\\n\\n== Awards and nominations ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==',\n",
       " 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\\n\\n\\n== Description ==\\nGiven a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        Var\\n        \\u2061\\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}={\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}|S_{i}|\\\\operatorname {Var} S_{i}}\\n  where μi is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n        \\n          \\n            1\\n            \\n              \\n                |\\n              \\n              \\n                S\\n                \\n                  i\\n                \\n              \\n              \\n                |\\n              \\n            \\n          \\n        \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ,\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\,{\\\\frac {1}{|S_{i}|}}\\\\,\\\\sum _{\\\\mathbf {x} ,\\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  The equivalence can be deduced from identity \\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ≠\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |S_{i}|\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}=\\\\sum _{\\\\mathbf {x} \\\\neq \\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  . Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS),. This deterministic relationship is also related to the law of total variance in probability theory.\\n\\n\\n== History ==\\nThe term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.\\n\\n\\n== Algorithms ==\\n\\n\\n=== Standard algorithm (naive k-means) ===\\n\\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the k-means algorithm\"; it is also referred to as Lloyd\\'s algorithm, particularly in the computer science community. It is sometimes also referred to as \"naïve k-means\", because there exist much faster alternatives.Given an initial set of k means m1(1),...,mk(1) (see below), the algorithm proceeds by alternating between two steps:\\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            )\\n          \\n        \\n        =\\n        \\n          {\\n          \\n            \\n              x\\n              \\n                p\\n              \\n            \\n            :\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      i\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n            ≤\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      j\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n             \\n            ∀\\n            j\\n            ,\\n            1\\n            ≤\\n            j\\n            ≤\\n            k\\n          \\n          }\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle S_{i}^{(t)}=\\\\left\\\\{x_{p}:\\\\left\\\\|x_{p}-m_{i}^{(t)}\\\\right\\\\|^{2}\\\\leq \\\\left\\\\|x_{p}-m_{j}^{(t)}\\\\right\\\\|^{2}\\\\ \\\\forall j,1\\\\leq j\\\\leq k\\\\right\\\\},}\\n  \\nwhere each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{p}}\\n   is assigned to exactly one \\n  \\n    \\n      \\n        \\n          S\\n          \\n            (\\n            t\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S^{(t)}}\\n  , even if it could be assigned to two or more of them.\\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\\n\\n  \\n    \\n      \\n        \\n          m\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            +\\n            1\\n            )\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              |\\n              \\n                S\\n                \\n                  i\\n                \\n                \\n                  (\\n                  t\\n                  )\\n                \\n              \\n              |\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n              \\n                j\\n              \\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n              \\n                (\\n                t\\n                )\\n              \\n            \\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m_{i}^{(t+1)}={\\\\frac {1}{\\\\left|S_{i}^{(t)}\\\\right|}}\\\\sum _{x_{j}\\\\in S_{i}^{(t)}}x_{j}}\\n  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\\n\\n\\n==== Initialization methods ====\\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster\\'s randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad\\'s approach performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\\n\\nDemonstration of the standard algorithm\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.\\n\\n\\n=== Complexity ===\\nFinding the optimal solution to the k-means clustering problem for observations in d dimensions is:\\n\\nNP-hard in general Euclidean space (of d dimensions) even for two clusters,\\nNP-hard for a general number of clusters k even in the plane,\\nif k and d (the dimension) are fixed, the problem can be exactly solved in time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            d\\n            k\\n            +\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{dk+1})}\\n  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd\\'s algorithm given above are generally used.\\nThe running time of Lloyd\\'s algorithm (and most variants) is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        k\\n        d\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nkdi)}\\n  , where:\\n\\nn is the number of d-dimensional vectors (to be clustered)\\nk the number of clusters\\ni the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd\\'s algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.\\nIn the worst-case, Lloyd\\'s algorithm needs \\n  \\n    \\n      \\n        i\\n        =\\n        \\n          2\\n          \\n            Ω\\n            (\\n            \\n              \\n                n\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=2^{\\\\Omega ({\\\\sqrt {n}})}}\\n   iterations, so that the worst-case complexity of Lloyd\\'s algorithm is superpolynomial.\\nLloyd\\'s k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in \\n  \\n    \\n      \\n        [\\n        0\\n        ,\\n        1\\n        \\n          ]\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [0,1]^{d}}\\n  , if each point is independently perturbed by a normal distribution with mean 0 and variance \\n  \\n    \\n      \\n        \\n          σ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}}\\n  , then the expected running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            34\\n          \\n        \\n        \\n          k\\n          \\n            34\\n          \\n        \\n        \\n          d\\n          \\n            8\\n          \\n        \\n        \\n          log\\n          \\n            4\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        \\n          /\\n        \\n        \\n          σ\\n          \\n            6\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{34}k^{34}d^{8}\\\\log ^{4}(n)/\\\\sigma ^{6})}\\n  , which is a polynomial in n, k, d and \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        σ\\n      \\n    \\n    {\\\\displaystyle 1/\\\\sigma }\\n  .\\nBetter bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        d\\n        \\n          n\\n          \\n            4\\n          \\n        \\n        \\n          M\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(dn^{4}M^{2})}\\n   for n points in an integer lattice \\n  \\n    \\n      \\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        M\\n        \\n          }\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{1,\\\\dots ,M\\\\}^{d}}\\n  .Lloyd\\'s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd\\'s algorithm.\\n\\n\\n=== Variations ===\\nJenks natural breaks optimization: k-means applied to univariate data\\nk-medians clustering uses the median in each dimension instead of the mean, and this way minimizes \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm (Taxicab geometry).\\nk-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.\\nFuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\\nGaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.\\nk-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.\\nThe filtering algorithm uses kd-trees to speed up each k-means step.\\nSome methods attempt to speed up each k-means step using the triangle inequality.\\nEscape local optima by swapping points between clusters.\\nThe Spherical k-means clustering algorithm is suitable for textual data.\\nHierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.\\nInternal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.\\nMinkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.\\nMini-batch k-means: k-means variation using \"mini batch\" samples for data sets that do not fit into memory.\\nOtsu\\'s method\\n\\n\\n=== Hartigan–Wong method ===\\nHartigan and Wong\\'s method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\\nLet \\n  \\n    \\n      \\n        φ\\n        (\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\varphi (S_{j})}\\n   be the individual cost of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{j}}\\n   defined by \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              S\\n              \\n                j\\n              \\n            \\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          μ\\n          \\n            j\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x\\\\in S_{j}}(x-\\\\mu _{j})^{2}}\\n  , with \\n  \\n    \\n      \\n        \\n          μ\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{j}}\\n   the center of the cluster.\\nAssignment step: Hartigan and Wong\\'s method starts by partitioning the points into random clusters \\n  \\n    \\n      \\n        {\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        \\n          }\\n          \\n            j\\n            ∈\\n            {\\n            1\\n            ,\\n            ⋯\\n            k\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{S_{j}\\\\}_{j\\\\in \\\\{1,\\\\cdots k\\\\}}}\\n  .\\nUpdate step: Next it determines the \\n  \\n    \\n      \\n        n\\n        ,\\n        m\\n        ∈\\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        k\\n        }\\n      \\n    \\n    {\\\\displaystyle n,m\\\\in \\\\{1,\\\\ldots ,k\\\\}}\\n   and \\n  \\n    \\n      \\n        x\\n        ∈\\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\in S_{n}}\\n   for which the following function reaches a maximum\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n        =\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        )\\n        +\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        ∖\\n        {\\n        x\\n        }\\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        ∪\\n        {\\n        x\\n        }\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)=\\\\varphi (S_{n})+\\\\varphi (S_{m})-\\\\varphi (S_{n}\\\\smallsetminus \\\\{x\\\\})-\\\\varphi (S_{m}\\\\cup \\\\{x\\\\}).}\\n  For the \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n   that reach this maximum, \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   moves from the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{n}}\\n   to the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{m}}\\n  .\\nTermination: The algorithm terminates once \\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)}\\n   is less than zero for all \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n  .\\nDifferent move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function \\n  \\n    \\n      \\n        Δ\\n      \\n    \\n    {\\\\displaystyle \\\\Delta }\\n   used to calculate the result of a relocation can also be efficiently evaluated by using equality\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        x\\n        ,\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n              −\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n              +\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            m\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (x,n,m)={\\\\frac {\\\\mid S_{n}\\\\mid }{\\\\mid S_{n}\\\\mid -1}}\\\\cdot \\\\lVert \\\\mu _{n}-x\\\\rVert ^{2}-{\\\\frac {\\\\mid S_{m}\\\\mid }{\\\\mid S_{m}\\\\mid +1}}\\\\cdot \\\\lVert \\\\mu _{m}-x\\\\rVert ^{2}.}\\n  \\n\\n\\n=== Global optimization and metaheuristics ===\\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as  \\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}.}\\n  Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood searchand genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\\n\\n\\n== Discussion ==\\n\\nThree key features of k-means that make it efficient are often regarded as its biggest drawbacks:\\n\\nEuclidean distance is used as a metric and variance is used as a measure of cluster scatter.\\nThe number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\\nConvergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  , the two visible clusters (one containing two species) will be discovered, whereas with \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   one of the two clusters will be split into two even parts. In fact, \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n   is more appropriate for this data set, despite the data set\\'s containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\\nThe result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.\\n\\n\\n== Applications ==\\nk-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd\\'s algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\\n\\n\\n=== Vector quantization ===\\n\\nk-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\\n\\n\\n=== Cluster analysis ===\\nIn cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\\nHowever, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\\n\\n\\n=== Feature learning ===\\nk-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\\n\\n\\n== Relation to other algorithms ==\\n\\n\\n=== Gaussian mixture model ===\\n\\nThe slow \"standard algorithm\" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.:\\u200a850\\u200a Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of \"hard\" Gaussian mixture modelling.:\\u200a354,\\u200a11.4.2.5\\u200a This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.:\\u200a849\\u200a\\n\\n\\n=== K-SVD ===\\n\\nAnother generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\\n\\n\\n=== Principal component analysis ===\\n\\nThe relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\\n\\n\\n=== Mean shift clustering ===\\n\\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\\n\\n\\n=== Independent component analysis ===\\n\\nUnder sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.\\n\\n\\n=== Bilateral filtering ===\\n\\nk-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\\n\\n\\n== Similar problems ==\\nThe set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.\\n\\n\\n== Software implementations ==\\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\\n\\n\\n=== Free Software/Open Source ===\\nThe following implementations are available under Free/Open Source Software licenses, with publicly available source code.\\n\\nAccord.NET contains C# implementations for k-means, k-means++ and k-modes.\\nALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.\\nAOSP contains a Java implementation for k-means.\\nCrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.\\nELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.\\nSmile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).\\nJulia contains a k-means implementation in the JuliaStats Clustering package.\\nKNIME contains nodes for k-means and k-medoids.\\nMahout contains a MapReduce based k-means.\\nmlpack contains a C++ implementation of k-means.\\nOctave contains k-means.\\nOpenCV contains a k-means implementation.\\nOrange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.\\nPSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.\\nR contains three k-means variations.\\nSciPy and scikit-learn contain multiple k-means implementations.\\nSpark MLlib implements a distributed k-means algorithm.\\nTorch contains an unsup package that provides k-means clustering.\\nWeka contains k-means and x-means.\\n\\n\\n=== Proprietary ===\\nThe following implementations are available under proprietary license terms, and may not have publicly available source code.\\n\\n\\n== See also ==\\nBFR algorithm\\nCentroidal Voronoi tessellation\\nHead/tail Breaks\\nk q-flats\\nK-means++\\nLinde–Buzo–Gray algorithm\\nSelf-organizing map\\n\\n\\n== References ==',\n",
       " 'Pakistan, officially the Islamic Republic of Pakistan, is a country in South Asia. It is the world\\'s fifth-most populous country, with a population of almost 242 million, and has the world\\'s second-largest Muslim population. Pakistan is the 33rd-largest country by area, spanning 881,913 square kilometres (340,509 square miles). It has a 1,046-kilometre (650-mile) coastline along the Arabian Sea and Gulf of Oman in the south, and is bordered by India to the east, Afghanistan to the west, Iran to the southwest, and China to the northeast. It is separated narrowly from Tajikistan by Afghanistan\\'s Wakhan Corridor in the north, and also shares a maritime border with Oman.\\nPakistan is the site of several ancient cultures, including the 8,500-year-old Neolithic site of Mehrgarh in Balochistan, and the Indus Valley civilisation of the Bronze Age, the most extensive of the civilisations of the Afro-Eurasia. The region that comprises the modern state of Pakistan was the realm of multiple empires and dynasties, including the Achaemenid; briefly that of Alexander the Great; the Seleucid, the Maurya, the Kushan, the Gupta; the Umayyad Caliphate in its southern regions, the Hindu Shahis, the Ghaznavids, the Delhi Sultanate, the Mughals, the Durranis, the Sikh Empire, British East India Company rule, and most recently, the British Indian Empire from 1858 to 1947.\\nSpurred by the Pakistan Movement, which sought a homeland for the Muslims of British India, and election victories in 1946 by the All-India Muslim League, Pakistan gained independence in 1947 after the Partition of the British Indian Empire, which awarded separate statehood to its Muslim-majority regions and was accompanied by an unparalleled mass migration and loss of life. Initially a Dominion of the British Commonwealth, Pakistan officially drafted its constitution in 1956, and emerged as a declared Islamic republic. In 1971, the exclave of East Pakistan seceded as the new country of Bangladesh after a nine-month-long civil war. In the following four decades, Pakistan has been ruled by governments whose descriptions, although complex, commonly alternated between civilian and military, democratic and authoritarian, relatively secular and Islamist. Pakistan elected a civilian government in 2008, and in 2010 adopted a parliamentary system with periodic elections.Pakistan is a regional and middle power nation, and has the world\\'s sixth-largest standing armed forces. It is a declared nuclear-weapons state, and is ranked amongst the emerging and growth-leading economies, with a large and rapidly-growing middle class. Pakistan\\'s political history since independence has been characterised by periods of significant economic and military growth as well as those of political and economic instability. It is an ethnically and linguistically diverse country, with similarly diverse geography and wildlife. The country continues to face challenges, including poverty, illiteracy, corruption and terrorism. Pakistan is a member of the United Nations, the Shanghai Cooperation Organisation, the Organisation of Islamic Cooperation, the Commonwealth of Nations, the South Asian Association for Regional Cooperation, and the Islamic Military Counter-Terrorism Coalition, and is designated as a major non-NATO ally by the United States.\\n\\n\\n== Etymology ==\\nThe name Pakistan literally means \"land of the pure\" or \"land of purity\", in Urdu and Persian. It alludes to the word پاک (pāk), meaning \"pure\" in Persian and Pashto. The suffix ـستان (transliterated in English as -stan) is from Persian, and means \"land\" or \"place of\".The name of the country was coined in 1933 by Choudhry Rahmat Ali, a Pakistan Movement activist, who published it in a pamphlet Now or Never, using it as an acronym (\"thirty million Muslim brethren who live in PAKISTAN\") to refer to the names of the five northern regions of the British Raj: Punjab, Afghania, Kashmir, Sindh, and Baluchistan.\\n\\n\\n== History ==\\n\\n\\n=== Early and medieval age ===\\n\\nSome of the earliest ancient human civilisations in South Asia originated from areas encompassing present-day Pakistan. The earliest known inhabitants in the region were Soanian during the Lower Paleolithic, of whom stone tools have been found in the Soan Valley of Punjab. The Indus region, which covers most of present day Pakistan, was the site of several successive ancient cultures including the Neolithic Mehrgarh and the Bronze Age Indus Valley civilisation (2,800–1,800 BCE) at Harappa and Mohenjo-Daro.\\n\\nThe Vedic period (1500–500 BCE) was characterised by an Indo-Aryan culture; during this period the Vedas, the oldest scriptures associated with Hinduism, were composed, and this culture later became well established in the region. Multan was an important Hindu pilgrimage centre. The Vedic civilisation flourished in the ancient Gandhāran city of Takṣaśilā, now Taxila in the Punjab, which was founded around 1000 BCE. Successive ancient empires and kingdoms ruled the region: the Persian Achaemenid Empire (around 519 BCE), Alexander the Great\\'s empire in 326 BCE and the Maurya Empire, founded by Chandragupta Maurya and extended by Ashoka the Great, until 185 BCE. The Indo-Greek Kingdom founded by Demetrius of Bactria (180–165 BCE) included Gandhara and Punjab and reached its greatest extent under Menander (165–150 BCE), prospering the Greco-Buddhist culture in the region. Taxila had one of the earliest universities and centres of higher education in the world, which was established during the late Vedic period in 6th century BCE. The school consisted of several monasteries without large dormitories or lecture halls where the religious instruction was provided on an individualistic basis. The ancient university was documented by the invading forces of Alexander the Great and was also recorded by Chinese pilgrims in the 4th or 5th century CE.At its zenith, the Rai Dynasty (489–632 CE) of Sindh ruled this region and the surrounding territories. The Pala Dynasty was the last Buddhist empire, which, under Dharmapala and Devapala, stretched across South Asia from what is now Bangladesh through Northern India to Pakistan.\\n\\n\\n=== Islamic conquest ===\\nThe Arab conqueror Muhammad ibn Qasim conquered Sindh in 711 CE. The Pakistan government\\'s official chronology claims this as the time when the foundation of Pakistan was laid but the concept of Pakistan arrived in the 19th century. The Early Medieval period (642–1219 CE) witnessed the spread of Islam in the region. During this period, Sufi missionaries played a pivotal role in converting a majority of the regional Buddhist and Hindu population to Islam. Upon the defeat of the Turk and Hindu Shahi dynasties which governed the Kabul Valley, Gandhara (present-day Khyber Pakhtunkwa), and western Punjab in the 7th to 11th centuries CE, several successive Muslim empires ruled over the region, including the Ghaznavid Empire (975–1187 CE), the Ghorid Kingdom, and the Delhi Sultanate (1206–1526 CE). The Lodi dynasty, the last of the Delhi Sultanate, was replaced by the Mughal Empire (1526–1857 CE).\\n\\nThe Mughals introduced Persian literature and high culture, establishing the roots of Indo-Persian culture in the region. In the region of modern-day Pakistan, key cities during the Mughal period were Lahore and Thatta, both of which were chosen as the site of impressive Mughal buildings. In the early 16th century, the region remained under the Mughal Empire.In the 18th century, the slow disintegration of the Mughal Empire was hastened by the emergence of the rival powers of the Maratha Confederacy and later the Sikh Empire, as well as invasions by Nader Shah from Iran in 1739 and the Durrani Empire of Afghanistan in 1759. The growing political power of the British in Bengal had not yet reached the territories of modern Pakistan.\\n\\n\\n=== Colonial period ===\\n\\nNone of the territory of modern Pakistan was ruled by the British, or other European powers, until 1839, when Karachi, then a small fishing village with a mud fort guarding the harbour, was taken, and held as an enclave with a port and military base for the First Afghan War that soon followed. The rest of Sindh was taken in 1843, and in the following decades, first the East India Company, and then after the post-Sepoy Mutiny (1857–1858) direct rule of Queen Victoria of the British Empire, took over most of the country partly through wars, and also treaties. The main wars were that against the Baloch Talpur dynasty, ended by the Battle of Miani (1843) in Sindh, the Anglo-Sikh Wars (1845–1849) and the Anglo-Afghan Wars (1839–1919). By 1893, all modern Pakistan was part of the British Indian Empire, and remained so until independence in 1947.\\nUnder the British, modern Pakistan was mostly divided into the Sind Division, Punjab Province, and the Baluchistan Agency. There were various princely states, of which the largest was Bahawalpur.\\nA rebellion in 1857 called the Sepoy mutiny of Bengal was the region\\'s major armed struggle against the British. Divergence in the relationship between Hinduism and Islam created a major rift in British India that led to motivated religious violence in British India. The language controversy further escalated the tensions between Hindus and Muslims. The Hindu renaissance witnessed an awakening of intellectualism in traditional Hinduism and saw the emergence of more assertive influence in the social and political spheres in British India. A Muslim intellectual movement, founded by Sir Syed Ahmed Khan to counter the Hindu renaissance, envisioned as well as advocated for the two-nation theory and led to the creation of the All-India Muslim League in 1906. In contrast to the Indian National Congress\\'s anti-British efforts, the Muslim League was a pro-British movement whose political program inherited the British values that would shape Pakistan\\'s future civil society. The largely non-violent independence struggle led by the Indian Congress engaged millions of protesters in mass campaigns of civil disobedience in the 1920s and 1930s against the British Empire.\\n\\nThe Muslim League slowly rose to mass popularity in the 1930s amid fears of under-representation and neglect by the British of the Indian Muslims in politics. In his presidential address of 29 December 1930, Allama Iqbal called for \"the amalgamation of North-West Muslim-majority Indian states\" consisting of Punjab, North-West Frontier Province, Sind, and Baluchistan. The perceived neglect of Muslim interests by Congress led British provincial governments during the period of 1937–39 convinced Muhammad Ali Jinnah, the founder of Pakistan to espouse the two-nation theory and led the Muslim League to adopt the Lahore Resolution of 1940 presented by Sher-e-Bangla A.K. Fazlul Haque, popularly known as the Pakistan Resolution. In World War II, Jinnah and British-educated founding fathers in the Muslim League supported the United Kingdom\\'s war efforts, countering opposition against it whilst working towards Sir Syed\\'s vision.\\n\\n\\n=== Pakistan Movement ===\\n\\nThe 1946 elections resulted in the Muslim League winning 90 percent of the seats reserved for Muslims. Thus, the 1946 election was effectively a plebiscite in which the Indian Muslims were to vote on the creation of Pakistan, a plebiscite won by the Muslim League. This victory was assisted by the support given to the Muslim League by the support of the landowners of Sindh and Punjab. The Indian National Congress, which initially denied the Muslim League\\'s claim of being the sole representative of Indian Muslims, was now forced to recognise the fact. The British had no alternative except to take Jinnah\\'s views into account as he had emerged as the sole spokesperson of the entirety of British India\\'s Muslims. However, the British did not want colonial India to be partitioned, and in one last effort to prevent it, they devised the Cabinet Mission plan.As the cabinet mission failed, the British government announced its intention to end the British Rule in 1946–47. Nationalists in British India—including Jawaharlal Nehru and Abul Kalam Azad of Congress, Jinnah of the All-India Muslim League, and Master Tara Singh representing the Sikhs—agreed to the proposed terms of transfer of power and independence in June 1947 with the Viceroy of India, Lord Mountbatten of Burma. As the United Kingdom agreed to the partitioning of India in 1947, the modern state of Pakistan was established on 14 August 1947 (27th of Ramadan in 1366 of the Islamic Calendar), amalgamating the Muslim-majority eastern and northwestern regions of British India. It comprised the provinces of Balochistan, East Bengal, the North-West Frontier Province, West Punjab, and Sindh.In the riots that accompanied the partition in Punjab Province, it is believed that between 200,000 and 2,000,000 people were killed in what some have described as a retributive genocide between the religions while 50,000 Muslim women were abducted and raped by Hindu and Sikh men, 33,000 Hindu and Sikh women also experienced the same fate at the hands of Muslims. Around 6.5 million Muslims moved from India to West Pakistan and 4.7 million Hindus and Sikhs moved from West Pakistan to India. It was the largest mass migration in human history. A subsequent dispute over the princely state of Jammu and Kashmir eventually sparked the Indo-Pakistani War of 1947–1948.\\n\\n\\n=== Independence and modern Pakistan ===\\n\\nAfter independence in 1947, Jinnah, the President of the Muslim League, became the nation\\'s first Governor-General as well as the first President-Speaker of the Parliament, but he died of tuberculosis on 11 September 1948. Meanwhile, Pakistan\\'s founding fathers agreed to appoint Liaquat Ali Khan, the secretary-general of the party, the nation\\'s first Prime Minister. From 1947 to 1956, Pakistan was a monarchy within the Commonwealth of Nations, and had two monarchs before it became a republic.\\n\\nThe creation of Pakistan was never fully accepted by many British leaders, among them Lord Mountbatten. Mountbatten clearly expressed his lack of support and faith in the Muslim League\\'s idea of Pakistan. Jinnah refused Mountbatten\\'s offer to serve as Governor-General of Pakistan. When Mountbatten was asked by Collins and Lapierre if he would have sabotaged Pakistan had he known that Jinnah was dying of tuberculosis, he replied \\'most probably\\'.Maulana Shabbir Ahmad Usmani, a respected Deobandi alim (scholar) who occupied the position of Shaykh al-Islam in Pakistan in 1949, and Maulana Mawdudi of Jamaat-i-Islami played a pivotal role in the demand for an Islamic constitution. Mawdudi demanded that the Constituent Assembly make an explicit declaration affirming the \"supreme sovereignty of God\" and the supremacy of the shariah in Pakistan.A significant result of the efforts of the Jamaat-i-Islami and the ulama was the passage of the Objectives Resolution in March 1949. The Objectives Resolution, which Liaquat Ali Khan called the second most important step in Pakistan\\'s history, declared that \"sovereignty over the entire universe belongs to God Almighty alone and the authority which He has delegated to the State of Pakistan through its people for being exercised within the limits prescribed by Him is a sacred trust\". The Objectives Resolution has been incorporated as a preamble to the constitutions of 1956, 1962, and 1973.Democracy was stalled by the martial law that had been enforced by President Iskander Mirza, who was replaced by the army chief, General Ayub Khan. After adopting a presidential system in 1962, the country experienced exceptional growth until a second war with India in 1965 that led to an economic downturn and wide-scale public disapproval in 1967. Consolidating control from Ayub Khan in 1969, President Yahya Khan had to deal with a devastating cyclone that caused 500,000 deaths in East Pakistan.\\n\\nIn 1970 Pakistan held its first democratic elections since independence, meant to mark a transition from military rule to democracy, but after the East Pakistani Awami League won against the Pakistan Peoples Party (PPP), Yahya Khan and the military establishment refused to hand over power. Operation Searchlight, a military crackdown on the Bengali nationalist movement, led to a declaration of independence and the waging of a war of liberation by the Bengali Mukti Bahini forces in East Pakistan, which in West Pakistan was described as a civil war as opposed to a war of liberation.Independent researchers estimate that between 300,000 and 500,000 civilians died during this period while the Bangladesh government puts the number of dead at three million, a figure that is now nearly universally regarded as excessively inflated. Some academics such as Rudolph Rummel and Rounaq Jahan say both sides committed genocide; others such as Richard Sisson and Leo E. Rose believe there was no genocide. In response to India\\'s support for the insurgency in East Pakistan, preemptive strikes on India by Pakistan\\'s air force, navy, and marines sparked a conventional war in 1971 that resulted in an Indian victory and East Pakistan gaining independence as Bangladesh.With Pakistan surrendering in the war, Yahya Khan was replaced by Zulfikar Ali Bhutto as president; the country worked towards promulgating its constitution and putting the country on the road to democracy. Democratic rule resumed from 1972 to 1977—an era of self-consciousness, intellectual leftism, nationalism, and nationwide reconstruction. In 1972 Pakistan embarked on an ambitious plan to develop its nuclear deterrence capability with the goal of preventing any foreign invasion; the country\\'s first nuclear power plant was inaugurated in that same year. Accelerated in response to India\\'s first nuclear test in 1974, this crash program was completed in 1979.Democracy ended with a military coup in 1977 against the leftist PPP, which saw General Zia-ul-Haq become the president in 1978. From 1977 to 1988, President Zia\\'s corporatisation and economic Islamisation initiatives led to Pakistan becoming one of the fastest-growing economies in South Asia. While building up the country\\'s nuclear program, increasing Islamisation, and the rise of a homegrown conservative philosophy, Pakistan helped subsidise and distribute US resources to factions of the mujahideen against the USSR\\'s intervention in communist Afghanistan. Pakistan\\'s North-West Frontier Province became a base for the anti-Soviet Afghan fighters, with the province\\'s influential Deobandi ulama playing a significant role in encouraging and organising the \\'jihad\\'.President Zia died in a plane crash in 1988, and Benazir Bhutto, daughter of Zulfikar Ali Bhutto, was elected as the country\\'s first female Prime Minister. The PPP was followed by conservative Pakistan Muslim League (N), and over the next decade the leaders of the two parties fought for power, alternating in office while the country\\'s situation worsened; economic indicators fell sharply, in contrast to the 1980s. This period is marked by prolonged stagflation, instability, corruption, nationalism, geopolitical rivalry with India, and the clash of left wing-right wing ideologies. As PML (N) secured a supermajority in elections in 1997, Sharif authorised nuclear testings (See:Chagai-I and Chagai-II), as a retaliation to the second nuclear tests ordered by India, led by Prime Minister Atal Bihari Vajpayee in May 1998.\\n\\nMilitary tension between the two countries in the Kargil district led to the Kargil War of 1999, and turmoil in civic-military relations allowed General Pervez Musharraf to take over through a bloodless coup d\\'état. Musharraf governed Pakistan as chief executive from 1999 to 2001 and as President from 2001 to 2008—a period of enlightenment, social liberalism, extensive economic reforms, and direct involvement in the US-led war on terrorism. When the National Assembly historically completed its first full five-year term on 15 November 2007, the new elections were called by the Election Commission.After the assassination of Benazir Bhutto in 2007, the PPP secured the most votes in the elections of 2008, appointing party member Yousaf Raza Gillani as Prime Minister. Threatened with impeachment, President Musharraf resigned on 18 August 2008, and was succeeded by Asif Ali Zardari. Clashes with the judicature prompted Gillani\\'s disqualification from the Parliament and as the Prime Minister in June 2012. By its own financial calculations, Pakistan\\'s involvement in the war on terrorism has cost up to $118 billion, sixty thousand casualties and more than 1.8 million displaced civilians. The general election held in 2013 saw the PML (N) almost achieve a supermajority, following which Nawaz Sharif was elected as the Prime Minister, returning to the post for the third time in fourteen years, in a democratic transition. In 2018, Imran Khan (the chairman of PTI) won the 2018 Pakistan general election with 116 general seats and became the 22nd Prime Minister of Pakistan in election of National Assembly of Pakistan for Prime Minister by getting 176 votes against Shehbaz Sharif (the chairman of PML (N)) who got 96 votes. In April 2022, Shehbaz Sharif was elected as Pakistan\\'s new prime minister, after Imran Khan lost a no-confidence vote in the parliament.\\n\\n\\n== Role of Islam ==\\n\\nPakistan is the only country to have been created in the name of Islam. The idea of Pakistan, which had received overwhelming popular support among Indian Muslims, especially those in the provinces of British India where Muslims were in a minority such as the United Provinces, was articulated in terms of an Islamic state by the Muslim League leadership, the ulama (Islamic clergy) and Jinnah. Jinnah had developed a close association with the ulama and upon his death was described by one such alim, Maulana Shabbir Ahmad Usmani, as the greatest Muslim after Aurangzeb and as someone who desired to unite the Muslims of the world under the banner of Islam.The Objectives Resolution in March 1949, which declared God as the sole sovereign over the entire universe, represented the first formal step to transform Pakistan into an Islamic state. Muslim League leader Chaudhry Khaliquzzaman asserted that Pakistan could only truly become an Islamic state after bringing all believers of Islam into a single political unit. Keith Callard, one of the earliest scholars on Pakistani politics, observed that Pakistanis believed in the essential unity of purpose and outlook in the Muslim world and assumed that Muslim from other countries would share their views on the relationship between religion and nationality.\\n\\nHowever, Pakistan\\'s pan-Islamist sentiments for a united Islamic bloc called Islamistan were not shared by other Muslim governments, although Islamists such as the Grand Mufti of Palestine, Al-Haj Amin al-Husseini, and leaders of the Muslim Brotherhood, became drawn to the country. Pakistan\\'s desire for an international organization of Muslim countries was fulfilled in the 1970s when the Organization of Islamic Conference (OIC) was formed.The strongest opposition to the Islamist ideological paradigm being imposed on the state came from the Bengali Muslims of East Pakistan whose educated class, according to a survey by social scientist Nasim Ahmad Jawed, preferred secularism and focused on ethnic identity unlike educated West Pakistanis who tended to prefer an Islamic identity. The Islamist party Jamaat-e-Islami considered Pakistan to be an Islamic state and believed Bengali nationalism to be unacceptable. In the 1971 conflict over East Pakistan, the Jamaat-e-Islami fought the Bengali nationalists on the Pakistan Army\\'s side. The conflict concluded with East Pakistan seceding and the creation of independent Bangladesh.\\nAfter Pakistan\\'s first ever general elections, the 1973 Constitution was created by an elected Parliament. The Constitution declared Pakistan an Islamic Republic and Islam as the state religion. It also stated that all laws would have to be brought into accordance with the injunctions of Islam as laid down in the Quran and Sunnah and that no law repugnant to such injunctions could be enacted. The 1973 Constitution also created certain institutions such as the Shariat Court and the Council of Islamic Ideology to channel the interpretation and application of Islam.Pakistan\\'s leftist Prime Minister Zulfikar Ali Bhutto faced vigorous opposition which coalesced into a movement united under the revivalist banner of Nizam-e-Mustafa (\"Rule of the Prophet\") which aimed to establish an Islamic state based on Sharia laws. Bhutto agreed to some Islamist demands before being overthrown in a coup.In 1977, after taking power from Bhutto in a coup d\\'état, General Zia-ul-Haq, who came from a religious background, committed himself to establishing an Islamic state and enforcing sharia law. Zia established separate Shariat judicial courts and court benches to judge legal cases using Islamic doctrine. Zia bolstered the influence of the ulama (Islamic clergy) and the Islamic parties. Zia-ul-Haq forged a strong alliance between the military and Deobandi institutions and even though most Barelvi ulama and only a few Deobandi scholars had supported Pakistan\\'s creation, Islamic state politics came to be mostly in favour of Deobandi (and later Ahl-e-Hadith/Salafi) institutions instead of Barelvi. Sectarian tensions increased with Zia\\'s anti-Shia policies.According to a Pew Research Center (PEW) opinion poll, a majority of Pakistanis support making Sharia the official law of the land. In a survey of several Muslim countries, PEW also found that Pakistanis tend to identify with their religion more than their nationality in contrast to Muslims in other nations such as Egypt, Indonesia and Jordan.\\n\\n\\n== Geography, environment, and climate ==\\n\\nThe geography and climate of Pakistan are extremely diverse, and the country is home to a wide variety of wildlife. Pakistan covers an area of 881,913 km2 (340,509 sq mi), approximately equal to the combined land areas of France and the United Kingdom. It is the 33rd-largest nation by total area, although this ranking varies depending on how the disputed territory of Kashmir is counted. Pakistan has a 1,046 km (650 mi) coastline along the Arabian Sea and the Gulf of Oman in the south and land borders of 6,774 km (4,209 mi) in total: 2,430 km (1,510 mi) with Afghanistan, 523 km (325 mi) with China, 2,912 km (1,809 mi) with India and 909 km (565 mi) with Iran. It shares a marine border with Oman, and is separated from Tajikistan by the cold, narrow Wakhan Corridor. Pakistan occupies a geopolitically important location at the crossroads of South Asia, the Middle East, and Central Asia.Geologically, Pakistan is located in the Indus–Tsangpo Suture Zone and overlaps the Indian tectonic plate in its Sindh and Punjab provinces; Balochistan and most of Khyber Pakhtunkhwa are within the Eurasian plate, mainly on the Iranian plateau. Gilgit-Baltistan and Azad Kashmir lie along the edge of the Indian plate and hence are prone to violent earthquakes. This region has the highest rates of seismicity and the largest earthquakes in the Himalaya region. Ranging from the coastal areas of the south to the glaciated mountains of the north, Pakistan\\'s landscapes vary from plains to deserts, forests, hills, and plateaus.Pakistan is divided into three major geographic areas: the northern highlands, the Indus River plain, and the Balochistan Plateau. The northern highlands contain the Karakoram, Hindu Kush, and Pamir mountain ranges (see mountains of Pakistan), which contain some of the world\\'s highest peaks, including five of the fourteen eight-thousanders (mountain peaks over 8,000 metres or 26,250 feet), which attract adventurers and mountaineers from all over the world, notably K2 (8,611 m or 28,251 ft) and Nanga Parbat (8,126 m or 26,660 ft). The Balochistan Plateau lies in the west and the Thar Desert in the east. The 1,609 km (1,000 mi) Indus River and its tributaries flow through the country from the Kashmir region to the Arabian Sea. There is an expanse of alluvial plains along it in the Punjab and Sindh.The climate varies from tropical to temperate, with arid conditions in the coastal south. There is a monsoon season with frequent flooding due to heavy rainfall, and a dry season with significantly less rainfall or none at all. There are four distinct seasons in Pakistan: a cool, dry winter from December through February; a hot, dry spring from March through May; the summer rainy season, or southwest monsoon period, from June through September; and the retreating monsoon period of October and November. Rainfall varies greatly from year to year, and patterns of alternate flooding and drought are common.\\n\\n\\n=== Flora and fauna ===\\n\\nThe diversity of the landscape and climate in Pakistan allows a wide variety of trees and plants to flourish. The forests range from coniferous alpine and subalpine trees such as spruce, pine, and deodar cedar in the extreme northern mountains to deciduous trees in most of the country (for example, the mulberry-like shisham found in the Sulaiman Mountains), to palms such as coconut and date in the southern Punjab, southern Balochistan, and all of Sindh. The western hills are home to juniper, tamarisk, coarse grasses, and scrub plants. Mangrove forests form much of the coastal wetlands along the coast in the south.\\n\\nConiferous forests are found at altitudes ranging from 1,000 to 4,000 metres (3,300 to 13,100 feet) in most of the northern and northwestern highlands. In the xeric regions of Balochistan, date palm and Ephedra are common. In most of the Punjab and Sindh, the Indus plains support tropical and subtropical dry and moist broadleaf forest as well as tropical and xeric shrublands. These forests are mostly of mulberry, acacia, and eucalyptus. About 2.2% or 1,687,000 hectares (16,870 km2) of Pakistan was forested in 2010.The fauna of Pakistan also reflects the country\\'s varied climate. Around 668 bird species are found there, including crows, sparrows, mynas, hawks, falcons, and eagles. Palas, Kohistan, has a significant population of western tragopan. Many birds sighted in Pakistan are migratory, coming from Europe, Central Asia, and India.The southern plains are home to mongooses, small Indian civet, hares, the Asiatic jackal, the Indian pangolin, the jungle cat, and the desert cat. There are mugger crocodiles in the Indus, and wild boar, deer, porcupines, and small rodents in the surrounding areas. The sandy scrublands of central Pakistan are home to Asiatic jackals, striped hyenas, wildcats, and leopards. The lack of vegetative cover, the severe climate, and the impact of grazing on the deserts have left wild animals in a precarious position. The chinkara is the only animal that can still be found in significant numbers in Cholistan. A small number of nilgai are found along the Pakistan–India border and in some parts of Cholistan. A wide variety of animals live in the mountainous north, including the Marco Polo sheep, the urial (a subspecies of wild sheep), the markhor goat, the ibex goat, the Asian black bear, and the Himalayan brown bear. Among the rare animals found in the area are the snow leopard and the blind Indus river dolphin, of which there are believed to be about 1,100 remaining, protected at the Indus River Dolphin Reserve in Sindh. In total, 174 mammals, 177 reptiles, 22 amphibians, 198 freshwater fish species and 5,000 species of invertebrates (including insects) have been recorded in Pakistan.The flora and fauna of Pakistan suffer from a number of problems. Pakistan has the second-highest rate of deforestation in the world, which, along with hunting and pollution, has had adverse effects on the ecosystem. It had a 2019 Forest Landscape Integrity Index mean score of 7.42/10, ranking it 41st globally out of 172 countries. The government has established a large number of protected areas, wildlife sanctuaries, and game reserves to address these issues.\\n\\n\\n== Government and politics ==\\n\\nPakistan\\'s political experience is essentially related to the struggle of Indian Muslims to regain the power they lost to British colonisation. Pakistan is a democratic parliamentary federal republic, with Islam as the state religion. The first constitution was adopted in 1956 but suspended by Ayub Khan in 1958, who replaced it with the second constitution in 1962. A complete and comprehensive constitution was adopted in 1973, it was suspended by Zia-ul-Haq in 1977 but reinstated in 1985. This constitution is the country\\'s most important document, laying the foundations of the current government. The Pakistani military establishment has played an influential role in mainstream politics throughout Pakistan\\'s political history. The periods 1958–1971, 1977–1988, and 1999–2008 saw military coups that resulted in the imposition of martial law and military commanders who governed as de facto presidents. Today Pakistan has a multi-party parliamentary system with clear division of powers and checks and balances among the branches of government. The first successful democratic transition occurred in May 2013. Politics in Pakistan is centred on, and dominated by, a homegrown social philosophy comprising a blend of ideas from socialism, conservatism, and the third way. As of the general elections held in 2013, the three main political parties in the country are: the centre-right conservative Pakistan Muslim League-N; the centre-left socialist PPP; and the centrist and third-way Pakistan Movement for Justice (PTI).\\n\\nHead of State: The President, who is elected by an Electoral College is the ceremonial head of the state and is the civilian commander-in-chief of the Pakistan Armed Forces (with the Chairman Joint Chiefs of Staff Committee as principal military adviser), but military appointments and key confirmations in the armed forces are made by the Prime Minister after reviewing the reports on candidates\\' merit and performance. Almost all appointed officers in the judicature, military, the chairman joint chiefs, joint staff, and legislature require the executive confirmation from the Prime Minister, whom the President must consult by law. However, the powers to pardon and grant clemency lie with the President of Pakistan.\\nLegislative: The bicameral legislature comprises a 104-member Senate (upper house) and a 342-member National Assembly (lower house). Members of the National Assembly are elected through the first-past-the-post system under universal adult suffrage, representing electoral districts known as National Assembly constituencies. According to the constitution, the 70 seats reserved for women and religious minorities are allocated to the political parties according to their proportional representation. Senate members are elected by provincial legislators, with all the provinces having equal representation.\\nExecutive: The Prime Minister is usually the leader of the majority rule party or a coalition in the National Assembly— the lower house. The Prime Minister serves as the head of government and is designated to exercise as the country\\'s chief executive. The Prime Minister is responsible for appointing a cabinet consisting of ministers and advisers as well as running the government operations, taking and authorising executive decisions, appointments and recommendations of senior civil servants that require executive confirmation of the Prime Minister.\\nProvincial governments: Each of the four provinces has a similar system of government, with a directly elected Provincial Assembly in which the leader of the largest party or coalition is elected Chief Minister. Chief Ministers oversee the provincial governments and head the provincial cabinet. It is common in Pakistan to have different ruling parties or coalitions in each of the provinces. The provincial bureaucracy is headed by the Chief Secretary, who is appointed by the Prime Minister. The provincial assemblies have power to make laws and approve the provincial budget which is commonly presented by the provincial finance minister every fiscal year. Provincial governors who are the ceremonial heads of the provinces are appointed by the President.\\nJudicature: The judiciary of Pakistan is a hierarchical system with two classes of courts: the superior (or higher) judiciary and the subordinate (or lower) judiciary. The Chief Justice of Pakistan is the chief judge who oversees the judicature\\'s court system at all levels of command. The superior judiciary is composed of the Supreme Court of Pakistan, the Federal Shariat Court and five high courts, with the Supreme Court at the apex. The Constitution of Pakistan entrusts the superior judiciary with the obligation to preserve, protect and defend the constitution. Other regions of Azad Kashmir and Gilgit-Baltistan have separate court systems.\\n\\n\\n=== Foreign relations ===\\n\\nSince Independence, Pakistan has attempted to balance its relations with foreign nations. Pakistan is a strong ally of China, with both countries placing considerable importance on the maintenance of an extremely close and supportive special relationship. It has also been a major non-NATO ally of the United States ever since the war against terrorism – a status achieved in 2004. Pakistan\\'s foreign policy and geostrategy mainly focus on the economy and security against threats to its national identity and territorial integrity, and on the cultivation of close relations with other Muslim countries.The Kashmir conflict remains the major point of contention between Pakistan and India; three of their four wars were fought over this territory. Due partly to difficulties in relations with its geopolitical rival India, Pakistan maintains close political relations with Turkey and Iran, and both countries have been a focal point in Pakistan\\'s foreign policy. Saudi Arabia also maintains a respected position in Pakistan\\'s foreign policy.\\nA non-signatory party of the Treaty on Nuclear Non-Proliferation, Pakistan is an influential member of the IAEA. In recent events, Pakistan has blocked an international treaty to limit fissile material, arguing that the \"treaty would target Pakistan specifically\". In the 20th century, Pakistan\\'s nuclear deterrence program focused on countering India\\'s nuclear ambitions in the region, and nuclear tests by India eventually led Pakistan to reciprocate to maintain a geopolitical balance as becoming a nuclear power. Currently, Pakistan maintains a policy of credible minimum deterrence, calling its program vital nuclear deterrence against foreign aggression.Located in the strategic and geopolitical corridor of the world\\'s major maritime oil supply lines and communication fibre optics, Pakistan has proximity to the natural resources of Central Asian countries. Briefing on the country\\'s foreign policy in 2004, a Pakistani senator reportedly explained: \"Pakistan highlights sovereign equality of states, bilateralism, mutuality of interests, and non-interference in each other\\'s domestic affairs as the cardinal features of its foreign policy.\" Pakistan is an active member of the United Nations and has a Permanent Representative to represent Pakistan\\'s positions in international politics. Pakistan has lobbied for the concept of \"enlightened moderation\" in the Muslim world. Pakistan is also a member of Commonwealth of Nations, the South Asian Association for Regional Cooperation (SAARC), the Economic Cooperation Organization (ECO), and the G20 developing nations.\\n\\nDue to ideological differences, Pakistan opposed the Soviet Union in the 1950s. During the Soviet–Afghan War in the 1980s, Pakistan was one of the closest allies of the United States. Relations between Pakistan and Russia have greatly improved since 1999, and co-operation in various sectors has increased. Pakistan has had an \"on-and-off\" relationship with the United States. A close ally of the United States during the Cold War, Pakistan\\'s relationship with the US soured in the 1990s when the latter imposed sanctions because of Pakistan\\'s secretive nuclear development. Since 9/11, Pakistan has been a close ally of the US on the issue of counterterrorism in the regions of the Middle East and South Asia, with the US supporting Pakistan with aid money and weapons. Initially, the US-led war on terrorism led to an improvement in the relationship, but it was strained by a divergence of interests and resulting mistrust during the war in Afghanistan and by issues related to terrorism. The Pakistani intelligence agency, the ISI, was accused of supporting Taliban insurgents in Afghanistan.Pakistan does not have diplomatic relations with Israel; nonetheless, some Israeli citizens have visited the country on tourist visas. However, an exchange took place between the two countries using Turkey as a communication conduit. Despite Pakistan being the only country in the world that has not established diplomatic relations with Armenia, an Armenian community still resides in Pakistan. Pakistan had warm relations with Bangladesh, despite some initial strains in their relationship.\\n\\n\\n==== Relations with China ====\\n\\nPakistan was one of the first countries to establish formal diplomatic relations with the People\\'s Republic of China, and the relationship continues to be strong since China\\'s war with India in 1962, forming a special relationship. From the 1960s to 1980s, Pakistan greatly helped China in reaching out to the world\\'s major countries and helped facilitate US President Richard Nixon\\'s state visit to China. Despite the change of governments in Pakistan and fluctuations in the regional and global situation, China\\'s policy in Pakistan continues to be a dominant factor at all times. In return, China is Pakistan\\'s largest trading partner, and economic co-operation has flourished, with substantial Chinese investment in Pakistan\\'s infrastructural expansion such as the Pakistani deep-water port at Gwadar. Friendly Sino-Pakistani relations reached new heights as both countries signed 51 agreements and Memorandums of Understanding (MoUs) in 2015 for co-operation in different areas. Both countries signed a Free Trade Agreement in the 2000s, and Pakistan continues to serve as China\\'s communication bridge to the Muslim world. In 2016, China announced that it will set up an anti-terrorism alliance with Pakistan, Afghanistan, and Tajikistan. In December 2018, Pakistan\\'s government defended China\\'s re-education camps for a million Uyghur Muslims.\\n\\n\\n==== Emphasis on relations with Muslim world ====\\nAfter Independence, Pakistan vigorously pursued bilateral relations with other Muslim countries and made an active bid for leadership of the Muslim world, or at least for leadership in efforts to achieve unity. The Ali brothers had sought to project Pakistan as the natural leader of the Islamic world, in part due to its large manpower and military strength. A top-ranking Muslim League leader, Khaliquzzaman, declared that Pakistan would bring together all Muslim countries into Islamistan – a pan-Islamic entity.Such developments (along with Pakistan\\'s creation) did not get American approval, and British Prime Minister Clement Attlee voiced international opinion at the time by stating that he wished that India and Pakistan would re-unite. Since most of the Arab world was undergoing a nationalist awakening at the time, there was little attraction to Pakistan\\'s Pan-Islamic aspirations. Some of the Arab countries saw the \\'Islamistan\\' project as a Pakistani attempt to dominate other Muslim states.Pakistan vigorously championed the right of self-determination for Muslims around the world. Pakistan\\'s efforts for the independence movements of Indonesia, Algeria, Tunisia, Morocco, and Eritrea were significant and initially led to close ties between these countries and Pakistan. However, Pakistan also masterminded an attack on the Afghan city of Jalalabad during the Afghan Civil War to establish an Islamic government there. Pakistan had wished to foment an \\'Islamic Revolution\\' that would transcend national borders, covering Pakistan, Afghanistan, and Central Asia.On the other hand, Pakistan\\'s relations with Iran have been strained at times due to sectarian tensions. Iran and Saudi Arabia used Pakistan as a battleground for their proxy sectarian war, and by the 1990s Pakistan\\'s support for the Sunni Taliban organisation in Afghanistan became a problem for Shia Iran, which opposed a Taliban-controlled Afghanistan. Tensions between Iran and Pakistan intensified in 1998 when Iran accused Pakistan of war crimes after Pakistani warplanes had bombarded Afghanistan\\'s last Shia stronghold in support of the Taliban.Pakistan is an influential and founding member of the Organisation of Islamic Cooperation (OIC). Maintaining cultural, political, social, and economic relations with the Arab world and other countries in the Muslim world is a vital factor in Pakistan\\'s foreign policy.\\n\\n\\n=== Administrative divisions ===\\n\\nA federal parliamentary republic state, Pakistan is a federation that comprises four provinces: Punjab, Khyber Pakhtunkhwa, Sindh and Balochistan, and three territories: Islamabad Capital Territory, Gilgit-Baltistan and Azad Kashmir. The Government of Pakistan exercises the de facto jurisdiction over the Frontier Regions and the western parts of the Kashmir Regions, which are organised into the separate political entities Azad Kashmir and Gilgit-Baltistan (formerly Northern Areas). In 2009, the constitutional assignment (the Gilgit–Baltistan Empowerment and Self-Governance Order) awarded the Gilgit-Baltistan a semi-provincial status, giving it self-government.The local government system consists of a three-tier system of districts, tehsils, and union councils, with an elected body at each tier. There are about 130 districts altogether, of which Azad Kashmir has ten and Gilgit-Baltistan seven.\\n\\nLaw enforcement is carried out by a joint network of the intelligence community with jurisdiction limited to the relevant province or territory. The National Intelligence Directorate coordinates the information intelligence at both federal and provincial levels; including the FIA, IB, Motorway Police, and paramilitary forces such as the Pakistan Rangers and the Frontier Corps.Pakistan\\'s \"premier\" intelligence agency, the Inter-Services Intelligence (ISI), was formed just within a year after the Independence of Pakistan in 1947. ABC News Point in 2014 reported that the ISI was ranked as the top intelligence agency in the world while Zee News reported the ISI as ranking fifth among the world\\'s most powerful intelligence agencies.The court system is organised as a hierarchy, with the Supreme Court at the apex, below which are high courts, Federal Shariat Courts (one in each province and one in the federal capital), district courts (one in each district), Judicial Magistrate Courts (in every town and city), Executive Magistrate Courts, and civil courts. The Penal code has limited jurisdiction in the Tribal Areas, where law is largely derived from tribal customs.\\n\\n\\n=== Kashmir conflict ===\\n\\nKashmir, a Himalayan region situated at the northernmost point of the Indian subcontinent, was governed as an autonomous princely state known as Jammu and Kashmir in the British Raj prior to the Partition of India in August 1947. Following the independence of India and Pakistan post-partition, the region became the subject of a major territorial dispute that has hindered their bilateral relations. The two states have engaged each other in two large-scale wars over the region in 1947–1948 and 1965. India and Pakistan have also fought smaller-scale protracted conflicts over the region in 1984 and 1999. Approximately 45.1% of the Kashmir region is controlled by India (administratively split into Jammu and Kashmir and Ladakh), which also claims the entire territory of the former princely state of Jammu and Kashmir that is not under its control. India\\'s control over Jammu and Kashmir and Ladakh as well as its claim to the rest of the region has likewise been contested by Pakistan, which controls approximately 38.2% of the region (administratively split into Azad Jammu and Kashmir and Gilgit−Baltistan) and claims all of the territory under Indian control. Additionally, approximately 20% of the region has been controlled by China (known as Aksai Chin and the Shaksgam Valley) since the Sino-Indian War of 1962 and the Sino-Pakistani Agreement of 1963. The Chinese-controlled areas of Kashmir remain subject to an Indian territorial claim, but are not claimed by Pakistan.\\n\\nIndia claims the entire Kashmir region on the basis of the Instrument of Accession—a legal agreement with the princely state of Jammu and Kashmir that was executed by Hari Singh, the maharaja of the state, who agreed to cede the entire area to newly-independent India. Pakistan claims most of Kashmir on the basis of its Muslim-majority population and of its geography, the same principles that were applied for the creation of the two independent states. India referred the dispute to the United Nations on 1 January 1948. In a resolution passed in 1948, the UN\\'s General Assembly asked Pakistan to remove most of its military troops to set the conditions for the holding of a plebiscite. However, Pakistan failed to vacate the region and a ceasefire was reached in 1949 establishing a ceasefire line known as the Line of Control (LoC) that divided Kashmir between the two states as a de facto border. India, fearful that the Muslim-majority populace of Kashmir would vote to secede from India, did not allow a plebiscite to take place in the region. This was confirmed in a statement by India\\'s Defense Minister, Krishna Menon, who stated: \"Kashmir would vote to join Pakistan and no Indian Government responsible for agreeing to plebiscite would survive.\"Pakistan claims that its position is for the right of the Kashmiri people to determine their future through impartial elections as mandated by the United Nations, while India has stated that Kashmir is an \"integral part\" of India, referring to the 1972 Simla Agreement and to the fact that regional elections take place regularly. In recent developments, certain Kashmiri independence groups believe that Kashmir should be independent of both India and Pakistan.\\n\\n\\n=== Law enforcement ===\\n\\nThe law enforcement in Pakistan is carried out by joint network of several federal and provincial police agencies. The four provinces and the Islamabad Capital Territory (ICT) each have a civilian police force with jurisdiction extending only to the relevant province or territory. At the federal level, there are a number of civilian intelligence agencies with nationwide jurisdictions including the Federal Investigation Agency (FIA) and the Intelligence Bureau (IB), as well as several paramilitary forces such as the National Guards (Northern Areas), the Rangers (Punjab and Sindh), and the Frontier Corps (Khyber Pakhtunkhwa and Balochistan).\\nThe most senior officers of all the civilian police forces also form part of the Police Service, which is a component of the civil service of Pakistan. Namely, there is four provincial police service including the Punjab Police, Sindh Police, Khyber-Pakhtunkhwa Police, and the Balochistan Police; all headed by the appointed senior Inspector-Generals. The ICT has its own police component, the Capital Police, to maintain law and order in the capital. The CID bureaus are the crime investigation unit and form a vital part in each provincial police service.\\nThe law enforcement in Pakistan also has a Motorway Patrol which is responsible for enforcement of traffic and safety laws, security and recovery on Pakistan\\'s inter-provincial motorway network. In each of provincial Police Service, it also maintains a respective Elite Police units led by the NACTA—a counter-terrorism police unit as well as providing VIP escorts. In the Punjab and Sindh, the Pakistan Rangers are an internal security force with the prime objective to provide and maintain security in war zones and areas of conflict as well as maintaining law and order which includes providing assistance to the police. The Frontier Corps serves the similar purpose in Khyber-Pakhtunkhwa, and the Balochistan.\\n\\n\\n=== Human rights ===\\n\\nMale homosexuality is illegal in Pakistan and punishable with up to life in prison. In its 2018 Press Freedom Index, Reporters Without Borders ranked Pakistan number 139 out of 180 countries based on freedom of the press. Television stations and newspapers are routinely shut down for publishing any reports critical of the government or the military.\\n\\n\\n== Military ==\\n\\nThe armed forces of Pakistan are the sixth largest in the world in terms of numbers in full-time service, with about 651,800 personnel on active duty and 291,000 paramilitary personnel, as of tentative estimates in 2021. They came into existence after independence in 1947, and the military establishment has frequently influenced the national politics ever since. Chain of command of the military is kept under the control of the Joint Chiefs of Staff Committee; all of the branches joint works, co-ordination, military logistics, and joint missions are under the Joint Staff HQ. The Joint Staff HQ is composed of the Air HQ, Navy HQ, and Army GHQ in the vicinity of the Rawalpindi Military District.The Chairman Joint Chiefs of Staff Committee is the highest principle staff officer in the armed forces, and the chief military adviser to the civilian government though the chairman has no authority over the three branches of armed forces. The Chairman joint chiefs controls the military from the JS HQ and maintains strategic communications between the military and the civilian government. As of 2021, the CJCSC is General Nadeem Raza alongside chief of army staff General Qamar Javed Bajwa, chief of naval staff Admiral Muhammad Amjad Khan Niazi, and chief of air staff Air Chief Marshal Zaheer Ahmad Babar. The main branches are the Army, the Air Force and the Navy, which are supported by a large number of paramilitary forces in the country. Control over the strategic arsenals, deployment, employment, development, military computers and command and control is a responsibility vested under the National Command Authority which oversaw the work on the nuclear policy as part of the credible minimum deterrence.The United States, Turkey, and China maintain close military relations and regularly export military equipment and technology transfer to Pakistan. Joint logistics and major war games are occasionally carried out by the militaries of China and Turkey. Philosophical basis for the military draft is introduced by the Constitution in times of emergency, but it has never been imposed.\\n\\n\\n=== Military history ===\\nSince 1947 Pakistan has been involved in four conventional wars. The first occurred in Kashmir with Pakistan gaining control of Western Kashmir, (Azad Kashmir and Gilgit-Baltistan), and India retaining Eastern Kashmir (Jammu and Kashmir and Ladakh). Territorial problems eventually led to another conventional war in 1965. The issue of Bengali refugees led to another war in 1971 which resulted in Pakistan\\'s unconditional surrender in East Pakistan. Tensions in Kargil brought the two countries at the brink of war. Since 1947 the unresolved territorial problems with Afghanistan saw border skirmishes which were kept mostly at the mountainous border. In 1961, the military and intelligence community repelled the Afghan incursion in the Bajaur Agency near the Durand Line border.Rising tensions with neighbouring USSR in their involvement in Afghanistan, Pakistani intelligence community, mostly the ISI, systematically coordinated the US resources to the Afghan mujahideen and foreign fighters against the Soviet Union\\'s presence in the region. Military reports indicated that the PAF was in engagement with the Soviet Air Force, supported by the Afghan Air Force during the course of the conflict; one of which belonged to Alexander Rutskoy. Apart from its own conflicts, Pakistan has been an active participant in United Nations peacekeeping missions. It played a major role in rescuing trapped American soldiers from Mogadishu, Somalia, in 1993 in Operation Gothic Serpent. According to UN reports, the Pakistani military is the third largest troop contributor to UN peacekeeping missions after Ethiopia and India.Pakistan has deployed its military in some Arab countries, providing defence, training, and playing advisory roles. The PAF and Navy\\'s fighter pilots have voluntarily served in Arab nations\\' militaries against Israel in the Six-Day War (1967) and in the Yom Kippur War (1973). Pakistan\\'s fighter pilots shot down ten Israeli planes in the Six-Day War. In the 1973 war, one of the PAF pilots, Flt. Lt. Sattar Alvi (flying a MiG-21), shot down an Israeli Air Force Mirage and was honoured by the Syrian government. Requested by the Saudi monarchy in 1979, Pakistan\\'s special forces units, operatives, and commandos were rushed to assist Saudi forces in Mecca to lead the operation of the Grand Mosque. For almost two weeks Saudi Special Forces and Pakistani commandos fought the insurgents who had occupied the Grand Mosque\\'s compound. In 1991, Pakistan became involved with the Gulf War and sent 5,000 troops as part of a US-led coalition, specifically for the defence of Saudi Arabia.Despite the UN arms embargo on Bosnia, General Javed Nasir of the ISI airlifted anti-tank weapons and missiles to Bosnian mujahideen which turned the tide in favour of Bosnian Muslims and forced the Serbs to lift the siege. Under Nasir\\'s leadership the ISI was also involved in supporting Chinese Muslims in Xinjiang Province, rebel Muslim groups in the Philippines, and some religious groups in Central Asia.Since 2004, the military has been engaged in an insurgency in the Khyber Pakhtunkhwa province, mainly against the Tehrik-i-Taliban factions. Major operations undertaken by the army include Operation Black Thunderstorm, Operation Rah-e-Nijat and Operation Zarb-e-Azb.According to SIPRI, Pakistan was the 9th-largest recipient and importer of arms between 2012 and 2016.\\n\\n\\n== Economy ==\\n\\nThe Economy of Pakistan is the 23rd-largest in the world in terms of purchasing power parity (PPP), and 42nd-largest in terms of nominal gross domestic product. Economists estimate that Pakistan was part of the wealthiest region of the world throughout the first millennium CE, with the largest economy by GDP. This advantage was lost in the 18th century as other regions such as China and Western Europe edged forward. Pakistan is considered a developing country and is one of the Next Eleven, a group of eleven countries that, along with the BRICs, have a high potential to become the world\\'s largest economies in the 21st century.\\nIn recent years, after decades of social instability, as of 2013, serious deficiencies in macromanagement and unbalanced macroeconomics in basic services such as rail transportation and electrical energy generation have developed. The economy is considered to be semi-industrialized, with centres of growth along the Indus River. The diversified economies of Karachi and Punjab\\'s urban centres coexist with less-developed areas in other parts of the country, particularly in Balochistan. According to the Economic complexity index, Pakistan is the 67th-largest export economy in the world and the 106th most complex economy. During the fiscal year 2015–16, Pakistan\\'s exports stood at US$20.81 billion and imports at US$44.76 billion, resulting in a negative trade balance of US$23.96 billion.\\n\\nAs of 2019, Pakistan\\'s estimated nominal GDP is US$284.2 billion. The GDP by PPP is US$1.254 trillion. The estimated nominal per capita GDP is US$1,388, the GDP (PPP)/capita is US$6,016 (international dollars), According to the World Bank, Pakistan has important strategic endowments and development potential. The increasing proportion of Pakistan\\'s youth provides the country with both a potential demographic dividend and a challenge to provide adequate services and employment. 21.04% of the population live below the international poverty line of US$1.25 a day. The unemployment rate among the aged 15 and over population is 5.5%. Pakistan has an estimated 40 million middle class citizens, projected to increase to 100 million by 2050. A 2015 report published by the World Bank ranked Pakistan\\'s economy at 24th-largest in the world by purchasing power and 41st-largest in absolute terms. It is South Asia\\'s second-largest economy, representing about 15.0% of regional GDP.\\nPakistan\\'s economic growth since its inception has been varied. It has been slow during periods of democratic transition, but robust during the three periods of martial law, although the foundation for sustainable and equitable growth was not formed. The early to middle 2000s was a period of rapid economic reforms; the government raised development spending, which reduced poverty levels by 10% and increased GDP by 3%. The economy cooled again from 2007. Inflation reached 25.0% in 2008, and Pakistan had to depend on a fiscal policy backed by the International Monetary Fund to avoid possible bankruptcy. A year later, the Asian Development Bank reported that Pakistan\\'s economic crisis was easing. The inflation rate for the fiscal year 2010–11 was 14.1%. Since 2013, as part of an International Monetary Fund program, Pakistan\\'s economic growth has picked up. In 2014 Goldman Sachs predicted that Pakistan\\'s economy would grow 15 times in the next 35 years to become the 18th-largest economy in the world by 2050. In his 2016 book, The Rise and Fall of Nations, Ruchir Sharma termed Pakistan\\'s economy as at a \\'take-off\\' stage and the future outlook until 2020 has been termed \\'Very Good\\'. Sharma termed it possible to transform Pakistan from a \"low-income to a middle-income country during the next five years\".\\nPakistan is one of the largest producers of natural commodities, and its labour market is the 10th-largest in the world. The 7-million–strong Pakistani diaspora contributed US$19.9 billion to the economy in 2015–16. The major source countries of remittances to Pakistan are: the UAE; the United States; Saudi Arabia; the Gulf states (Bahrain, Kuwait, Qatar, and Oman); Australia; Canada; Japan; the United Kingdom; Norway; and Switzerland. According to the World Trade Organization, Pakistan\\'s share of overall world exports is declining; it contributed only 0.13% in 2007.\\n\\n\\n=== Agriculture and primary sector ===\\n\\nThe structure of the Pakistani economy has changed from a mainly agricultural to a strong service base. Agriculture as of 2015 accounts for only 20.9% of the GDP. Even so, according to the United Nations Food and Agriculture Organization, Pakistan produced 21,591,400 metric tons of wheat in 2005, more than all of Africa (20,304,585 metric tons) and nearly as much as all of South America (24,557,784 metric tons). Majority of the population, directly or indirectly, is dependent on this sector. It accounts for 43.5% of employed labour force and is the largest source of foreign exchange earnings.A large portion of the country\\'s manufactured exports is dependent on raw materials such as cotton and hides that are part of the agriculture sector, while supply shortages and market disruptions in farm products do push up inflationary pressures. The country is also the fifth-largest producer of cotton, with cotton production of 14 million bales from a modest beginning of 1.7 million bales in the early 1950s; is self-sufficient in sugarcane; and is the fourth-largest producer in the world of milk. Land and water resources have not risen proportionately, but the increases have taken place mainly due to gains in labour and agriculture productivity. The major breakthrough in crop production took place in the late 1960s and 1970s due to the Green Revolution that made a significant contribution to land and yield increases of wheat and rice. Private tube wells led to a 50 percent increase in the cropping intensity which was augmented by tractor cultivation. While the tube wells raised crop yields by 50 percent, the High Yielding Varieties (HYVs) of wheat and rice led to a 50–60 percent higher yield. Meat industry accounts for 1.4 percent of overall GDP.\\n\\n\\n=== Industry ===\\n\\nIndustry is the second-largest sector of the economy, accounting for 19.74% of gross domestic product (GDP), and 24 percent of total employment. Large-scale manufacturing (LSM), at 12.2% of GDP, dominates the overall sector, accounting for 66% of the sectoral share, followed by small-scale manufacturing, which accounts for 4.9% of total GDP. Pakistan\\'s cement industry is also fast growing mainly because of demand from Afghanistan and from the domestic real estate sector. In 2013 Pakistan exported 7,708,557 metric tons of cement. Pakistan has an installed capacity of 44,768,250 metric tons of cement and 42,636,428 metric tons of clinker. In 2012 and 2013, the cement industry in Pakistan became the most profitable sector of the economy.The textile industry has a pivotal position in the manufacturing sector of Pakistan. In Asia, Pakistan is the eighth-largest exporter of textile products, contributing 9.5% to the GDP and providing employment to around 15 million people (some 30% of the 49 million people in the workforce). Pakistan is the fourth-largest producer of cotton with the third-largest spinning capacity in Asia after China and India, contributing 5% to the global spinning capacity. China is the second largest buyer of Pakistani textiles, importing US$1.527 billion of textiles last fiscal. Unlike the US, where mostly value-added textiles are imported, China buys only cotton yarn and cotton fabric from Pakistan. In 2012, Pakistani textile products accounted for 3.3% or US$1.07bn of all UK textile imports, 12.4% or $4.61bn of total Chinese textile imports, 3.0% of all US textile imports ($2,980 million), 1.6% of total German textile imports ($880 million) and 0.7% of total Indian textile imports ($888 million).\\n\\n\\n=== Services ===\\n\\nThe services sector makes up 58.8% of GDP and has emerged as the main driver of economic growth. Pakistani society like other developing countries is a consumption oriented society, having a high marginal propensity to consume. The growth rate of services sector is higher than the growth rate of agriculture and industrial sector. Services sector accounts for 54 percent of GDP in 2014 and little over one-third of total employment. Services sector has strong linkages with other sectors of economy; it provides essential inputs to agriculture sector and manufacturing sector. Pakistan\\'s I.T sector is regarded as among the fastest growing sector\\'s in Pakistan. The World Economic Forum, assessing the development of Information and Communication Technology in the country ranked Pakistan 110th among 139 countries on the \\'Networked Readiness Index 2016\\'.As of May 2020, Pakistan has about 82 million internet users, making it the 9th-largest population of Internet users in the world. The current growth rate and employment trend indicate that Pakistan\\'s Information Communication Technology (ICT) industry will exceed the $10-billion mark by 2020. The sector employees 12,000 and count\\'s among top five freelancing nations. The country has also improved its export performance in telecom, computer and information services, as the share of their exports surged from 8.2pc in 2005–06 to 12.6pc in 2012–13. This growth is much better than that of China, whose share in services exports was 3pc and 7.7pc for the same period respectively.\\n\\n\\n=== Tourism ===\\n\\nWith its diverse cultures, people, and landscapes, Pakistan attracted around 6.6 million foreign tourists in 2018, which represented a significant decline since the 1970s when the country received unprecedented numbers of foreign tourists due to the popular Hippie trail. The trail attracted thousands of Europeans and Americans in the 1960s and 1970s who travelled via land through Turkey and Iran into India through Pakistan. The main destinations of choice for these tourists were the Khyber Pass, Peshawar, Karachi, Lahore, Swat and Rawalpindi. The numbers following the trail declined after the Iranian Revolution and the Soviet–Afghan War.Pakistan\\'s tourist attractions range from the mangroves in the south to the Himalayan hill stations in the north-east. The country\\'s tourist destinations range from the Buddhist ruins of Takht-i-Bahi and Taxila, to the 5,000-year-old cities of the Indus Valley civilization such as Mohenjo-daro and Harappa. Pakistan is home to several mountain peaks over 7,000 metres (23,000 feet). The northern part of Pakistan has many old fortresses, examples of ancient architecture, and the Hunza and Chitral valleys, home to the small pre-Islamic Kalasha community claiming descent from Alexander the Great. Pakistan\\'s cultural capital, Lahore, contains many examples of Mughal architecture such as the Badshahi Masjid, the Shalimar Gardens, the Tomb of Jahangir, and the Lahore Fort.\\nIn October 2006, just one year after the 2005 Kashmir earthquake, The Guardian released what it described as \"The top five tourist sites in Pakistan\" in order to help the country\\'s tourism industry. The five sites included Taxila, Lahore, the Karakoram Highway, Karimabad, and Lake Saiful Muluk. To promote Pakistan\\'s unique cultural heritage, the government organises various festivals throughout the year. In 2015, the World Economic Forum\\'s Travel & Tourism Competitiveness Report ranked Pakistan 125 out of 141 countries.\\n\\n\\n== Infrastructure ==\\n\\nPakistan was recognised as the best country for infrastructure development in South Asia during the IWF and World Bank annual meetings in 2016.\\n\\n\\n=== Nuclear power and energy ===\\n\\nAs of May 2021, nuclear power is provided by six licensed commercial nuclear power plants. The Pakistan Atomic Energy Commission (PAEC) is solely responsible for operating these power plants, while the Pakistan Nuclear Regulatory Authority regulates safe usage of the nuclear energy. The electricity generated by commercial nuclear power plants constitutes roughly 5.8% of Pakistan\\'s electrical energy, compared to 64.2% from fossil fuels (crude oil and natural gas), 29.9% from hydroelectric power, and 0.1% from coal. Pakistan is one of the four nuclear armed states (along with India, Israel, and North Korea) that is not a party to the Nuclear Non-Proliferation Treaty, but it is a member in good standing of the International Atomic Energy Agency.\\nThe KANUPP-I, a Candu-type nuclear reactor, was supplied by Canada in 1971—the country\\'s first commercial nuclear power plant. The Sino-Pakistani nuclear cooperation began in the early 1980s. After a Sino-Pakistani nuclear cooperation agreement in 1986, China provided Pakistan with a nuclear reactor dubbed CHASNUPP-I for energy and the industrial growth of the country. In 2005 both countries proposed working on a joint energy security plan, calling for a huge increase in generation capacity to more than 160,000 MWe by 2030. Under its Nuclear Energy Vision 2050, the Pakistani government plans to increase nuclear power generation capacity to 40,000 MWe, 8,900 MWe of it by 2030. In June 2008 the nuclear commercial complex was expanded with the ground work of installing and operationalising the Chashma-III and Chashma–IV reactors at Chashma, Punjab Province, each with 325–340 MWe and costing ₨ 129 billion; from which the ₨ 80 billion came from international sources, principally China. A further agreement for China\\'s help with the project was signed in October 2008, and given prominence as a counter to the US–India agreement that shortly preceded it. The cost quoted then was US$1.7 billion, with a foreign loan component of US$1.07 billion. In 2013 Pakistan established a second commercial nuclear complex in Karachi with plans of additional reactors, similar to the one in Chashma. The electrical energy is generated by various energy corporations and evenly distributed by the National Electric Power Regulatory Authority (NEPRA) among the four provinces. However, the Karachi-based K-Electric and the Water and Power Development Authority (WAPDA) generates much of the electrical energy used in Pakistan in addition to gathering revenue nationwide. In 2014, Pakistan had an installed electricity generation capacity of ~22,797MWt.\\n\\n\\n=== Transport ===\\n\\nThe transport industry accounts for ~10.5% of the nation\\'s GDP.\\n\\n\\n==== Motorways ====\\n\\nMotorways of Pakistan are a network of multiple-lane, high-speed, controlled-access highways in Pakistan, which are owned, maintained, and operated federally by Pakistan\\'s National Highway Authority. As of 20 February 2020, 1882 km of motorways are operational, while an additional 1854 km are under construction or planned. All motorways in Pakistan are pre-fixed with the letter \\'M\\' (for \"Motorway\") followed by the unique numerical designation of the specific highway (with a hyphen in the middle), e.g. \"M-1\".Pakistan\\'s motorways are an important part of Pakistan\\'s \"National Trade Corridor Project\", which aims to link Pakistan\\'s three Arabian Sea ports (Karachi Port, Port Bin Qasim and Gwadar Port) to the rest of the country through its national highways and motorways network and further north with Afghanistan, Central Asia and China. The project was planned in 1990. The China Pakistan Economic Corridor project aims to link Gwadar Port and Kashgar (China) using Pakistani motorways, national highways, and expressways.\\n\\n\\n==== Highways ====\\n\\nHighways form the backbone of Pakistan\\'s transport system; a total road length of 263,942 kilometres (164,006 miles) accounts for 92% of passengers and 96% of inland freight traffic. Road transport services are largely in the hands of the private sector. The National Highway Authority is responsible for the maintenance of national highways and motorways. The highway and motorway system depends mainly on north–south links connecting the southern ports to the populous provinces of Punjab and Khyber-Pakhtunkhwa. Although this network only accounts for 4.6% of total road length, it carries 85% of the country\\'s traffic.\\n\\n\\n==== Railways ====\\n\\nThe Pakistan Railways, under the Ministry of Railways (MoR), operates the railroad system. From 1947 until the 1970s the train system was the primary means of transport until the nationwide constructions of the national highways and the economic boom of the automotive industry. Beginning in the 1990s there was a marked shift in traffic from rail to highways; dependence grew on roads after the introduction of vehicles in the country. Now the railway\\'s share of inland traffic is below 8% for passengers and 4% for freight traffic. As personal transportation began to be dominated by the automobile, total rail track decreased from 8,775 kilometres (5,453 miles) in 1990–91 to 7,791 kilometres (4,841 miles) in 2011. Pakistan expects to use the rail service to boost foreign trade with China, Iran, and Turkey.\\n\\n\\n==== Airports ====\\n\\nThere are an estimated 151 airports and airfields in Pakistan as of 2013—including both the military and the mostly publicly owned civilian airports. Although Jinnah International Airport is the principal international gateway to Pakistan, the international airports in Lahore, Islamabad, Peshawar, Quetta, Faisalabad, Sialkot, and Multan also handle significant amounts of traffic.\\nThe civil aviation industry is mixed with public and private sectors, which was deregulated in 1993. While the state-owned Pakistan International Airlines (PIA) is the major and dominant air carrier that carries about 73% of domestic passengers and all domestic freight, the private airlines such as airBlue and Air Indus, also provide similar services at a low cost.\\n\\n\\n==== Seaports ====\\n\\nMajor seaports are in Karachi, Sindh (the Karachi port, Port Qasim). Since the 1990s some seaport operations have been moved to Balochistan with the construction of Gwadar Port, Port of Pasni and Gadani Port. Gwadar Port is the deepest sea port of the world. According to the WEF\\'s Global Competitiveness Report, quality ratings of Pakistan\\'s port infrastructure increased from 3.7 to 4.1 between 2007 and 2016.\\n\\n\\n==== Metro ====\\n\\n\\n===== Metro Train =====\\n\\nThe Orange Line Metro Train is an automated rapid transit system in Lahore. The Orange line is the first of the three proposed rail lines part for the Lahore Metro. The line spans 27.1 km (16.8 mi) with 25.4 km (15.8 mi) elevated and 1.72 km (1.1 mi) underground and has a cost of 251.06 billion Rupees ($1.6 billion). The line consists of 26 subway stations and is designed to carry over 250,000 passengers daily. The line became operational on 25 October 2020.\\n\\n\\n===== Metro Bus and BRTs =====\\nLahore Metrobus is a bus rapid transit service operating in the city of Lahore. The Metrobus network\\'s first phase was opened in February 2013. It was the first Metro bus system in Pakistan.\\nRawalpindi-Islamabad Metrobus is a 48.1 km (29.9 mi) bus rapid transit system operating in the Islamabad-Rawalpindi metropolitan area. The Metrobus network\\'s first phase was opened on 4 June 2015, and stretches 22.5 kilometres between Pak Secretariat, in Islamabad, and Saddar in Rawalpindi. The second stage stretches 25.6 kilometres between the Peshawar Morr Interchange and New Islamabad International Airport and was inaugurated on 18th April 2022. The system uses e-ticketing and an Intelligent Transportation System and is managed by the Punjab Mass Transit Authority.\\nMultan Metrobus is a bus rapid transit (BRT) system in Multan. Construction on the line began in May 2015, while operations commenced on 24 January 2017.\\nPeshawar Bus Rapid Transit (Peshawar BRT) is a bus rapid transit system in Peshawar, capital of Khyber Pakhtunkhwa province. The construction of the project was started in October 2017 and was inaugurated on 13 August 2020, it is the fourth BRT system in Pakistan.\\nGreen Line Metrobus is the first phase of Karachi Metrobus that has been operational since 25 December 2021. The Government of Pakistan financed the majority of the project. Construction of the Green Line began on 26 February 2016.\\nFaisalabad shuttle train service and Faisalabad Metrobus are the proposed rapid transit projects in the city of Faisalabad. These projects are the part of a mega-project of China–Pakistan Economic Corridor.\\n\\n\\n===== Other Systems =====\\nKarachi Circular Railway is a partially active regional public transit system in Karachi, which serves the Karachi metropolitan area. KCR was fully operational between 1969 and 1999. Since 2001, restoration of the railway and restarting the system had been sought. In November 2020, the KCR partially revived operations.\\nA tramway service was started in 1884 in Karachi but was closed in 1975 due to various factors. The Sindh Government is planning to restart the tramway services in the city, collaborating with Austrian experts.\\nIn October 2019, a project for the construction of tramway service in Lahore has also been signed by the Punjab Government. This project will be launched under public-private partnership in a joint venture of European and Chinese companies along with the Punjab transport department.\\n\\n\\n==== Flyovers and underpasses ====\\n\\nMany flyovers and underpasses are located in major urban areas of the country to segregate the flow of traffic. The highest number of flyovers and under passes are located in Karachi, followed by Lahore. Other cities having flyovers and underpasses for the regulation of flow of traffic includes Islamabad-Rawalpindi, Faisalabad, Gujranwala, Multan, Peshawar, Hyderabad, Quetta, Sargodha, Bahawalpur, Sukkur, Larkana, Rahim Yar Khan and Sahiwal etc.Beijing Underpass, Lahore is the longest underpass of Pakistan with a length of about 1.3 km (0.81 mi). Muslim Town Flyover, Lahore is the longest flyover of the country with a length of about 2.6 km (1.6 mi).\\n\\n\\n=== Science and technology ===\\n\\nDevelopments in science and technology have played an important role in Pakistan\\'s infrastructure and helped the country connect to the rest of the world. Every year, scientists from around the world are invited by the Pakistan Academy of Sciences and the Pakistan Government to participate in the International Nathiagali Summer College on Physics. Pakistan hosted an international seminar on \"Physics in Developing Countries\" for the International Year of Physics 2005. The Pakistani theoretical physicist Abdus Salam won a Nobel Prize in Physics for his work on the electroweak interaction. Influential publications and critical scientific work in the advancement of mathematics, biology, economics, computer science, and genetics have been produced by Pakistani scientists at both the domestic and international levels.In chemistry, Salimuzzaman Siddiqui was the first Pakistani scientist to bring the therapeutic constituents of the neem tree to the attention of natural products chemists. Pakistani neurosurgeon Ayub Ommaya invented the Ommaya reservoir, a system for treatment of brain tumours and other brain conditions. Scientific research and development play a pivotal role in Pakistani universities, government- sponsored national laboratories, science parks, and the industry. Abdul Qadeer Khan, regarded as the founder of the HEU-based gas-centrifuge uranium enrichment program for Pakistan\\'s integrated atomic bomb project. He founded and established the Kahuta Research Laboratories (KRL) in 1976, serving as both its senior scientist and the Director-General until his retirement in 2001, and he was an early and vital figure in other science projects. Apart from participating in Pakistan\\'s atomic bomb project, he made major contributions in molecular morphology, physical martensite, and its integrated applications in condensed and material physics.In 2010 Pakistan was ranked 43rd in the world in terms of published scientific papers. The Pakistan Academy of Sciences, a strong scientific community, plays an influential and vital role in formulating recommendations regarding science policies for the government. Pakistan was ranked 99th in the Global Innovation Index in 2021, up from 107th in 2020.\\nThe 1960s saw the emergence of an active space program led by SUPARCO that produced advances in domestic rocketry, electronics, and aeronomy. The space program recorded a few notable feats and achievements. The successful launch of its first rocket into space made Pakistan the first South Asian country to have achieved such a task. Successfully producing and launching the nation\\'s first space satellite in 1990, Pakistan became the first Muslim country and second South Asian country to put a satellite into space.As an aftermath of the 1971 war with India, the clandestine crash program developed atomic weapons partly motivated by fear and to prevent any foreign intervention, while ushering in the atomic age in the post cold war era. Competition with India and tensions eventually led to Pakistan\\'s decision to conduct underground nuclear tests in 1998, thus becoming the seventh country in the world to successfully develop nuclear weapons.Pakistan is the first and only Muslim country that maintains an active research presence in Antarctica. Since 1991 Pakistan has maintained two summer research stations and one weather observatory on the continent and plans to open another full-fledged permanent base in Antarctica.Energy consumption by computers and usage has grown since the 1990s when PCs were introduced; Pakistan has about 82 million Internet users and is ranked as one of the top countries that have registered a high growth rate in Internet penetration as of 2020. Key publications have been produced by Pakistan, and domestic software development has gained considerable international praise.As of May 2020, Pakistan has about 82 million internet users, making it the 9th-largest population of Internet users in the world. Since the 2000s Pakistan has made a significant amount of progress in supercomputing, and various institutions offer research opportunities in parallel computing. The Pakistan government reportedly spends ₨ 4.6 billion on information technology projects, with emphasis on e-government, human resources, and infrastructure development.\\n\\n\\n=== Education ===\\n\\nThe constitution of Pakistan requires the state to provide free primary and secondary education. \\nAt the time of the establishment of Pakistan as a state, the country had only one university, Punjab University in Lahore. Very soon the Pakistan government established public universities in each of the four provinces, including Sindh University (1949), Peshawar University (1950), Karachi University (1953), and Balochistan University (1970). Pakistan has a large network of both public and private universities, which includes collaboration between the universities aimed at providing research and higher education opportunities in the country, although there is concern about the low quality of teaching in many of the newer schools. It is estimated that there are 3,193 technical and vocational institutions in Pakistan, and there are also madrassahs that provide free Islamic education and offer free board and lodging to students, who come mainly from the poorer strata of society. Strong public pressure and popular criticism over extremists\\' usage of madrassahs for recruitment, the Pakistan government has made repeated efforts to regulate and monitor the quality of education in the madrassahs.\\n\\nEducation in Pakistan is divided into six main levels: nursery (preparatory classes); primary (grades one through five); middle (grades six through eight); matriculation (grades nine and ten, leading to the secondary certificate); intermediate (grades eleven and twelve, leading to a higher secondary certificate); and university programmes leading to graduate and postgraduate degrees. There is a network of private schools that constitutes a parallel secondary education system based on a curriculum set and administered by the Cambridge International Examinations of the United Kingdom. Some students choose to take the O-level and A level exams conducted by the British Council. According to the International Schools Consultancy, Pakistan has 439 international schools.\\n\\nAs a result of initiatives taken in 2007, the English medium education has been made compulsory in all schools across the country. In 2012, Malala Yousafzai, a campaigner for female education, was shot by a Taliban gunman in retaliation for her activism. Yousafzai went on to become the youngest ever Nobel laureate for her global education-related advocacy. Additional reforms enacted in 2013 required all educational institutions in Sindh to begin offering Chinese language courses, reflecting China\\'s growing role as a superpower and its increasing influence in Pakistan. The literacy rate of the population is 62.3% as of 2018. The rate of male literacy is 72.5% while the rate of female literacy is 51.8%. Literacy rates vary by region and particularly by sex; as one example, in tribal areas female literacy is 9.5%, while Azad Jammu & Kashmir has a literacy rate of 74%. With the advent of computer literacy in 1995, the government launched a nationwide initiative in 1998 with the aim of eradicating illiteracy and providing a basic education to all children. Through various educational reforms, by 2015 the Ministry of Education expected to attain 100% enrollment levels among children of primary school age and a literacy rate of ~86% among people aged over 10. Pakistan is currently spending 2.3 percent of its GDP on education; which according to the Institute of Social and Policy Sciences is one of the lowest in South Asia.\\n\\n\\n== Demographics ==\\n\\n\\n=== Languages ===\\n\\nMore than sixty languages are spoken in Pakistan, including a number of provincial languages. Urdu—the lingua franca and a symbol of Muslim identity and national unity—is the national language and understood by over 75% of Pakistanis. It is the main medium of communication in the country, but the primary language of only 7% of the population. Urdu and English are the official languages of Pakistan. English is primarily used in official business and government, and in legal contracts; the local variety is known as Pakistani English. Punjabi, the most common language and the first language of 38.78% of the population, is mostly spoken in the Punjab. Saraiki is mainly spoken in South Punjab, and Hindko is predominant in the Hazara region of Khyber Pakhtunkhwa. Pashto is the provincial language of Khyber Pakhtunkhwa. Sindhi is commonly spoken in Sindh, while Balochi is dominant in Balochistan. Brahui, a Dravidian language, is spoken by the Brahui people who live in Balochistan. There are also speakers of Gujarati in Karachi. Marwari, a Rajasthani language, is also spoken in parts of Sindh. Various languages such as Shina, Balti, and Burushaski are spoken in Gilgit-Baltistan, whilst languages such as Pahari, Gojri, and Kashmiri are spoken by many in Azad Kashmir.\\nArabic is officially recognised by the constitution of Pakistan. It declares in article 31 No. 2 that \"The State shall endeavour, as respects the Muslims of Pakistan (a) to make the teaching of the Holy Quran and Islamiat compulsory, to encourage and facilitate the learning of Arabic language ...\"\\n\\n\\n=== Immigration ===\\n\\nEven after partition in 1947, Indian Muslims continued to migrate to Pakistan throughout the 1950s and 1960s, and these migrants settled mainly in Karachi and other towns of Sindh province. The wars in neighboring Afghanistan during the 1980s and 1990s also forced millions of Afghan refugees into Pakistan. The Pakistan census excludes the 1.41 million registered refugees from Afghanistan, who are found mainly in the Khyber-Pakhtunkhwa and tribal belt, with small numbers residing in Karachi and Quetta. Pakistan is home to one of the world\\'s largest refugee populations. In addition to Afghans, around 2 million Bangladeshis and half a million other undocumented people live in Pakistan. They are claimed to be from other areas such as Myanmar, Iran, Iraq, and Africa.Experts say that the migration of both Bengalis and Burmese (Rohingya) to Pakistan started in the 1980s and continued until 1998. Shaikh Muhammad Feroze, the chairman of the Pakistani Bengali Action Committee, claims that there are 200 settlements of Bengali-speaking people in Pakistan, of which 132 are in Karachi. They are also found in various other areas of Pakistan such as Thatta, Badin, Hyderabad, Tando Adam, and Lahore. Large-scale Rohingya migration to Karachi made that city one of the largest population centres of Rohingyas in the world after Myanmar. The Burmese community of Karachi is spread out over 60 of the city\\'s slums such as the Burmi Colony in Korangi, Arakanabad, Machchar colony, Bilal colony, Ziaul Haq Colony, and Godhra Camp.Thousands of Uyghur Muslims have also migrated to the Gilgit-Baltistan region of Pakistan, fleeing religious and cultural persecution in Xinjiang, China. Since 1989 thousands of Kashmiri Muslim refugees have sought refuge in Pakistan, complaining that many of the refugee women had been raped by Indian soldiers and that they were forced out of their homes by the soldiers.\\n\\n\\n=== Ethnic groups ===\\n\\nThe major ethnic groups are Punjabis (44.7% of the country\\'s population), Pashtuns, also known as Pathans (15.4%), Sindhis (14.1%), Saraikis (8.4%), Muhajirs (the Indian emigrants, mostly Urdu-speaking), who make up 7.6% of the population, and the Baloch with 3.6%. The remaining 6.3% consist of a number of ethnic minorities such as the Brahuis, the Hindkowans, the various peoples of Gilgit-Baltistan, the Kashmiris, the Sheedis (who are of African descent), and the Hazaras. There is also a large Pakistani diaspora worldwide, numbering over seven million, which has been recorded as the sixth largest diaspora in the world.\\n\\n\\n=== Urbanisation ===\\n\\nSince achieving independence as a result of the partition of India, the urbanisation has increased exponentially, with several different causes. The majority of the population in the south resides along the Indus River, with Karachi the most populous commercial city. In the east, west, and north, most of the population lives in an arc formed by the cities of Lahore, Faisalabad, Rawalpindi, Islamabad, Sargodha, Gujranwala, Sialkot, Gujrat, Jhelum, Sheikhupura, Nowshera, Mardan, and Peshawar. During the period 1990–2008, city dwellers made up 36% of Pakistan\\'s population, making it the most urbanised nation in South Asia. Furthermore, more than 50% of Pakistanis live in towns of 5,000 people or more. Immigration, from both within and outside the country, is regarded as one of the main factors contributing to urbanisation in Pakistan. One analysis of the 1998 national census highlighted the significance of the partition of India in the 1940s as it relates to urban change in Pakistan.\\nDuring and after the independence period, Urdu speaking Muslims from India migrated in large numbers to Pakistan, especially to the port city of Karachi, which is today the largest metropolis in Pakistan. Migration from other countries, mainly from those nearby, has further accelerated the process of urbanisation in Pakistani cities. Inevitably, the rapid urbanisation caused by these large population movements has also created new political and socio-economic challenges. In addition to immigration, economic trends such as the green revolution and political developments, among a host of other factors, are also important causes of urbanisation.\\n\\n\\n=== Religion ===\\n\\nThe state religion in Pakistan is Islam. Freedom of religion is guaranteed by the Constitution of Pakistan, which provides all its citizens the right to profess, practice and propagate their religion subject to law, public order, and morality.The majority of Pakistanis are Muslims (96.47%) followed by Hindus (2.14%) and Christians (1.27%). There are also people in Pakistan who follow other religions, such as Sikhism, Buddhism, Jainism and the minority of Parsi (who follow Zoroastrianism). The Kalash people maintain a unique identity and religion within Pakistan.Hinduism is mostly associated with Sindhis, and Pakistan hosts major events such as the Hinglaj Yatra pilgrimage. Hindu temples may be found throughout Sindh, where the dharma features prominently. Many Hindus in Pakistan complain about the prospect of religious violence against them and being treated like second-class citizens, and many have emigrated to India or further abroad.In addition, some Pakistanis also do not profess any faith (such as atheists and agnostics) in Pakistan. According to the 1998 census, people who did not state their religion accounted for 0.5% of the population.\\n\\n\\n==== Islam ====\\n\\nIslam is the dominant religion. About 96.47% of Pakistanis are Muslim, according to the 2017 Census. Pakistan has the second-largest number of Muslims in the world after Indonesia. and home for (10.5%) of the world\\'s Muslim population. The majority of them are Sunni and mostly follow Sufism (estimated between 75 and 95%) while Shias represent between 5–25%. In 2019, the Shia population in Pakistan was estimated to be 42 million out of total population of 210 million. Pakistan also has the largest Muslim city in the world (Karachi).\\n\\nThe Ahmadis, a small minority representing 0.22–2% of Pakistan\\'s population, are officially considered non-Muslims by virtue of the constitutional amendment. The Ahmadis are particularly persecuted, especially since 1974 when they were banned from calling themselves Muslims. In 1984, Ahmadiyya places of worship were banned from being called \"mosques\". As of 2012, 12% of Pakistani Muslims self-identify as non-denominational Muslims. There are also several Quraniyoon communities. They are mainly concentratd in the Lalian Tehsil, Chiniot District, where approximately 13% of the population.Sufism, a mystical Islamic tradition, has a long history and a large following among the Sunni Muslims in Pakistan, at both the academic and popular levels. Popular Sufi culture is centered around gatherings and celebrations at the shrines of saints and annual festivals that feature Sufi music and dance. Two Sufis whose shrines receive much national attention are Ali Hajweri in Lahore (c. 12th century) and Shahbaz Qalander in Sehwan, Sindh (c. 12th century).There are two levels of Sufism in Pakistan. The first is the \\'populist\\' Sufism of the rural population. This level of Sufism involves belief in intercession through saints, veneration of their shrines, and forming bonds (Mureed) with a pir (saint). Many rural Pakistani Muslims associate with pirs and seek their intercession. The second level of Sufism in Pakistan is \\'intellectual Sufism\\', which is growing among the urban and educated population. They are influenced by the writings of Sufis such as the medieval theologian al-Ghazali, the Sufi reformer Shaykh Aḥmad Sirhindi, and Shah Wali Allah. Contemporary Islamic fundamentalists criticise Sufism\\'s popular character, which in their view does not accurately reflect the teachings and practice of Muhammad and his companions.\\n\\n\\n==== Hinduism ====\\n\\nHinduism is the second-largest religion in Pakistan after Islam and is followed by 2.14% of the population according to the 2017 census. According to the 2010 Pew report, Pakistan had the fifth-largest Hindu population in the world. In the 2017 census, the Hindu population was found to be 4,444,437. Hindus are found in all provinces of Pakistan but are mostly concentrated in Sindh, where they account for 8.73% of the population. Umerkot district (52.15%) is the only Hindu majority district in Pakistan. Tharparkar district has the highest population of Hindus in terms of absolute terms. The four districts in Sindh- Umerkot, Tharparkar, Mirpurkhas and Sanghar hosts more than half of the Hindu population in Pakistan.\\nAt the time of Pakistan\\'s creation, the \\'hostage theory\\' gained currency. According to this theory, the Hindu minority in Pakistan was to be given a fair deal in Pakistan in order to ensure the protection of the Muslim minority in India. However, Khawaja Nazimuddin, the second Prime Minister of Pakistan, stated:I do not agree that religion is a private affair of the individual nor do I agree that in an Islamic state every citizen has identical rights, no matter what his caste, creed or faith be. Some Hindus in Pakistan feel that they are treated as second-class citizens and many have continued to migrate to India. Pakistani Hindus faced riots after the Babri Masjid demolition and have experienced other attacks, forced conversions, and abductions.\\n\\n\\n==== Christianity and other religions ====\\n\\nChristians formed the next largest religious minority after Hindus, with 1.27% of the population following it. The highest concentration of Christians in Pakistan is in Lahore District (5%) in Punjab province and in Islamabad Capital Territory (over 4% Christian). There is a Roman Catholic community in Karachi that was established by Goan and Tamil migrants when Karachi\\'s infrastructure was being developed by the British during the colonial administration between World War I and World War II.They are followed by the Bahá\\'í Faith, which had a following of 30,000, then Sikhism, Buddhism, and Zoroastrianism, each back then claiming 20,000 adherents, and a very small community of Jains.\\n1.0% of the population identified as atheist in 2005. However, the figure rose to 2.0% in 2012 according to Gallup.\\n\\n\\n== Culture and society ==\\n\\nCivil society in Pakistan is largely hierarchical, emphasising local cultural etiquette and traditional Islamic values that govern personal and political life. The basic family unit is the extended family, although for socio-economic reasons there has been a growing trend towards nuclear families. The traditional dress for both men and women is the Shalwar Kameez; trousers, jeans, and shirts are also popular among men. In recent decades, the middle class has increased to around 35 million and the upper and upper-middle classes to around 17 million, and power is shifting from rural landowners to the urbanised elites. Pakistani festivals, including Eid-ul-Fitr, Eid-ul-Azha, Ramazan, Christmas, Easter, Holi, and Diwali, are mostly religious in origin. Increasing globalisation has resulted in Pakistan ranking 56th on the A.T. Kearney/FP Globalization Index.\\n\\n\\n=== Clothing, arts, and fashion ===\\n\\nThe Shalwar Kameez is the national dress of Pakistan and is worn by both men and women in all four provinces: Punjab, Sindh, Balochistan, and Khyber-Pakhtunkhwa, and Azad Kashmir. Each province has its own style of Shalwar Kameez. Pakistanis wear clothes in a range of exquisite colours and designs and in type of fabric (silk, chiffon, cotton, etc.). Besides the national dress, domestically tailored suits and neckties are often worn by men, and are customary in offices, schools, and social gatherings.The fashion industry has flourished in the changing environment of the fashion world. Since Pakistan came into being, its fashion has evolved in different phases and developed a unique identity. Today, Pakistani fashion is a combination of traditional and modern dress and has become a mark of Pakistani culture. Despite modern trends, regional and traditional forms of dress have developed their own significance as a symbol of native tradition. This regional fashion continues to evolve into both more modern and purer forms. The Pakistan Fashion Design Council based in Lahore organizes PFDC Fashion Week and the Fashion Pakistan Council based in Karachi organizes Fashion Pakistan Week. Pakistan\\'s first fashion week was held in November 2009.\\n\\n\\n=== Media and entertainment ===\\n\\nThe private print media, state-owned Pakistan Television Corporation (PTV), and Pakistan Broadcasting Corporation (PBC) for radio were the dominant media outlets until the beginning of the 21st century. Pakistan now has a large network of domestic, privately owned 24-hour news media and television channels. A 2016 report by the Reporters Without Borders ranked Pakistan 147th on the Press Freedom Index, while at the same time terming the Pakistani media \"among the freest in Asia when it comes to covering the squabbling among politicians.\" The BBC terms the Pakistani media \"among the most outspoken in South Asia\". Pakistani media has also played a vital role in exposing corruption.The Lollywood, Kariwood, Punjabi and Pashto film industry is based in Karachi, Lahore and Peshawar. While Bollywood films were banned from public cinemas from 1965 until 2008, they have remained an important part of popular culture. In contrast to the ailing Pakistani film industry, Urdu televised dramas and theatrical performances continue to be popular, as many entertainment media outlets air them regularly. Urdu dramas dominate the television entertainment industry, which has launched critically acclaimed miniseries and featured popular actors and actresses since the 1990s. In the 1960s–1970s, pop music and disco (1970s) dominated the country\\'s music industry. In the 1980s–1990s, British influenced rock music appeared and jolted the country\\'s entertainment industry. In the 2000s, heavy metal music gained popular and critical acclaim.Pakistani music ranges from diverse forms of provincial folk music and traditional styles such as Qawwali and Ghazal Gayaki to modern musical forms that fuse traditional and western music. Pakistan has many famous folk singers. The arrival of Afghan refugees in the western provinces has stimulated interest in Pashto music, although there has been intolerance of it in some places.\\n\\n\\n=== Diaspora ===\\nAccording to the UN Department of Economic and Social Affairs, Pakistan has the sixth-largest diaspora in the world. Statistics gathered by the Pakistani government show that there are around 7 million Pakistanis residing abroad, with the vast majority living in the Middle East, Europe, and North America. Pakistan ranks 10th in the world for remittances sent home. The largest inflow of remittances, as of 2016, is from Saudi Arabia, amounting to $5.9 billion. The term Overseas Pakistani is officially recognised by the Government of Pakistan. The Ministry of Overseas Pakistanis was established in 2008 to deal exclusively with all matters of overseas Pakistanis such as attending to their needs and problems, developing projects for their welfare, and working for resolution of their problems and issues. Overseas Pakistanis are the second-largest source of foreign exchange remittances to Pakistan after exports. Over the last several years, home remittances have maintained a steadily rising trend, with a more than 100% increase from US$8.9 billion in 2009–10 to US$19.9 billion in 2015–16.The Overseas Pakistani Division (OPD) was created in September 2004 within the Ministry of Labour (MoL). It has since recognised the importance of overseas Pakistanis and their contribution to the nation\\'s economy. Together with Community Welfare Attaches (CWAs) and the Overseas Pakistanis Foundation (OPF), the OPD is making efforts to improve the welfare of Pakistanis who reside abroad. The division aims to provide better services through improved facilities at airports, and suitable schemes for housing, education, and health care. It also facilitates the reintegration into society of returning overseas Pakistanis. Notable members of the Pakistani diaspora include the London Mayor Sadiq Khan, the UK cabinet member Sajid Javid, the former UK Conservative Party chair Baroness Warsi, the singers Zayn Malik and Nadia Ali, MIT physics Professor Dr. Nergis Mavalvala, the actors Riz Ahmed and Kumail Nanjiani, the businessmen Shahid Khan and Sir Anwar Pervez, Boston University professors Adil Najam and Hamid Nawab, Texas A&M professor Muhammad Suhail Zubairy, Yale professor Sara Suleri, UC San Diego professor Farooq Azam and the historian Ayesha Jalal.\\n\\n\\n=== Literature and philosophy ===\\n\\nPakistan has literature in Urdu, Sindhi, Punjabi, Pashto, Baluchi, Persian, English, and many other languages. The Pakistan Academy of Letters is a large literary community that promotes literature and poetry in Pakistan and abroad. The National Library publishes and promotes literature in the country. Before the 19th century, Pakistani literature consisted mainly of lyric and religious poetry and mystical and folkloric works. During the colonial period, native literary figures were influenced by western literary realism and took up increasingly varied topics and narrative forms. Prose fiction is now very popular.\\n\\nThe national poet of Pakistan, Muhammad Iqbal, wrote poetry in Urdu and Persian. He was a strong proponent of the political and spiritual revival of Islamic civilisation and encouraged Muslims all over the world to bring about a successful revolution. Well-known figures in contemporary Pakistani Urdu literature include Josh Malihabadi Faiz Ahmed Faiz and Saadat Hasan Manto. Sadequain and Gulgee are known for their calligraphy and paintings. The Sufi poets Shah Abdul Latif, Bulleh Shah, Mian Muhammad Bakhsh, and Khawaja Farid enjoy considerable popularity in Pakistan. Mirza Kalich Beg has been termed the father of modern Sindhi prose. Historically, philosophical development in the country was dominated by Muhammad Iqbal, Sir Syed Ahmad Khan, Muhammad Asad, Maududi, and Mohammad Ali Johar.Ideas from British and American philosophy greatly shaped philosophical development in Pakistan. Analysts such as M. M. Sharif and Zafar Hassan established the first major Pakistani philosophical movement in 1947. After the 1971 war, philosophers such as Jalaludin Abdur Rahim, Gianchandani, and Malik Khalid incorporated Marxism into Pakistan\\'s philosophical thinking. Influential work by Manzoor Ahmad, Jon Elia, Hasan Askari Rizvi, and Abdul Khaliq brought mainstream social, political, and analytical philosophy to the fore in academia. Works by Noam Chomsky have influenced philosophical ideas in various fields of social and political philosophy.\\n\\n\\n=== Architecture ===\\n\\nFour periods are recognised in Pakistani architecture: pre-Islamic, Islamic, colonial, and post-colonial. With the beginning of the Indus civilization around the middle of the 3rd millennium BCE, an advanced urban culture developed for the first time in the region, with large buildings, some of which survive to this day. Mohenjo Daro, Harappa, and Kot Diji are among the pre-Islamic settlements that are now tourist attractions. The rise of Buddhism and the influence of Greek civilisation led to the development of a Greco-Buddhist style, starting from the 1st century CE. The high point of this era was the Gandhara style. An example of Buddhist architecture is the ruins of the Buddhist monastery Takht-i-Bahi in Khyber-Pakhtunkhwa.The arrival of Islam in what is today Pakistan meant the sudden end of Buddhist architecture in the area and a smooth transition to the predominantly pictureless Islamic architecture. The most important Indo-Islamic-style building still standing is the tomb of the Shah Rukn-i-Alam in Multan. During the Mughal era, design elements of Persian-Islamic architecture were fused with and often produced playful forms of Hindustani art. Lahore, as the occasional residence of Mughal rulers, contains many important buildings from the empire. Most prominent among them are the Badshahi Mosque, the fortress of Lahore with the famous Alamgiri Gate, the colourful, Mughal-style Wazir Khan Mosque, the Shalimar Gardens in Lahore, and the Shahjahan Mosque in Thatta. In the British colonial period, predominantly functional buildings of the Indo-European representative style developed from a mixture of European and Indian-Islamic components. Post-colonial national identity is expressed in modern structures such as the Faisal Mosque, the Minar-e-Pakistan, and the Mazar-e-Quaid. Several examples of architectural infrastructure demonstrating the influence of British design can be found in Lahore, Peshawar, and Karachi.\\n\\n\\n=== Food and drink ===\\n\\n\\n==== Traditional food ====\\n\\nPakistani cuisine is similar to that of other regions of South Asia, with some of it being originated from the royal kitchens of 16th-century Mughal emperors. Most of those dishes have their roots in British, Indian, Central Asian and Middle Eastern cuisine. Unlike Middle Eastern cuisine, Pakistani cooking uses large quantities of spices, herbs, and seasoning. Garlic, ginger, turmeric, red chili, and garam masala are used in most dishes, and home cooking regularly includes curry, roti, a thin flatbread made from wheat, is a staple food, usually served with curry, meat, vegetables, and lentils. Rice is also common; it is served plain, fried with spices, and in sweet dishes.Lassi is a traditional drink in the Punjab region. Black tea with milk and sugar is popular throughout Pakistan and is consumed daily by most of the population. Sohan halwa is a popular sweet dish from the southern region of Punjab province and is enjoyed all over Pakistan.\\n\\n\\n=== Sports ===\\n\\nMost sports played in Pakistan originated and were substantially developed by athletes and sports fans from the United Kingdom who introduced them during the British Raj. Field hockey is the national sport of Pakistan; it has won three gold medals in the Olympic Games held in 1960, 1968, and 1984. Pakistan has also won the Hockey World Cup a record four times, held in 1971, 1978, 1982, and 1994.\\n\\nCricket, however, is the most popular game across the country. The country has had an array of success in the sport over the years, and has the distinct achievement of having won each of the major ICC international cricket tournaments: ICC Cricket World Cup, ICC World Twenty20, and ICC Champions Trophy; as well as the ICC Test Championship. The cricket team (known as Shaheen) won the Cricket World Cup held in 1992; it was runner-up once, in 1999. Pakistan was runner-up in the inaugural World Twenty20 (2007) in South Africa and won the World Twenty20 in England in 2009. In March 2009, militants attacked the touring Sri Lankan cricket team, after which no international cricket was played in Pakistan until May 2015, when the Zimbabwean team agreed to a tour. Pakistan also won the 2017 ICC Champions Trophy by defeating arch-rivals India in the final.\\nPakistan Super League is one of the largest cricket leagues of the world with a brand value of about ₨32.26 billion (US$200 million).Association Football is the second most played sports in Pakistan and it is organised and regulated by the Pakistan Football Federation. Football in Pakistan is as old as the country itself. Shortly after the creation of Pakistan in 1947, the Pakistan Football Federation (PFF) was created, and Muhammad Ali Jinnah became its first Patron-in-Chief. The highest football division in Pakistan is the Pakistan Premier League. Pakistan is known as one of the best manufacturer of the official FIFA World Cup ball. The best football players to play for Pakistan are Kaleemullah, Zesh Rehman, Muhammad Essa, Haroon Yousaf and Muhammad Adil.\\nPakistan has hosted or co-hosted several international sporting events: the 1989 and 2004 South Asian Games; the 1984, 1993, 1996 and 2003 World Squash Championships; the 1987 and 1996 Cricket World Cup; and the 1990 Hockey World Cup.\\nPakistan is set to host the 2023 South Asian Games.\\n\\n\\n== See also ==\\n\\nOutline of Pakistan\\nCrime in Pakistan \\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Bibliography ==\\n\\n\\n== External links ==\\n\\n\\n=== Government ===\\nOfficial website\\nPakistan Public Policies & Researches\\n\\n\\n=== General information ===\\nPakistan. The World Factbook. Central Intelligence Agency.\\nPakistan from UCB Libraries GovPubs\\nPakistan at Curlie\\nPakistan from BBC News\\n Wikimedia Atlas of Pakistan\\nKey Development Forecasts for Pakistan from International Futures\\n Geographic data related to Pakistan at OpenStreetMap',\n",
       " 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\\n\\n\\n== Description ==\\nGiven a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        Var\\n        \\u2061\\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}={\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}|S_{i}|\\\\operatorname {Var} S_{i}}\\n  where μi is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n        \\n          \\n            1\\n            \\n              \\n                |\\n              \\n              \\n                S\\n                \\n                  i\\n                \\n              \\n              \\n                |\\n              \\n            \\n          \\n        \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ,\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\,{\\\\frac {1}{|S_{i}|}}\\\\,\\\\sum _{\\\\mathbf {x} ,\\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  The equivalence can be deduced from identity \\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ≠\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |S_{i}|\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}=\\\\sum _{\\\\mathbf {x} \\\\neq \\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  . Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS),. This deterministic relationship is also related to the law of total variance in probability theory.\\n\\n\\n== History ==\\nThe term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.\\n\\n\\n== Algorithms ==\\n\\n\\n=== Standard algorithm (naive k-means) ===\\n\\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the k-means algorithm\"; it is also referred to as Lloyd\\'s algorithm, particularly in the computer science community. It is sometimes also referred to as \"naïve k-means\", because there exist much faster alternatives.Given an initial set of k means m1(1),...,mk(1) (see below), the algorithm proceeds by alternating between two steps:\\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            )\\n          \\n        \\n        =\\n        \\n          {\\n          \\n            \\n              x\\n              \\n                p\\n              \\n            \\n            :\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      i\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n            ≤\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      j\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n             \\n            ∀\\n            j\\n            ,\\n            1\\n            ≤\\n            j\\n            ≤\\n            k\\n          \\n          }\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle S_{i}^{(t)}=\\\\left\\\\{x_{p}:\\\\left\\\\|x_{p}-m_{i}^{(t)}\\\\right\\\\|^{2}\\\\leq \\\\left\\\\|x_{p}-m_{j}^{(t)}\\\\right\\\\|^{2}\\\\ \\\\forall j,1\\\\leq j\\\\leq k\\\\right\\\\},}\\n  \\nwhere each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{p}}\\n   is assigned to exactly one \\n  \\n    \\n      \\n        \\n          S\\n          \\n            (\\n            t\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S^{(t)}}\\n  , even if it could be assigned to two or more of them.\\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\\n\\n  \\n    \\n      \\n        \\n          m\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            +\\n            1\\n            )\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              |\\n              \\n                S\\n                \\n                  i\\n                \\n                \\n                  (\\n                  t\\n                  )\\n                \\n              \\n              |\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n              \\n                j\\n              \\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n              \\n                (\\n                t\\n                )\\n              \\n            \\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m_{i}^{(t+1)}={\\\\frac {1}{\\\\left|S_{i}^{(t)}\\\\right|}}\\\\sum _{x_{j}\\\\in S_{i}^{(t)}}x_{j}}\\n  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\\n\\n\\n==== Initialization methods ====\\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster\\'s randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad\\'s approach performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\\n\\nDemonstration of the standard algorithm\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.\\n\\n\\n=== Complexity ===\\nFinding the optimal solution to the k-means clustering problem for observations in d dimensions is:\\n\\nNP-hard in general Euclidean space (of d dimensions) even for two clusters,\\nNP-hard for a general number of clusters k even in the plane,\\nif k and d (the dimension) are fixed, the problem can be exactly solved in time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            d\\n            k\\n            +\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{dk+1})}\\n  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd\\'s algorithm given above are generally used.\\nThe running time of Lloyd\\'s algorithm (and most variants) is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        k\\n        d\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nkdi)}\\n  , where:\\n\\nn is the number of d-dimensional vectors (to be clustered)\\nk the number of clusters\\ni the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd\\'s algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.\\nIn the worst-case, Lloyd\\'s algorithm needs \\n  \\n    \\n      \\n        i\\n        =\\n        \\n          2\\n          \\n            Ω\\n            (\\n            \\n              \\n                n\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=2^{\\\\Omega ({\\\\sqrt {n}})}}\\n   iterations, so that the worst-case complexity of Lloyd\\'s algorithm is superpolynomial.\\nLloyd\\'s k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in \\n  \\n    \\n      \\n        [\\n        0\\n        ,\\n        1\\n        \\n          ]\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [0,1]^{d}}\\n  , if each point is independently perturbed by a normal distribution with mean 0 and variance \\n  \\n    \\n      \\n        \\n          σ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}}\\n  , then the expected running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            34\\n          \\n        \\n        \\n          k\\n          \\n            34\\n          \\n        \\n        \\n          d\\n          \\n            8\\n          \\n        \\n        \\n          log\\n          \\n            4\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        \\n          /\\n        \\n        \\n          σ\\n          \\n            6\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{34}k^{34}d^{8}\\\\log ^{4}(n)/\\\\sigma ^{6})}\\n  , which is a polynomial in n, k, d and \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        σ\\n      \\n    \\n    {\\\\displaystyle 1/\\\\sigma }\\n  .\\nBetter bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        d\\n        \\n          n\\n          \\n            4\\n          \\n        \\n        \\n          M\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(dn^{4}M^{2})}\\n   for n points in an integer lattice \\n  \\n    \\n      \\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        M\\n        \\n          }\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{1,\\\\dots ,M\\\\}^{d}}\\n  .Lloyd\\'s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd\\'s algorithm.\\n\\n\\n=== Variations ===\\nJenks natural breaks optimization: k-means applied to univariate data\\nk-medians clustering uses the median in each dimension instead of the mean, and this way minimizes \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm (Taxicab geometry).\\nk-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.\\nFuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\\nGaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.\\nk-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.\\nThe filtering algorithm uses kd-trees to speed up each k-means step.\\nSome methods attempt to speed up each k-means step using the triangle inequality.\\nEscape local optima by swapping points between clusters.\\nThe Spherical k-means clustering algorithm is suitable for textual data.\\nHierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.\\nInternal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.\\nMinkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.\\nMini-batch k-means: k-means variation using \"mini batch\" samples for data sets that do not fit into memory.\\nOtsu\\'s method\\n\\n\\n=== Hartigan–Wong method ===\\nHartigan and Wong\\'s method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\\nLet \\n  \\n    \\n      \\n        φ\\n        (\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\varphi (S_{j})}\\n   be the individual cost of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{j}}\\n   defined by \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              S\\n              \\n                j\\n              \\n            \\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          μ\\n          \\n            j\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x\\\\in S_{j}}(x-\\\\mu _{j})^{2}}\\n  , with \\n  \\n    \\n      \\n        \\n          μ\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{j}}\\n   the center of the cluster.\\nAssignment step: Hartigan and Wong\\'s method starts by partitioning the points into random clusters \\n  \\n    \\n      \\n        {\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        \\n          }\\n          \\n            j\\n            ∈\\n            {\\n            1\\n            ,\\n            ⋯\\n            k\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{S_{j}\\\\}_{j\\\\in \\\\{1,\\\\cdots k\\\\}}}\\n  .\\nUpdate step: Next it determines the \\n  \\n    \\n      \\n        n\\n        ,\\n        m\\n        ∈\\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        k\\n        }\\n      \\n    \\n    {\\\\displaystyle n,m\\\\in \\\\{1,\\\\ldots ,k\\\\}}\\n   and \\n  \\n    \\n      \\n        x\\n        ∈\\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\in S_{n}}\\n   for which the following function reaches a maximum\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n        =\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        )\\n        +\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        ∖\\n        {\\n        x\\n        }\\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        ∪\\n        {\\n        x\\n        }\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)=\\\\varphi (S_{n})+\\\\varphi (S_{m})-\\\\varphi (S_{n}\\\\smallsetminus \\\\{x\\\\})-\\\\varphi (S_{m}\\\\cup \\\\{x\\\\}).}\\n  For the \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n   that reach this maximum, \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   moves from the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{n}}\\n   to the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{m}}\\n  .\\nTermination: The algorithm terminates once \\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)}\\n   is less than zero for all \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n  .\\nDifferent move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function \\n  \\n    \\n      \\n        Δ\\n      \\n    \\n    {\\\\displaystyle \\\\Delta }\\n   used to calculate the result of a relocation can also be efficiently evaluated by using equality\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        x\\n        ,\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n              −\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n              +\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            m\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (x,n,m)={\\\\frac {\\\\mid S_{n}\\\\mid }{\\\\mid S_{n}\\\\mid -1}}\\\\cdot \\\\lVert \\\\mu _{n}-x\\\\rVert ^{2}-{\\\\frac {\\\\mid S_{m}\\\\mid }{\\\\mid S_{m}\\\\mid +1}}\\\\cdot \\\\lVert \\\\mu _{m}-x\\\\rVert ^{2}.}\\n  \\n\\n\\n=== Global optimization and metaheuristics ===\\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as  \\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}.}\\n  Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood searchand genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\\n\\n\\n== Discussion ==\\n\\nThree key features of k-means that make it efficient are often regarded as its biggest drawbacks:\\n\\nEuclidean distance is used as a metric and variance is used as a measure of cluster scatter.\\nThe number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\\nConvergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  , the two visible clusters (one containing two species) will be discovered, whereas with \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   one of the two clusters will be split into two even parts. In fact, \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n   is more appropriate for this data set, despite the data set\\'s containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\\nThe result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.\\n\\n\\n== Applications ==\\nk-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd\\'s algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\\n\\n\\n=== Vector quantization ===\\n\\nk-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\\n\\n\\n=== Cluster analysis ===\\nIn cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\\nHowever, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\\n\\n\\n=== Feature learning ===\\nk-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\\n\\n\\n== Relation to other algorithms ==\\n\\n\\n=== Gaussian mixture model ===\\n\\nThe slow \"standard algorithm\" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.:\\u200a850\\u200a Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of \"hard\" Gaussian mixture modelling.:\\u200a354,\\u200a11.4.2.5\\u200a This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.:\\u200a849\\u200a\\n\\n\\n=== K-SVD ===\\n\\nAnother generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\\n\\n\\n=== Principal component analysis ===\\n\\nThe relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\\n\\n\\n=== Mean shift clustering ===\\n\\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\\n\\n\\n=== Independent component analysis ===\\n\\nUnder sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.\\n\\n\\n=== Bilateral filtering ===\\n\\nk-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\\n\\n\\n== Similar problems ==\\nThe set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.\\n\\n\\n== Software implementations ==\\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\\n\\n\\n=== Free Software/Open Source ===\\nThe following implementations are available under Free/Open Source Software licenses, with publicly available source code.\\n\\nAccord.NET contains C# implementations for k-means, k-means++ and k-modes.\\nALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.\\nAOSP contains a Java implementation for k-means.\\nCrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.\\nELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.\\nSmile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).\\nJulia contains a k-means implementation in the JuliaStats Clustering package.\\nKNIME contains nodes for k-means and k-medoids.\\nMahout contains a MapReduce based k-means.\\nmlpack contains a C++ implementation of k-means.\\nOctave contains k-means.\\nOpenCV contains a k-means implementation.\\nOrange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.\\nPSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.\\nR contains three k-means variations.\\nSciPy and scikit-learn contain multiple k-means implementations.\\nSpark MLlib implements a distributed k-means algorithm.\\nTorch contains an unsup package that provides k-means clustering.\\nWeka contains k-means and x-means.\\n\\n\\n=== Proprietary ===\\nThe following implementations are available under proprietary license terms, and may not have publicly available source code.\\n\\n\\n== See also ==\\nBFR algorithm\\nCentroidal Voronoi tessellation\\nHead/tail Breaks\\nk q-flats\\nK-means++\\nLinde–Buzo–Gray algorithm\\nSelf-organizing map\\n\\n\\n== References ==',\n",
       " 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\\n\\n\\n== Description ==\\nGiven a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        Var\\n        \\u2061\\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}={\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}|S_{i}|\\\\operatorname {Var} S_{i}}\\n  where μi is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n        \\n          \\n            1\\n            \\n              \\n                |\\n              \\n              \\n                S\\n                \\n                  i\\n                \\n              \\n              \\n                |\\n              \\n            \\n          \\n        \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ,\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\,{\\\\frac {1}{|S_{i}|}}\\\\,\\\\sum _{\\\\mathbf {x} ,\\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  The equivalence can be deduced from identity \\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ≠\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |S_{i}|\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}=\\\\sum _{\\\\mathbf {x} \\\\neq \\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  . Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS),. This deterministic relationship is also related to the law of total variance in probability theory.\\n\\n\\n== History ==\\nThe term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.\\n\\n\\n== Algorithms ==\\n\\n\\n=== Standard algorithm (naive k-means) ===\\n\\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the k-means algorithm\"; it is also referred to as Lloyd\\'s algorithm, particularly in the computer science community. It is sometimes also referred to as \"naïve k-means\", because there exist much faster alternatives.Given an initial set of k means m1(1),...,mk(1) (see below), the algorithm proceeds by alternating between two steps:\\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            )\\n          \\n        \\n        =\\n        \\n          {\\n          \\n            \\n              x\\n              \\n                p\\n              \\n            \\n            :\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      i\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n            ≤\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      j\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n             \\n            ∀\\n            j\\n            ,\\n            1\\n            ≤\\n            j\\n            ≤\\n            k\\n          \\n          }\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle S_{i}^{(t)}=\\\\left\\\\{x_{p}:\\\\left\\\\|x_{p}-m_{i}^{(t)}\\\\right\\\\|^{2}\\\\leq \\\\left\\\\|x_{p}-m_{j}^{(t)}\\\\right\\\\|^{2}\\\\ \\\\forall j,1\\\\leq j\\\\leq k\\\\right\\\\},}\\n  \\nwhere each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{p}}\\n   is assigned to exactly one \\n  \\n    \\n      \\n        \\n          S\\n          \\n            (\\n            t\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S^{(t)}}\\n  , even if it could be assigned to two or more of them.\\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\\n\\n  \\n    \\n      \\n        \\n          m\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            +\\n            1\\n            )\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              |\\n              \\n                S\\n                \\n                  i\\n                \\n                \\n                  (\\n                  t\\n                  )\\n                \\n              \\n              |\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n              \\n                j\\n              \\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n              \\n                (\\n                t\\n                )\\n              \\n            \\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m_{i}^{(t+1)}={\\\\frac {1}{\\\\left|S_{i}^{(t)}\\\\right|}}\\\\sum _{x_{j}\\\\in S_{i}^{(t)}}x_{j}}\\n  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\\n\\n\\n==== Initialization methods ====\\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster\\'s randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad\\'s approach performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\\n\\nDemonstration of the standard algorithm\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.\\n\\n\\n=== Complexity ===\\nFinding the optimal solution to the k-means clustering problem for observations in d dimensions is:\\n\\nNP-hard in general Euclidean space (of d dimensions) even for two clusters,\\nNP-hard for a general number of clusters k even in the plane,\\nif k and d (the dimension) are fixed, the problem can be exactly solved in time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            d\\n            k\\n            +\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{dk+1})}\\n  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd\\'s algorithm given above are generally used.\\nThe running time of Lloyd\\'s algorithm (and most variants) is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        k\\n        d\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nkdi)}\\n  , where:\\n\\nn is the number of d-dimensional vectors (to be clustered)\\nk the number of clusters\\ni the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd\\'s algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.\\nIn the worst-case, Lloyd\\'s algorithm needs \\n  \\n    \\n      \\n        i\\n        =\\n        \\n          2\\n          \\n            Ω\\n            (\\n            \\n              \\n                n\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=2^{\\\\Omega ({\\\\sqrt {n}})}}\\n   iterations, so that the worst-case complexity of Lloyd\\'s algorithm is superpolynomial.\\nLloyd\\'s k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in \\n  \\n    \\n      \\n        [\\n        0\\n        ,\\n        1\\n        \\n          ]\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [0,1]^{d}}\\n  , if each point is independently perturbed by a normal distribution with mean 0 and variance \\n  \\n    \\n      \\n        \\n          σ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}}\\n  , then the expected running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            34\\n          \\n        \\n        \\n          k\\n          \\n            34\\n          \\n        \\n        \\n          d\\n          \\n            8\\n          \\n        \\n        \\n          log\\n          \\n            4\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        \\n          /\\n        \\n        \\n          σ\\n          \\n            6\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{34}k^{34}d^{8}\\\\log ^{4}(n)/\\\\sigma ^{6})}\\n  , which is a polynomial in n, k, d and \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        σ\\n      \\n    \\n    {\\\\displaystyle 1/\\\\sigma }\\n  .\\nBetter bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        d\\n        \\n          n\\n          \\n            4\\n          \\n        \\n        \\n          M\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(dn^{4}M^{2})}\\n   for n points in an integer lattice \\n  \\n    \\n      \\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        M\\n        \\n          }\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{1,\\\\dots ,M\\\\}^{d}}\\n  .Lloyd\\'s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd\\'s algorithm.\\n\\n\\n=== Variations ===\\nJenks natural breaks optimization: k-means applied to univariate data\\nk-medians clustering uses the median in each dimension instead of the mean, and this way minimizes \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm (Taxicab geometry).\\nk-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.\\nFuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\\nGaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.\\nk-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.\\nThe filtering algorithm uses kd-trees to speed up each k-means step.\\nSome methods attempt to speed up each k-means step using the triangle inequality.\\nEscape local optima by swapping points between clusters.\\nThe Spherical k-means clustering algorithm is suitable for textual data.\\nHierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.\\nInternal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.\\nMinkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.\\nMini-batch k-means: k-means variation using \"mini batch\" samples for data sets that do not fit into memory.\\nOtsu\\'s method\\n\\n\\n=== Hartigan–Wong method ===\\nHartigan and Wong\\'s method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\\nLet \\n  \\n    \\n      \\n        φ\\n        (\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\varphi (S_{j})}\\n   be the individual cost of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{j}}\\n   defined by \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              S\\n              \\n                j\\n              \\n            \\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          μ\\n          \\n            j\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x\\\\in S_{j}}(x-\\\\mu _{j})^{2}}\\n  , with \\n  \\n    \\n      \\n        \\n          μ\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{j}}\\n   the center of the cluster.\\nAssignment step: Hartigan and Wong\\'s method starts by partitioning the points into random clusters \\n  \\n    \\n      \\n        {\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        \\n          }\\n          \\n            j\\n            ∈\\n            {\\n            1\\n            ,\\n            ⋯\\n            k\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{S_{j}\\\\}_{j\\\\in \\\\{1,\\\\cdots k\\\\}}}\\n  .\\nUpdate step: Next it determines the \\n  \\n    \\n      \\n        n\\n        ,\\n        m\\n        ∈\\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        k\\n        }\\n      \\n    \\n    {\\\\displaystyle n,m\\\\in \\\\{1,\\\\ldots ,k\\\\}}\\n   and \\n  \\n    \\n      \\n        x\\n        ∈\\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\in S_{n}}\\n   for which the following function reaches a maximum\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n        =\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        )\\n        +\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        ∖\\n        {\\n        x\\n        }\\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        ∪\\n        {\\n        x\\n        }\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)=\\\\varphi (S_{n})+\\\\varphi (S_{m})-\\\\varphi (S_{n}\\\\smallsetminus \\\\{x\\\\})-\\\\varphi (S_{m}\\\\cup \\\\{x\\\\}).}\\n  For the \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n   that reach this maximum, \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   moves from the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{n}}\\n   to the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{m}}\\n  .\\nTermination: The algorithm terminates once \\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)}\\n   is less than zero for all \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n  .\\nDifferent move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function \\n  \\n    \\n      \\n        Δ\\n      \\n    \\n    {\\\\displaystyle \\\\Delta }\\n   used to calculate the result of a relocation can also be efficiently evaluated by using equality\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        x\\n        ,\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n              −\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n              +\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            m\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (x,n,m)={\\\\frac {\\\\mid S_{n}\\\\mid }{\\\\mid S_{n}\\\\mid -1}}\\\\cdot \\\\lVert \\\\mu _{n}-x\\\\rVert ^{2}-{\\\\frac {\\\\mid S_{m}\\\\mid }{\\\\mid S_{m}\\\\mid +1}}\\\\cdot \\\\lVert \\\\mu _{m}-x\\\\rVert ^{2}.}\\n  \\n\\n\\n=== Global optimization and metaheuristics ===\\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as  \\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}.}\\n  Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood searchand genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\\n\\n\\n== Discussion ==\\n\\nThree key features of k-means that make it efficient are often regarded as its biggest drawbacks:\\n\\nEuclidean distance is used as a metric and variance is used as a measure of cluster scatter.\\nThe number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\\nConvergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  , the two visible clusters (one containing two species) will be discovered, whereas with \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   one of the two clusters will be split into two even parts. In fact, \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n   is more appropriate for this data set, despite the data set\\'s containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\\nThe result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.\\n\\n\\n== Applications ==\\nk-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd\\'s algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\\n\\n\\n=== Vector quantization ===\\n\\nk-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\\n\\n\\n=== Cluster analysis ===\\nIn cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\\nHowever, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\\n\\n\\n=== Feature learning ===\\nk-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\\n\\n\\n== Relation to other algorithms ==\\n\\n\\n=== Gaussian mixture model ===\\n\\nThe slow \"standard algorithm\" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.:\\u200a850\\u200a Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of \"hard\" Gaussian mixture modelling.:\\u200a354,\\u200a11.4.2.5\\u200a This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.:\\u200a849\\u200a\\n\\n\\n=== K-SVD ===\\n\\nAnother generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\\n\\n\\n=== Principal component analysis ===\\n\\nThe relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\\n\\n\\n=== Mean shift clustering ===\\n\\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\\n\\n\\n=== Independent component analysis ===\\n\\nUnder sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.\\n\\n\\n=== Bilateral filtering ===\\n\\nk-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\\n\\n\\n== Similar problems ==\\nThe set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.\\n\\n\\n== Software implementations ==\\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\\n\\n\\n=== Free Software/Open Source ===\\nThe following implementations are available under Free/Open Source Software licenses, with publicly available source code.\\n\\nAccord.NET contains C# implementations for k-means, k-means++ and k-modes.\\nALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.\\nAOSP contains a Java implementation for k-means.\\nCrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.\\nELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.\\nSmile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).\\nJulia contains a k-means implementation in the JuliaStats Clustering package.\\nKNIME contains nodes for k-means and k-medoids.\\nMahout contains a MapReduce based k-means.\\nmlpack contains a C++ implementation of k-means.\\nOctave contains k-means.\\nOpenCV contains a k-means implementation.\\nOrange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.\\nPSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.\\nR contains three k-means variations.\\nSciPy and scikit-learn contain multiple k-means implementations.\\nSpark MLlib implements a distributed k-means algorithm.\\nTorch contains an unsup package that provides k-means clustering.\\nWeka contains k-means and x-means.\\n\\n\\n=== Proprietary ===\\nThe following implementations are available under proprietary license terms, and may not have publicly available source code.\\n\\n\\n== See also ==\\nBFR algorithm\\nCentroidal Voronoi tessellation\\nHead/tail Breaks\\nk q-flats\\nK-means++\\nLinde–Buzo–Gray algorithm\\nSelf-organizing map\\n\\n\\n== References ==',\n",
       " 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\\n\\n\\n== Description ==\\nGiven a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        Var\\n        \\u2061\\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}={\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}|S_{i}|\\\\operatorname {Var} S_{i}}\\n  where μi is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n        \\n          \\n            1\\n            \\n              \\n                |\\n              \\n              \\n                S\\n                \\n                  i\\n                \\n              \\n              \\n                |\\n              \\n            \\n          \\n        \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ,\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\,{\\\\frac {1}{|S_{i}|}}\\\\,\\\\sum _{\\\\mathbf {x} ,\\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  The equivalence can be deduced from identity \\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ≠\\n            \\n              y\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                y\\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |S_{i}|\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}=\\\\sum _{\\\\mathbf {x} \\\\neq \\\\mathbf {y} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -\\\\mathbf {y} \\\\right\\\\|^{2}}\\n  . Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS),. This deterministic relationship is also related to the law of total variance in probability theory.\\n\\n\\n== History ==\\nThe term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.\\n\\n\\n== Algorithms ==\\n\\n\\n=== Standard algorithm (naive k-means) ===\\n\\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the k-means algorithm\"; it is also referred to as Lloyd\\'s algorithm, particularly in the computer science community. It is sometimes also referred to as \"naïve k-means\", because there exist much faster alternatives.Given an initial set of k means m1(1),...,mk(1) (see below), the algorithm proceeds by alternating between two steps:\\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            )\\n          \\n        \\n        =\\n        \\n          {\\n          \\n            \\n              x\\n              \\n                p\\n              \\n            \\n            :\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      i\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n            ≤\\n            \\n              \\n                ‖\\n                \\n                  \\n                    x\\n                    \\n                      p\\n                    \\n                  \\n                  −\\n                  \\n                    m\\n                    \\n                      j\\n                    \\n                    \\n                      (\\n                      t\\n                      )\\n                    \\n                  \\n                \\n                ‖\\n              \\n              \\n                2\\n              \\n            \\n             \\n            ∀\\n            j\\n            ,\\n            1\\n            ≤\\n            j\\n            ≤\\n            k\\n          \\n          }\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle S_{i}^{(t)}=\\\\left\\\\{x_{p}:\\\\left\\\\|x_{p}-m_{i}^{(t)}\\\\right\\\\|^{2}\\\\leq \\\\left\\\\|x_{p}-m_{j}^{(t)}\\\\right\\\\|^{2}\\\\ \\\\forall j,1\\\\leq j\\\\leq k\\\\right\\\\},}\\n  \\nwhere each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{p}}\\n   is assigned to exactly one \\n  \\n    \\n      \\n        \\n          S\\n          \\n            (\\n            t\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S^{(t)}}\\n  , even if it could be assigned to two or more of them.\\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\\n\\n  \\n    \\n      \\n        \\n          m\\n          \\n            i\\n          \\n          \\n            (\\n            t\\n            +\\n            1\\n            )\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              |\\n              \\n                S\\n                \\n                  i\\n                \\n                \\n                  (\\n                  t\\n                  )\\n                \\n              \\n              |\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n              \\n                j\\n              \\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n              \\n                (\\n                t\\n                )\\n              \\n            \\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m_{i}^{(t+1)}={\\\\frac {1}{\\\\left|S_{i}^{(t)}\\\\right|}}\\\\sum _{x_{j}\\\\in S_{i}^{(t)}}x_{j}}\\n  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\\n\\n\\n==== Initialization methods ====\\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster\\'s randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad\\'s approach performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\\n\\nDemonstration of the standard algorithm\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.\\n\\n\\n=== Complexity ===\\nFinding the optimal solution to the k-means clustering problem for observations in d dimensions is:\\n\\nNP-hard in general Euclidean space (of d dimensions) even for two clusters,\\nNP-hard for a general number of clusters k even in the plane,\\nif k and d (the dimension) are fixed, the problem can be exactly solved in time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            d\\n            k\\n            +\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{dk+1})}\\n  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd\\'s algorithm given above are generally used.\\nThe running time of Lloyd\\'s algorithm (and most variants) is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        k\\n        d\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nkdi)}\\n  , where:\\n\\nn is the number of d-dimensional vectors (to be clustered)\\nk the number of clusters\\ni the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd\\'s algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.\\nIn the worst-case, Lloyd\\'s algorithm needs \\n  \\n    \\n      \\n        i\\n        =\\n        \\n          2\\n          \\n            Ω\\n            (\\n            \\n              \\n                n\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=2^{\\\\Omega ({\\\\sqrt {n}})}}\\n   iterations, so that the worst-case complexity of Lloyd\\'s algorithm is superpolynomial.\\nLloyd\\'s k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in \\n  \\n    \\n      \\n        [\\n        0\\n        ,\\n        1\\n        \\n          ]\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [0,1]^{d}}\\n  , if each point is independently perturbed by a normal distribution with mean 0 and variance \\n  \\n    \\n      \\n        \\n          σ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}}\\n  , then the expected running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            34\\n          \\n        \\n        \\n          k\\n          \\n            34\\n          \\n        \\n        \\n          d\\n          \\n            8\\n          \\n        \\n        \\n          log\\n          \\n            4\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        \\n          /\\n        \\n        \\n          σ\\n          \\n            6\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{34}k^{34}d^{8}\\\\log ^{4}(n)/\\\\sigma ^{6})}\\n  , which is a polynomial in n, k, d and \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        σ\\n      \\n    \\n    {\\\\displaystyle 1/\\\\sigma }\\n  .\\nBetter bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by \\n  \\n    \\n      \\n        O\\n        (\\n        d\\n        \\n          n\\n          \\n            4\\n          \\n        \\n        \\n          M\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(dn^{4}M^{2})}\\n   for n points in an integer lattice \\n  \\n    \\n      \\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        M\\n        \\n          }\\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{1,\\\\dots ,M\\\\}^{d}}\\n  .Lloyd\\'s algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd\\'s algorithm.\\n\\n\\n=== Variations ===\\nJenks natural breaks optimization: k-means applied to univariate data\\nk-medians clustering uses the median in each dimension instead of the mean, and this way minimizes \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm (Taxicab geometry).\\nk-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.\\nFuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\\nGaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.\\nk-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.\\nThe filtering algorithm uses kd-trees to speed up each k-means step.\\nSome methods attempt to speed up each k-means step using the triangle inequality.\\nEscape local optima by swapping points between clusters.\\nThe Spherical k-means clustering algorithm is suitable for textual data.\\nHierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.\\nInternal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.\\nMinkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.\\nMini-batch k-means: k-means variation using \"mini batch\" samples for data sets that do not fit into memory.\\nOtsu\\'s method\\n\\n\\n=== Hartigan–Wong method ===\\nHartigan and Wong\\'s method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\\nLet \\n  \\n    \\n      \\n        φ\\n        (\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\varphi (S_{j})}\\n   be the individual cost of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{j}}\\n   defined by \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              S\\n              \\n                j\\n              \\n            \\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          μ\\n          \\n            j\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x\\\\in S_{j}}(x-\\\\mu _{j})^{2}}\\n  , with \\n  \\n    \\n      \\n        \\n          μ\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{j}}\\n   the center of the cluster.\\nAssignment step: Hartigan and Wong\\'s method starts by partitioning the points into random clusters \\n  \\n    \\n      \\n        {\\n        \\n          S\\n          \\n            j\\n          \\n        \\n        \\n          }\\n          \\n            j\\n            ∈\\n            {\\n            1\\n            ,\\n            ⋯\\n            k\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{S_{j}\\\\}_{j\\\\in \\\\{1,\\\\cdots k\\\\}}}\\n  .\\nUpdate step: Next it determines the \\n  \\n    \\n      \\n        n\\n        ,\\n        m\\n        ∈\\n        {\\n        1\\n        ,\\n        …\\n        ,\\n        k\\n        }\\n      \\n    \\n    {\\\\displaystyle n,m\\\\in \\\\{1,\\\\ldots ,k\\\\}}\\n   and \\n  \\n    \\n      \\n        x\\n        ∈\\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\in S_{n}}\\n   for which the following function reaches a maximum\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n        =\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        )\\n        +\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            n\\n          \\n        \\n        ∖\\n        {\\n        x\\n        }\\n        )\\n        −\\n        φ\\n        (\\n        \\n          S\\n          \\n            m\\n          \\n        \\n        ∪\\n        {\\n        x\\n        }\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)=\\\\varphi (S_{n})+\\\\varphi (S_{m})-\\\\varphi (S_{n}\\\\smallsetminus \\\\{x\\\\})-\\\\varphi (S_{m}\\\\cup \\\\{x\\\\}).}\\n  For the \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n   that reach this maximum, \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   moves from the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{n}}\\n   to the cluster \\n  \\n    \\n      \\n        \\n          S\\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{m}}\\n  .\\nTermination: The algorithm terminates once \\n  \\n    \\n      \\n        Δ\\n        (\\n        m\\n        ,\\n        n\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (m,n,x)}\\n   is less than zero for all \\n  \\n    \\n      \\n        x\\n        ,\\n        n\\n        ,\\n        m\\n      \\n    \\n    {\\\\displaystyle x,n,m}\\n  .\\nDifferent move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function \\n  \\n    \\n      \\n        Δ\\n      \\n    \\n    {\\\\displaystyle \\\\Delta }\\n   used to calculate the result of a relocation can also be efficiently evaluated by using equality\\n\\n  \\n    \\n      \\n        Δ\\n        (\\n        x\\n        ,\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  n\\n                \\n              \\n              ∣\\n              −\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n            \\n            \\n              ∣\\n              \\n                S\\n                \\n                  m\\n                \\n              \\n              ∣\\n              +\\n              1\\n            \\n          \\n        \\n        ⋅\\n        ‖\\n        \\n          μ\\n          \\n            m\\n          \\n        \\n        −\\n        x\\n        \\n          ‖\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\Delta (x,n,m)={\\\\frac {\\\\mid S_{n}\\\\mid }{\\\\mid S_{n}\\\\mid -1}}\\\\cdot \\\\lVert \\\\mu _{n}-x\\\\rVert ^{2}-{\\\\frac {\\\\mid S_{m}\\\\mid }{\\\\mid S_{m}\\\\mid +1}}\\\\cdot \\\\lVert \\\\mu _{m}-x\\\\rVert ^{2}.}\\n  \\n\\n\\n=== Global optimization and metaheuristics ===\\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as  \\n  \\n    \\n      \\n        \\n          \\n            \\n              a\\n              r\\n              g\\n              \\n              m\\n              i\\n              n\\n            \\n            \\n              S\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              x\\n            \\n            ∈\\n            \\n              S\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          \\n            ‖\\n            \\n              \\n                x\\n              \\n              −\\n              \\n                \\n                  μ\\n                \\n                \\n                  i\\n                \\n              \\n            \\n            ‖\\n          \\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\underset {\\\\mathbf {S} }{\\\\operatorname {arg\\\\,min} }}\\\\sum _{i=1}^{k}\\\\sum _{\\\\mathbf {x} \\\\in S_{i}}\\\\left\\\\|\\\\mathbf {x} -{\\\\boldsymbol {\\\\mu }}_{i}\\\\right\\\\|^{2}.}\\n  Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood searchand genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\\n\\n\\n== Discussion ==\\n\\nThree key features of k-means that make it efficient are often regarded as its biggest drawbacks:\\n\\nEuclidean distance is used as a metric and variance is used as a measure of cluster scatter.\\nThe number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\\nConvergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  , the two visible clusters (one containing two species) will be discovered, whereas with \\n  \\n    \\n      \\n        k\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle k=3}\\n   one of the two clusters will be split into two even parts. In fact, \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n   is more appropriate for this data set, despite the data set\\'s containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\\nThe result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.\\n\\n\\n== Applications ==\\nk-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd\\'s algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\\n\\n\\n=== Vector quantization ===\\n\\nk-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\\n\\n\\n=== Cluster analysis ===\\nIn cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\\nHowever, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\\n\\n\\n=== Feature learning ===\\nk-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\\n\\n\\n== Relation to other algorithms ==\\n\\n\\n=== Gaussian mixture model ===\\n\\nThe slow \"standard algorithm\" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.:\\u200a850\\u200a Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of \"hard\" Gaussian mixture modelling.:\\u200a354,\\u200a11.4.2.5\\u200a This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.:\\u200a849\\u200a\\n\\n\\n=== K-SVD ===\\n\\nAnother generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\\n\\n\\n=== Principal component analysis ===\\n\\nThe relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\\n\\n\\n=== Mean shift clustering ===\\n\\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\\n\\n\\n=== Independent component analysis ===\\n\\nUnder sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.\\n\\n\\n=== Bilateral filtering ===\\n\\nk-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\\n\\n\\n== Similar problems ==\\nThe set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.\\n\\n\\n== Software implementations ==\\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\\n\\n\\n=== Free Software/Open Source ===\\nThe following implementations are available under Free/Open Source Software licenses, with publicly available source code.\\n\\nAccord.NET contains C# implementations for k-means, k-means++ and k-modes.\\nALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.\\nAOSP contains a Java implementation for k-means.\\nCrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.\\nELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.\\nSmile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).\\nJulia contains a k-means implementation in the JuliaStats Clustering package.\\nKNIME contains nodes for k-means and k-medoids.\\nMahout contains a MapReduce based k-means.\\nmlpack contains a C++ implementation of k-means.\\nOctave contains k-means.\\nOpenCV contains a k-means implementation.\\nOrange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.\\nPSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.\\nR contains three k-means variations.\\nSciPy and scikit-learn contain multiple k-means implementations.\\nSpark MLlib implements a distributed k-means algorithm.\\nTorch contains an unsup package that provides k-means clustering.\\nWeka contains k-means and x-means.\\n\\n\\n=== Proprietary ===\\nThe following implementations are available under proprietary license terms, and may not have publicly available source code.\\n\\n\\n== See also ==\\nBFR algorithm\\nCentroidal Voronoi tessellation\\nHead/tail Breaks\\nk q-flats\\nK-means++\\nLinde–Buzo–Gray algorithm\\nSelf-organizing map\\n\\n\\n== References ==']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "articles=['Suga (rapper)','k-means clustering','Pakistan','k-means clustering','k-means clustering','k-means clustering']\n",
    "wiki_lst=[]\n",
    "title=[]\n",
    "for article in articles:\n",
    "    print(\"loading content: \",article)\n",
    "    wiki_lst.append(wikipedia.page(article).content)\n",
    "    title.append(article)\n",
    "print(\"examine content\")\n",
    "wiki_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939df930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "data= vectorizer.fit_transform(wiki_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c990012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '006', '016', '04', '046', '06', '062', '07', '07bn', '10', '100', '1000', '104', '106th', '107th', '10th', '11', '110th', '116', '118', '1187', '11th', '12', '1206', '1219', '125', '126', '129', '12th', '13', '130', '132', '1366', '139', '13th', '14', '141', '147th', '15', '150', '1500', '151', '1526', '16', '160', '164', '165', '16th', '17', '172', '1739', '174', '1759', '176', '177', '17th', '18', '180', '1839', '1843', '1845', '1849', '185', '1854', '1857', '1858', '1882', '1884', '1893', '18th', '19', '1906', '1919', '1920s', '193', '1930', '1930s', '1933', '1937', '1940', '1940s', '1946', '1947', '1948', '1949', '1950', '1950s', '1953', '1956', '1957', '1958', '1960', '1960s', '1961', '1962', '1963', '1965', '1967', '1968', '1969', '1970', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '198', '1980s', '1982', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19th', '1st', '20', '200', '2000s', '2001', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2030', '2050', '209', '20th', '21', '210', '21st', '22', '22nd', '23', '23rd', '24', '242', '24th', '25', '250', '251', '254', '25th', '26', '263', '26th', '27', '27th', '28', '284', '29', '291', '29th', '2pc', '2ω', '30', '300', '304', '31', '32', '325', '326', '329', '33', '33rd', '34', '340', '342', '35', '354', '35th', '36', '38', '388', '39', '3pc', '3rd', '40', '400', '41', '41st', '42', '428', '42nd', '43', '430', '437', '439', '43rd', '44', '444', '45', '453', '46', '47', '48', '489', '49', '4th', '50', '500', '509', '51', '510', '518', '519', '52', '523', '527', '531', '54', '557', '565', '56th', '58', '585', '591', '5th', '60', '609', '611', '61bn', '62', '632', '636', '64', '642', '650', '651', '66', '660', '668', '67th', '687', '6pc', '6th', '70', '708', '711', '72', '73', '74', '75', '76', '768', '774', '775', '78', '784', '791', '797mwt', '7pc', '7th', '80', '800', '809', '81', '82', '841', '849', '85', '850', '86', '870', '88', '880', '881', '888', '90', '900', '909', '91', '912', '913', '92', '942', '95', '96', '975', '980', '988', '99th', '9th', 'abc', 'abducted', 'abductions', 'abdul', 'abdur', 'abdus', 'able', 'about', 'above', 'abroad', 'absolute', 'abul', 'acacia', 'academia', 'academic', 'academics', 'academy', 'accelerate', 'accelerated', 'acceleration', 'acceptance', 'accepted', 'access', 'accession', 'acclaim', 'acclaimed', 'accommodate', 'accompanied', 'accord', 'accordance', 'according', 'account', 'accounted', 'accounting', 'accounts', 'accurately', 'accused', 'achaemenid', 'achieve', 'achieved', 'achievement', 'achievements', 'achieving', 'acronym', 'across', 'action', 'active', 'activism', 'activist', 'activities', 'actors', 'actresses', 'actual', 'actually', 'adam', 'added', 'addition', 'additional', 'additionally', 'address', 'adequate', 'adherents', 'adil', 'administered', 'administration', 'administrative', 'administratively', 'admiral', 'adolescent', 'adopt', 'adopted', 'adopting', 'adult', 'adulthood', 'advanced', 'advancement', 'advances', 'advantage', 'advantages', 'advent', 'adventurers', 'adverse', 'advertising', 'adviser', 'advisers', 'advisory', 'advocacy', 'advocated', 'aeronomy', 'affair', 'affairs', 'affects', 'affirming', 'afghan', 'afghania', 'afghanistan', 'afghans', 'africa', 'african', 'afro', 'after', 'aftermath', 'again', 'against', 'age', 'aged', 'agencies', 'agency', 'agenda', 'aggression', 'agnostics', 'agree', 'agreed', 'agreeing', 'agreement', 'agreements', 'agricultural', 'agriculture', 'agust', 'ahl', 'ahmad', 'ahmadis', 'ahmadiyya', 'ahmed', 'aid', 'aids', 'ailing', 'aim', 'aimed', 'aims', 'air', 'airblue', 'airfields', 'airlifted', 'airlines', 'airport', 'airports', 'aksai', 'al', 'alam', 'alamgiri', 'album', 'albums', 'alexander', 'algeria', 'alglib', 'algorithm', 'algorithms', 'ali', 'alias', 'alim', 'all', 'allah', 'allama', 'alliance', 'allies', 'allocated', 'allow', 'allowed', 'allows', 'alludes', 'alluvial', 'ally', 'almighty', 'almost', 'alone', 'along', 'alongside', 'alpine', 'also', 'alternate', 'alternated', 'alternating', 'alternative', 'alternatively', 'alternatives', 'although', 'altitudes', 'altogether', 'alvi', 'amalgamating', 'amalgamation', 'ambitions', 'ambitious', 'amendment', 'america', 'american', 'americans', 'amid', 'amin', 'amjad', 'among', 'amongst', 'amount', 'amounting', 'amounts', 'amphibians', 'an', 'analysis', 'analysts', 'analytical', 'ancient', 'and', 'anglo', 'animal', 'animals', 'animated', 'announced', 'annual', 'another', 'answer', 'antarctica', 'anti', 'anwar', 'any', 'anything', 'aosp', 'apart', 'apartment', 'apex', 'apgujeong', 'appeared', 'appendicitis', 'applicable', 'application', 'applications', 'applied', 'apply', 'applying', 'appoint', 'appointed', 'appointing', 'appointments', 'approach', 'approaches', 'appropriate', 'approval', 'approve', 'approximately', 'april', 'arab', 'arabia', 'arabian', 'arabic', 'arakanabad', 'arbitrary', 'arc', 'arch', 'architectural', 'architecture', 'are', 'area', 'areas', 'arg', 'arguably', 'arguing', 'aria', 'arid', 'arise', 'armed', 'armenia', 'armenian', 'arms', 'army', 'around', 'arrangement', 'arranges', 'arranging', 'array', 'arrival', 'arrived', 'arsenals', 'art', 'article', 'articulated', 'artist', 'artistry', 'artists', 'arts', 'aryan', 'as', 'asad', 'ashoka', 'asia', 'asian', 'asiatic', 'asif', 'askari', 'asked', 'aspirations', 'assassination', 'assemblies', 'assembly', 'asserted', 'assertive', 'assessing', 'assign', 'assigned', 'assigning', 'assignment', 'assignments', 'assigns', 'assist', 'assistance', 'assisted', 'associate', 'associated', 'association', 'assumed', 'assumes', 'assumptions', 'astronomy', 'at', 'atal', 'atheist', 'atheists', 'athletes', 'atlas', 'atomic', 'attaches', 'attack', 'attacked', 'attacks', 'attain', 'attaining', 'attempt', 'attempted', 'attempts', 'attend', 'attended', 'attending', 'attention', 'attlee', 'attract', 'attracted', 'attraction', 'attractions', 'attributed', 'attributes', 'audiences', 'augmented', 'august', 'aurangzeb', 'australia', 'austrian', 'authorised', 'authorising', 'authoritarian', 'authority', 'autobiographical', 'autoencoders', 'automated', 'automatic', 'automatically', 'automobile', 'automotive', 'autonomous', 'available', 'aviation', 'avoid', 'awakening', 'awami', 'award', 'awarded', 'awards', 'ayesha', 'ayub', 'azad', 'azam', 'azb', 'azha', 'aḥmad', 'babar', 'babri', 'back', 'backbone', 'backed', 'background', 'backwards', 'bactria', 'badin', 'badly', 'badshahi', 'bahawalpur', 'bahi', 'bahini', 'bahrain', 'bahá', 'bajaur', 'bajwa', 'bakhsh', 'balance', 'balances', 'bales', 'ball', 'baloch', 'balochi', 'balochistan', 'balti', 'baltistan', 'baluchi', 'baluchistan', 'band', 'bandwidth', 'bangla', 'bangladesh', 'bangladeshis', 'bank', 'bankart', 'bankruptcy', 'banned', 'banner', 'barelvi', 'baroness', 'barrier', 'base', 'based', 'basic', 'basis', 'basketball', 'batch', 'battle', 'battleground', 'bayesian', 'bbc', 'bce', 'bcss', 'be', 'bear', 'beautiful', 'became', 'because', 'become', 'becoming', 'beef', 'been', 'before', 'beg', 'began', 'begin', 'beginning', 'beginnings', 'behavior', 'beijing', 'being', 'belief', 'believe', 'believed', 'believers', 'bell', 'belonged', 'belonging', 'belongs', 'below', 'belt', 'benazir', 'benches', 'benchmark', 'bengal', 'bengali', 'bengalis', 'benjamin', 'besides', 'best', 'better', 'between', 'beyond', 'bfr', 'bhutto', 'bibliography', 'bicameral', 'bid', 'big', 'biggest', 'bihari', 'bilal', 'bilateral', 'bilateralism', 'billboard', 'billion', 'bin', 'biology', 'bird', 'birds', 'birthday', 'birthplace', 'bisecting', 'black', 'blend', 'blind', 'bloc', 'blocked', 'bloodless', 'boar', 'board', 'body', 'boldsymbol', 'bollywood', 'bolstered', 'boltzmann', 'bomb', 'bombarded', 'bonds', 'book', 'boom', 'boost', 'border', 'bordered', 'borders', 'born', 'bosnia', 'bosnian', 'boston', 'both', 'bound', 'bounded', 'bounds', 'boy', 'bradley', 'brahui', 'brahuis', 'brain', 'branch', 'branches', 'brand', 'brash', 'break', 'breaks', 'breakthrough', 'brethren', 'brics', 'bridge', 'briefing', 'briefly', 'bring', 'bringing', 'brink', 'british', 'broadcasting', 'broadleaf', 'bronze', 'brotherhood', 'brothers', 'brought', 'brown', 'brt', 'brts', 'bt21', 'bts', 'buddhism', 'buddhist', 'budget', 'build', 'building', 'buildings', 'bulleh', 'bureau', 'bureaucracy', 'bureaus', 'burma', 'burmese', 'burmi', 'burushaski', 'bus', 'business', 'businessmen', 'but', 'buy', 'buyer', 'buys', 'buzo', 'by', 'cabinet', 'caching', 'calculate', 'calculates', 'calculation', 'calculations', 'calendar', 'caliphate', 'callard', 'called', 'calligraphy', 'calling', 'cambridge', 'came', 'camp', 'campaigner', 'campaigns', 'camps', 'can', 'canada', 'cancer', 'candidates', 'candu', 'cannot', 'capability', 'capacity', 'capita', 'capital', 'caps', 'cardinal', 'care', 'career', 'carried', 'carrier', 'carries', 'carry', 'case', 'cases', 'caste', 'casualties', 'cat', 'catholic', 'caused', 'causes', 'cdot', 'cdots', 'ce', 'ceasefire', 'cedar', 'cede', 'celebi', 'celebration', 'celebrations', 'celebrity', 'cells', 'cement', 'censoring', 'census', 'center', 'centered', 'centers', 'central', 'centre', 'centred', 'centres', 'centrifuge', 'centrist', 'centroid', 'centroidal', 'centroids', 'centuries', 'century', 'ceremonial', 'certain', 'certificate', 'chagai', 'chain', 'chair', 'chairman', 'challenge', 'challenges', 'championed', 'champions', 'championship', 'championships', 'chances', 'chandragupta', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'character', 'characterised', 'chart', 'charted', 'charting', 'chashma', 'chasnupp', 'chaudhry', 'checks', 'chemistry', 'chemists', 'chief', 'chiefs', 'chiffon', 'child', 'children', 'chili', 'chin', 'china', 'chinese', 'chiniot', 'chinkara', 'chitral', 'choice', 'cholistan', 'chomsky', 'choose', 'chooses', 'choreography', 'chosen', 'choudhry', 'christian', 'christianity', 'christians', 'christmas', 'chronology', 'cid', 'cinemas', 'circular', 'cites', 'cities', 'citizen', 'citizens', 'city', 'civet', 'civic', 'civil', 'civilian', 'civilians', 'civilisation', 'civilisations', 'civilization', 'cjcsc', 'claim', 'claimed', 'claiming', 'claims', 'clandestine', 'clash', 'clashes', 'class', 'classes', 'classical', 'classification', 'classifier', 'classifiers', 'classifies', 'clear', 'clearly', 'clemency', 'clement', 'clergy', 'climate', 'clinker', 'close', 'closed', 'closely', 'closer', 'closest', 'clothes', 'clothing', 'cluster', 'clustered', 'clustering', 'clusters', 'co', 'coal', 'coalesced', 'coalition', 'coalitions', 'coarse', 'coast', 'coastal', 'coastline', 'coconut', 'code', 'codebook', 'coexist', 'coined', 'cold', 'collaborating', 'collaboration', 'college', 'collins', 'colonial', 'colonisation', 'colony', 'color', 'colors', 'colourful', 'colours', 'combination', 'combined', 'come', 'comes', 'coming', 'command', 'commander', 'commanders', 'commandos', 'commemorating', 'commenced', 'commercial', 'commercially', 'commission', 'committed', 'committee', 'commodities', 'common', 'commonly', 'commonwealth', 'communication', 'communications', 'communist', 'communities', 'community', 'companies', 'companions', 'company', 'comparable', 'compared', 'comparing', 'competition', 'competitive', 'competitiveness', 'compilation', 'compiler', 'complain', 'complaining', 'complete', 'completed', 'complex', 'complexity', 'component', 'components', 'composed', 'composes', 'composing', 'compound', 'comprehensive', 'comprised', 'comprises', 'comprising', 'compulsory', 'computational', 'computationally', 'compute', 'computer', 'computers', 'computes', 'computing', 'conceived', 'concentratd', 'concentrated', 'concentration', 'concept', 'concern', 'concluded', 'condensed', 'conditioned', 'conditions', 'conduct', 'conducted', 'conduit', 'confederacy', 'conference', 'confidence', 'configuration', 'confirmation', 'confirmations', 'confirmed', 'conflict', 'conflicts', 'confused', 'congress', 'coniferous', 'connect', 'connecting', 'conquered', 'conqueror', 'conquest', 'consciousness', 'consequently', 'conservatism', 'conservative', 'considerable', 'considered', 'consist', 'consisted', 'consistently', 'consisting', 'consists', 'consolidating', 'constant', 'constituencies', 'constituent', 'constituents', 'constitutes', 'constitution', 'constitutional', 'constitutions', 'constraints', 'construction', 'constructions', 'consult', 'consultancy', 'consume', 'consumed', 'consumption', 'contain', 'contained', 'containing', 'contains', 'contemporary', 'contention', 'contested', 'continent', 'continuation', 'continue', 'continued', 'continues', 'continuous', 'contracts', 'contrary', 'contrast', 'contributed', 'contributes', 'contributing', 'contribution', 'contributions', 'contributor', 'control', 'controlled', 'controls', 'controversy', 'conventional', 'converge', 'converged', 'convergence', 'converges', 'converging', 'conversions', 'converting', 'convex', 'convinced', 'cooking', 'cool', 'cooled', 'cooperation', 'coordinated', 'coordinates', 'copied', 'copyright', 'coronavirus', 'corporation', 'corporations', 'corporatisation', 'corps', 'correct', 'correlated', 'corresponds', 'corridor', 'corroborated', 'corruption', 'cost', 'costing', 'cotton', 'could', 'council', 'councils', 'count', 'countdown', 'counted', 'counter', 'counterexamples', 'countering', 'counterintuitive', 'counterpart', 'counterterrorism', 'countries', 'country', 'coup', 'coups', 'course', 'courses', 'court', 'courts', 'covariance', 'covariances', 'cover', 'covering', 'covers', 'crackdown', 'crash', 'create', 'created', 'creates', 'creation', 'credible', 'credited', 'credits', 'creed', 'crew', 'cricket', 'crime', 'crimes', 'crimestat', 'crisis', 'criteria', 'critical', 'critically', 'criticise', 'criticism', 'critics', 'crocodiles', 'crop', 'cropping', 'crossroads', 'crows', 'crude', 'cuisine', 'cultivation', 'cultural', 'culture', 'cultures', 'cup', 'curlie', 'currency', 'current', 'currently', 'curriculum', 'curry', 'customary', 'customs', 'cutting', 'cwas', 'cyber', 'cyclone', 'daechwita', 'daegu', 'daily', 'dance', 'daro', 'data', 'dataset', 'datasets', 'date', 'datum', 'daughter', 'day', 'de', 'dead', 'deal', 'death', 'deaths', 'debut', 'debuted', 'decade', 'decades', 'december', 'decided', 'deciduous', 'decision', 'decisions', 'declaration', 'declared', 'declares', 'decline', 'declined', 'declining', 'decreased', 'deduced', 'deep', 'deepest', 'deer', 'defeat', 'defeating', 'defence', 'defend', 'defended', 'defense', 'deficiencies', 'define', 'defined', 'deforestation', 'degree', 'degrees', 'delegated', 'delhi', 'delta', 'demand', 'demanded', 'demands', 'demetrius', 'democracy', 'democratic', 'demographic', 'demographics', 'demolition', 'demons', 'demonstrating', 'demonstration', 'denied', 'denoising', 'denominational', 'deobandi', 'deodar', 'department', 'depend', 'dependence', 'dependent', 'depending', 'depends', 'deployed', 'deployment', 'depression', 'deregulated', 'derived', 'descent', 'describe', 'described', 'describing', 'description', 'descriptions', 'desert', 'deserts', 'design', 'designated', 'designation', 'designed', 'designs', 'desire', 'desired', 'despite', 'destinations', 'determination', 'determine', 'determines', 'determining', 'deterministic', 'deterrence', 'devapala', 'devastating', 'develop', 'developed', 'developing', 'development', 'developments', 'deviations', 'devised', 'dharma', 'dharmapala', 'diagnosed', 'diagnostic', 'diagonal', 'diagram', 'diaspora', 'dichotomy', 'dictionary', 'did', 'died', 'diego', 'difference', 'differences', 'different', 'difficult', 'difficulties', 'digital', 'diji', 'dimension', 'dimensional', 'dimensions', 'diplomatic', 'direct', 'direction', 'directions', 'directly', 'director', 'directorate', 'disapproval', 'disaster', 'disc', 'discharged', 'disco', 'discography', 'discovered', 'discrete', 'discussed', 'discusses', 'discussion', 'dish', 'dishes', 'disintegration', 'disobedience', 'displaced', 'displaystyle', 'dispute', 'disputed', 'disqualification', 'disruptions', 'distance', 'distances', 'distinct', 'distribute', 'distributed', 'distribution', 'distributions', 'district', 'districts', 'divergence', 'diverse', 'diversified', 'diversity', 'divided', 'dividend', 'division', 'divisions', 'diwali', 'dk', 'dn', 'do', 'doctor', 'doctrine', 'document', 'documented', 'does', 'dollars', 'dolls', 'dolphin', 'domain', 'domains', 'domestic', 'domestically', 'dominant', 'dominate', 'dominated', 'dominates', 'dominion', 'don', 'donated', 'dong', 'dongsan', 'door', 'dormitories', 'dots', 'down', 'downloads', 'downturn', 'dozen', 'dr', 'draft', 'drafted', 'dramas', 'dravidian', 'drawbacks', 'drawn', 'dream', 'dreamed', 'dreams', 'dress', 'drink', 'driver', 'drought', 'dry', 'dt', 'dubbed', 'due', 'durand', 'during', 'durrani', 'durranis', 'duty', 'dwellers', 'dying', 'dynasties', 'dynasty', 'each', 'eagles', 'ear', 'earliest', 'early', 'earning', 'earnings', 'earthquake', 'earthquakes', 'easily', 'easing', 'east', 'easter', 'eastern', 'easy', 'eco', 'economic', 'economics', 'economies', 'economists', 'economy', 'ecosystem', 'edge', 'edged', 'educated', 'education', 'educational', 'edward', 'effectively', 'effects', 'efficient', 'efficiently', 'effort', 'efforts', 'egypt', 'eid', 'eight', 'eighth', 'either', 'elected', 'election', 'elections', 'electoral', 'electric', 'electrical', 'electricity', 'electronics', 'electroweak', 'elementary', 'elements', 'elevated', 'eleven', 'elia', 'elite', 'elites', 'elki', 'em', 'embargo', 'embarked', 'emerged', 'emergence', 'emergency', 'emerging', 'emigrants', 'emigrated', 'emperors', 'emphasis', 'emphasising', 'empire', 'empires', 'employed', 'employees', 'employment', 'empowerment', 'enacted', 'enclave', 'encoding', 'encompassing', 'encourage', 'encouraged', 'encouraging', 'end', 'endeavour', 'ended', 'endowments', 'energy', 'enforced', 'enforcement', 'enforcing', 'engaged', 'engagement', 'england', 'enjoy', 'enjoyed', 'enlightened', 'enlightenment', 'enrichment', 'enrolled', 'enrollment', 'ensure', 'entertainment', 'entire', 'entirety', 'entities', 'entitled', 'entity', 'entrusts', 'entry', 'environment', 'envisioned', 'envoy', 'ep', 'ephedra', 'epik', 'equal', 'equality', 'equipment', 'equitable', 'equivalence', 'equivalent', 'era', 'eradicating', 'eritrea', 'error', 'errors', 'escalated', 'escape', 'escorts', 'especially', 'espouse', 'essa', 'essential', 'essentially', 'establish', 'established', 'establishing', 'establishment', 'estate', 'estimate', 'estimated', 'estimates', 'et', 'etc', 'eternal', 'ethiopia', 'ethnic', 'ethnically', 'etiquette', 'etymology', 'eucalyptus', 'euclidean', 'eurasia', 'eurasian', 'europe', 'european', 'europeans', 'evaluate', 'evaluated', 'evaluation', 'even', 'evenly', 'event', 'events', 'eventually', 'ever', 'every', 'evocative', 'evolve', 'evolved', 'exactly', 'examinations', 'example', 'examples', 'exams', 'exceed', 'except', 'exceptional', 'excessively', 'exchange', 'exclave', 'excludes', 'exclusively', 'executed', 'execution', 'executive', 'exercise', 'exercised', 'exercises', 'exhibit', 'exist', 'existence', 'existing', 'expand', 'expanded', 'expanse', 'expansion', 'expectation', 'expected', 'expects', 'expense', 'experience', 'experienced', 'experts', 'explained', 'explaining', 'explicit', 'explored', 'exponential', 'exponentially', 'export', 'exported', 'exporter', 'exports', 'exposing', 'expressed', 'expressways', 'exquisite', 'extended', 'extending', 'extensive', 'extent', 'external', 'extreme', 'extremely', 'extremists', 'fabric', 'face', 'faced', 'facilitate', 'facilitates', 'facilities', 'fact', 'factbook', 'factions', 'facto', 'factor', 'factors', 'failed', 'fails', 'failure', 'fair', 'faisal', 'faisalabad', 'faith', 'faiz', 'falcons', 'fall', 'falling', 'fame', 'families', 'family', 'famous', 'fanbase', 'fans', 'farid', 'farm', 'farooq', 'fashion', 'fast', 'faster', 'fastest', 'fate', 'father', 'fathers', 'fauna', 'favors', 'favour', 'fayyad', 'fazlul', 'fear', 'fearful', 'fears', 'feats', 'feature', 'featured', 'features', 'featuring', 'february', 'federal', 'federally', 'federation', 'feel', 'feeling', 'feet', 'fell', 'female', 'feroze', 'festivals', 'few', 'fia', 'fibre', 'fiction', 'field', 'fields', 'fifa', 'fifth', 'fig', 'fighter', 'fighters', 'fighting', 'figure', 'figures', 'film', 'filmography', 'films', 'filter', 'filtering', 'final', 'finance', 'financed', 'financial', 'find', 'finding', 'finds', 'finishing', 'first', 'fiscal', 'fish', 'fishing', 'fissile', 'fit', 'fitr', 'five', 'fixed', 'fixing', 'flatbread', 'flats', 'fledged', 'fleeing', 'flexible', 'flooding', 'flora', 'flourish', 'flourished', 'flow', 'flower', 'flt', 'fluctuations', 'flying', 'flyover', 'flyovers', 'focal', 'focus', 'focused', 'folk', 'folkloric', 'follow', 'followed', 'following', 'foment', 'food', 'football', 'for', 'forall', 'force', 'forced', 'forces', 'fore', 'forecasts', 'foreign', 'forest', 'forested', 'forests', 'forever', 'forged', 'forgy', 'form', 'formal', 'formally', 'formed', 'former', 'formerly', 'forming', 'forms', 'formulating', 'fort', 'fortress', 'fortresses', 'forum', 'forward', 'fossil', 'fought', 'found', 'foundation', 'foundations', 'founded', 'founder', 'founding', 'four', 'fourteen', 'fourth', 'fp', 'frac', 'framework', 'franca', 'france', 'free', 'freedom', 'freelancing', 'freest', 'freight', 'frequent', 'frequently', 'freshwater', 'fried', 'friendly', 'from', 'frontier', 'ft', 'fuels', 'fulfilled', 'full', 'fully', 'function', 'functional', 'functions', 'fund', 'fundamentalists', 'further', 'furthermore', 'fuse', 'fused', 'future', 'futures', 'fuzzy', 'g20', 'gadani', 'gained', 'gaining', 'gains', 'gallup', 'game', 'games', 'gandhara', 'gandhāran', 'gaon', 'garam', 'gardens', 'garlic', 'gas', 'gate', 'gateway', 'gathered', 'gathering', 'gatherings', 'gaussian', 'gayaki', 'gdp', 'general', 'generalization', 'generalized', 'generally', 'generals', 'generated', 'generates', 'generation', 'generations', 'genetic', 'genetics', 'genocide', 'genre', 'geographic', 'geography', 'geologically', 'geometric', 'geometry', 'geopolitical', 'geopolitically', 'geostrategy', 'german', 'get', 'getting', 'ghazal', 'ghazali', 'ghaznavid', 'ghaznavids', 'ghorid', 'ghq', 'gi', 'gianchandani', 'gilgit', 'gillani', 'ginger', 'girl', 'given', 'gives', 'giving', 'glaciated', 'global', 'globalisation', 'globalization', 'globally', 'gloss', 'goal', 'goan', 'goat', 'god', 'godhra', 'goes', 'gojri', 'gold', 'golden', 'goldman', 'good', 'got', 'gothic', 'govern', 'governance', 'governed', 'government', 'governments', 'governor', 'governors', 'govpubs', 'grades', 'graduate', 'graduating', 'grand', 'grant', 'graphics', 'grasses', 'gray', 'grazing', 'great', 'greatest', 'greatly', 'greco', 'greek', 'green', 'grew', 'gross', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'growth', 'guarantee', 'guaranteed', 'guarantees', 'guard', 'guardian', 'guarding', 'guards', 'gujarati', 'gujranwala', 'gujrat', 'gulf', 'gulgee', 'gunman', 'gupta', 'gwadar', 'gwaneum', 'gwangju', 'had', 'hadith', 'haj', 'hajweri', 'half', 'halfway', 'halls', 'halsey', 'halwa', 'hamerly', 'hamid', 'hand', 'handle', 'hands', 'hannam', 'hanyang', 'haq', 'haque', 'harappa', 'harbour', 'hard', 'hardcore', 'hardest', 'hardness', 'hares', 'hari', 'harmonic', 'haroon', 'hartigan', 'has', 'hasan', 'hassan', 'hastened', 'have', 'having', 'hawks', 'hazara', 'hazaras', 'he', 'head', 'headed', 'heads', 'healed', 'health', 'heard', 'hearing', 'heatseekers', 'heavy', 'hectares', 'heights', 'heize', 'held', 'help', 'helped', 'helpful', 'hence', 'her', 'herbs', 'heritage', 'herman', 'heu', 'heuristic', 'heuristics', 'hidden', 'hides', 'hierarchical', 'hierarchy', 'high', 'higher', 'highest', 'highlands', 'highlighted', 'highlights', 'highway', 'highways', 'hill', 'hills', 'him', 'himalaya', 'himalayan', 'himself', 'hindered', 'hindko', 'hindkowans', 'hindu', 'hinduism', 'hindus', 'hindustani', 'hinglaj', 'hip', 'hippie', 'his', 'historian', 'historically', 'history', 'hit', 'hockey', 'holding', 'holi', 'holy', 'home', 'homegrown', 'homeland', 'homes', 'hometown', 'homosexuality', 'honoured', 'hop', 'hope', 'horizon', 'hospital', 'hospitalized', 'host', 'hostage', 'hosted', 'hosts', 'hot', 'hour', 'hours', 'house', 'housing', 'how', 'however', 'hq', 'huge', 'hugo', 'human', 'hunting', 'hunza', 'husseini', 'hwagwan', 'hybrid', 'hyderabad', 'hydroelectric', 'hyenas', 'hyphen', 'hyvs', 'iaea', 'ib', 'ibex', 'ibn', 'ica', 'icc', 'ict', 'idea', 'ideas', 'identical', 'identified', 'identify', 'identity', 'ideological', 'ideologies', 'ideology', 'idol', 'if', 'igniting', 'ii', 'iii', 'illegal', 'illiteracy', 'image', 'immigration', 'impact', 'impartial', 'impeachment', 'implementation', 'implementations', 'implements', 'implicitly', 'importance', 'important', 'imported', 'importer', 'importing', 'imports', 'imposed', 'imposition', 'impressive', 'improve', 'improved', 'improvement', 'improves', 'improving', 'imran', 'in', 'inappropriate', 'inaugural', 'inaugurated', 'inception', 'include', 'included', 'includes', 'including', 'income', 'incorporated', 'increase', 'increased', 'increases', 'increasing', 'increasingly', 'incremental', 'incursion', 'indeed', 'independence', 'independent', 'independently', 'index', 'indexes', 'india', 'indian', 'indicate', 'indicated', 'indicator', 'indicators', 'indirectly', 'individual', 'individualistic', 'indo', 'indonesia', 'indus', 'industrial', 'industrialized', 'industry', 'inefficient', 'inequality', 'inevitably', 'infinitesimal', 'inflammation', 'inflated', 'inflation', 'inflationary', 'inflow', 'influence', 'influenced', 'influential', 'information', 'infrastructural', 'infrastructure', 'inhabitants', 'inherited', 'initial', 'initialization', 'initializations', 'initially', 'initials', 'initiative', 'initiatives', 'injunctions', 'injury', 'inland', 'innovation', 'input', 'inputs', 'insects', 'inspector', 'inspirations', 'instability', 'installed', 'installing', 'instance', 'instead', 'institute', 'institutions', 'instruction', 'instrument', 'insurgency', 'insurgents', 'integer', 'integral', 'integrated', 'integrity', 'intellectual', 'intellectualism', 'intelligence', 'intelligent', 'intensified', 'intensity', 'intent', 'intention', 'inter', 'interaction', 'intercession', 'interchange', 'interest', 'interested', 'interests', 'interference', 'interlude', 'intermediate', 'internal', 'international', 'internet', 'interpretation', 'interpreted', 'intertwined', 'intervention', 'into', 'intolerance', 'intro', 'introduced', 'introduction', 'introductory', 'intros', 'intuition', 'intuitive', 'invading', 'invasion', 'invasions', 'invented', 'invertebrates', 'investigation', 'investment', 'invited', 'involve', 'involved', 'involvement', 'involves', 'iqbal', 'iran', 'iranian', 'iraq', 'iris', 'is', 'isi', 'iskander', 'islam', 'islamabad', 'islami', 'islamiat', 'islamic', 'islamisation', 'islamist', 'islamistan', 'islamists', 'israel', 'israeli', 'issue', 'issues', 'it', 'iterated', 'iteration', 'iterations', 'iterative', 'iteratively', 'its', 'itself', 'iu', 'iv', 'iwf', 'jackal', 'jackals', 'jae', 'jahan', 'jahangir', 'jainism', 'jains', 'jalal', 'jalalabad', 'jalaludin', 'jamaat', 'james', 'jammu', 'january', 'japan', 'japanese', 'java', 'javed', 'javid', 'jawaharlal', 'jawed', 'jeans', 'jeff', 'jenks', 'jhelum', 'jihad', 'jinnah', 'job', 'johar', 'join', 'joining', 'joint', 'jolted', 'jon', 'jordan', 'josh', 'journal', 'js', 'judge', 'judicature', 'judicial', 'judiciary', 'juice', 'julia', 'juliastats', 'july', 'june', 'jungle', 'juniper', 'jurisdiction', 'jurisdictions', 'just', 'justice', 'k2', 'kabul', 'kahuta', 'kalam', 'kalash', 'kalasha', 'kaleemullah', 'kalich', 'kameez', 'kanupp', 'karachi', 'karakoram', 'kargil', 'karimabad', 'kariwood', 'kashgar', 'kashmir', 'kashmiri', 'kashmiris', 'kcr', 'kd', 'kearney', 'keimyung', 'keith', 'kept', 'kernel', 'key', 'khalid', 'khaliq', 'khaliquzzaman', 'khan', 'khawaja', 'khyber', 'killed', 'kilometre', 'kilometres', 'kingdom', 'kingdoms', 'kippur', 'kitchens', 'km', 'km2', 'knime', 'known', 'kohistan', 'korangi', 'korea', 'korean', 'kot', 'kotlin', 'kr', 'krishna', 'krl', 'kumail', 'kush', 'kushan', 'kuwait', 'l_', 'labelled', 'laboratories', 'labour', 'labrum', 'labs', 'lack', 'ladakh', 'lahore', 'laid', 'lake', 'lalian', 'land', 'landowners', 'landscape', 'landscapes', 'lane', 'language', 'languages', 'lankan', 'lapierre', 'large', 'largely', 'larger', 'largest', 'larkana', 'lassi', 'last', 'late', 'later', 'latif', 'latter', 'lattice', 'launch', 'launched', 'launching', 'laureate', 'law', 'laws', 'layer', 'laying', 'ldots', 'lead', 'leader', 'leaders', 'leadership', 'leading', 'league', 'leagues', 'learn', 'learned', 'learning', 'least', 'lecture', 'led', 'lee', 'left', 'leftism', 'leftist', 'legal', 'legislative', 'legislators', 'legislature', 'length', 'lentils', 'leo', 'leopard', 'leopards', 'leq', 'less', 'let', 'letter', 'letters', 'level', 'levels', 'lgbtq', 'liaquat', 'liberalism', 'liberation', 'libraries', 'library', 'license', 'licensed', 'licenses', 'lie', 'lies', 'life', 'lift', 'like', 'likelihood', 'likely', 'likewise', 'limit', 'limitation', 'limited', 'limiting', 'limits', 'linde', 'line', 'linear', 'lines', 'lingua', 'linguistically', 'link', 'linkages', 'links', 'literacy', 'literally', 'literary', 'literature', 'little', 'live', 'lives', 'living', 'lloyd', 'loan', 'lobbied', 'loc', 'local', 'located', 'location', 'locations', 'lodging', 'lodi', 'log', 'logistics', 'lollywood', 'london', 'long', 'longer', 'longest', 'loose', 'lord', 'loss', 'lost', 'lot', 'love', 'low', 'lower', 'lowest', 'lsm', 'lt', 'lvert', 'lyric', 'lyrics', 'm1', 'm_', 'machchar', 'machine', 'machines', 'macqueen', 'macroeconomics', 'macromanagement', 'made', 'madrassahs', 'magistrate', 'maharaja', 'mahout', 'main', 'mainly', 'mainstream', 'maintain', 'maintained', 'maintaining', 'maintains', 'maintenance', 'major', 'majority', 'make', 'makes', 'making', 'malala', 'male', 'malihabadi', 'malik', 'mammals', 'managed', 'mandated', 'mangrove', 'mangroves', 'manic', 'manpower', 'manto', 'manufactured', 'manufacturer', 'manufacturing', 'many', 'manzoor', 'map', 'mapreduce', 'maratha', 'march', 'marco', 'mardan', 'marginal', 'marine', 'marines', 'maritime', 'mark', 'marked', 'market', 'markhor', 'marshal', 'martensite', 'martial', 'marwari', 'marxism', 'masala', 'masjid', 'mass', 'massive', 'master', 'masterminded', 'masters', 'material', 'materials', 'mathbf', 'mathematically', 'mathematics', 'matrices', 'matriculation', 'matrix', 'matter', 'matters', 'maududi', 'maulana', 'maurya', 'mavalvala', 'mawdudi', 'maximin', 'maximization', 'maximize', 'maximizing', 'maximum', 'may', 'mayor', 'mazar', 'mean', 'meaning', 'means', 'meant', 'meanwhile', 'measure', 'measures', 'meat', 'mecca', 'medals', 'media', 'median', 'medians', 'medieval', 'medium', 'medoid', 'medoids', 'meetings', 'mega', 'mehrgarh', 'melon', 'member', 'members', 'memorandums', 'memory', 'men', 'menander', 'menon', 'mental', 'merit', 'metaheuristics', 'metal', 'method', 'methodological', 'methods', 'metres', 'metric', 'metro', 'metrobus', 'metropolis', 'metropolitan', 'mi', 'mian', 'miani', 'mid', 'middle', 'midi', 'midway', 'mig', 'migrants', 'migrate', 'migrated', 'migration', 'migratory', 'mile', 'miles', 'militants', 'militaries', 'military', 'milk', 'millennium', 'million', 'millions', 'min', 'minar', 'mind', 'mini', 'minima', 'minimize', 'minimizes', 'minimizing', 'minimum', 'miniseries', 'minister', 'ministers', 'ministry', 'minkowski', 'minorities', 'minority', 'mirage', 'mirpurkhas', 'mirza', 'missiles', 'mission', 'missionaries', 'missions', 'mistrust', 'mit', 'mixed', 'mixes', 'mixtape', 'mixtapes', 'mixture', 'mixtures', 'mk', 'mllib', 'mlpack', 'mnet', 'model', 'modeling', 'modelled', 'modelling', 'models', 'moderation', 'modern', 'modes', 'modest', 'modifications', 'modulation', 'mogadishu', 'mohammad', 'mohenjo', 'moist', 'mol', 'molecular', 'moment', 'monarchs', 'monarchy', 'monasteries', 'monastery', 'monetary', 'money', 'mongooses', 'monitor', 'monologue', 'monsoon', 'month', 'moon', 'mor', 'morality', 'more', 'morocco', 'morphology', 'morr', 'mosque', 'mosques', 'most', 'mostly', 'motivated', 'motorway', 'motorways', 'mountain', 'mountaineers', 'mountainous', 'mountains', 'mountbatten', 'mous', 'mouse', 'move', 'moved', 'movement', 'movements', 'moves', 'mu', 'much', 'mud', 'muffin', 'mufti', 'mugger', 'mughal', 'mughals', 'muhajirs', 'muhammad', 'mujahideen', 'mukti', 'mulberry', 'multan', 'multi', 'multiple', 'multivariate', 'muluk', 'mureed', 'musharraf', 'music', 'musical', 'muslim', 'muslims', 'must', 'mustafa', 'mutiny', 'mutuality', 'mwe', 'my', 'myanmar', 'mynas', 'mystical', 'nacta', 'nadeem', 'nader', 'nadia', 'naive', 'najam', 'name', 'named', 'namely', 'names', 'nanga', 'nanjiani', 'narrative', 'narrow', 'narrowly', 'nasim', 'nasir', 'nathiagali', 'nation', 'national', 'nationalism', 'nationalist', 'nationalists', 'nationality', 'nations', 'nationwide', 'native', 'nato', 'natural', 'nature', 'naval', 'navy', 'nawab', 'nawaz', 'nazimuddin', 'naïve', 'near', 'nearby', 'nearest', 'nearly', 'necessarily', 'neckties', 'need', 'needed', 'needs', 'neem', 'negative', 'neglect', 'nehru', 'neighbor', 'neighborhood', 'neighboring', 'neighbouring', 'neolithic', 'nepra', 'neq', 'nergis', 'net', 'network', 'networked', 'neurosurgeon', 'never', 'new', 'newer', 'newly', 'news', 'newspapers', 'next', 'niazi', 'nijat', 'nilgai', 'nine', 'nixon', 'nizam', 'nkdi', 'nlp', 'no', 'noam', 'nobel', 'nodes', 'nominal', 'nominations', 'non', 'none', 'nonetheless', 'nonparametric', 'nor', 'norm', 'normal', 'north', 'northeast', 'northern', 'northernmost', 'northwestern', 'norway', 'not', 'notable', 'notably', 'noted', 'notes', 'november', 'now', 'nowshera', 'np', 'nuclear', 'number', 'numbering', 'numbers', 'numerical', 'nursery', 'object', 'objective', 'objectives', 'objects', 'obligation', 'observation', 'observations', 'observatory', 'observed', 'obtained', 'obtains', 'occasional', 'occasionally', 'occupied', 'occupies', 'occurred', 'octave', 'october', 'of', 'off', 'offer', 'offering', 'office', 'officer', 'officers', 'offices', 'official', 'officially', 'often', 'oic', 'oil', 'old', 'oldest', 'olympic', 'oman', 'omega', 'ommaya', 'on', 'once', 'one', 'only', 'onto', 'opd', 'open', 'opencv', 'opened', 'opening', 'openly', 'openstreetmap', 'operated', 'operates', 'operating', 'operation', 'operational', 'operationalising', 'operations', 'operatives', 'operatorname', 'opf', 'opinion', 'opportunities', 'opposed', 'opposition', 'optics', 'optima', 'optimal', 'optimality', 'optimization', 'optimized', 'optimizes', 'optimum', 'or', 'orange', 'order', 'ordered', 'ordering', 'ordination', 'organisation', 'organised', 'organises', 'organising', 'organization', 'organizes', 'organizing', 'oriented', 'origin', 'originally', 'originated', 'originates', 'orphanages', 'other', 'others', 'otsu', 'out', 'outbreak', 'outlets', 'outline', 'outlook', 'outside', 'outspoken', 'over', 'overall', 'overlaps', 'oversaw', 'overseas', 'oversee', 'oversees', 'overthrown', 'overwhelming', 'own', 'owned', 'package', 'paec', 'paf', 'pahari', 'paintings', 'pairwise', 'pak', 'pakhtunkhwa', 'pakhtunkwa', 'pakistan', 'pakistani', 'pakistanis', 'pala', 'palas', 'paleolithic', 'palestine', 'palette', 'palm', 'palms', 'pam', 'pamir', 'pamphlet', 'pan', 'pangolin', 'papers', 'paradigm', 'parallel', 'parallelized', 'parameter', 'parameters', 'paramilitary', 'parbat', 'pardon', 'parity', 'parks', 'parliament', 'parliamentary', 'parsi', 'part', 'partially', 'participant', 'participate', 'participating', 'particular', 'particularly', 'parties', 'partition', 'partitioned', 'partitioning', 'partitions', 'partly', 'partner', 'partnership', 'parts', 'party', 'pashto', 'pashtuns', 'pasni', 'pass', 'passage', 'passed', 'passengers', 'passes', 'past', 'pathans', 'patients', 'patrol', 'patron', 'patterns', 'pbc', 'pca', 'pcs', 'peacekeeping', 'peaked', 'peaks', 'pediatric', 'penal', 'penetration', 'people', 'peoples', 'per', 'perceived', 'percent', 'perform', 'performance', 'performances', 'performed', 'performing', 'performs', 'period', 'periodic', 'periods', 'permanent', 'persecuted', 'persecution', 'persian', 'personal', 'personnel', 'perturbed', 'pervez', 'peshawar', 'pew', 'pfdc', 'pff', 'phase', 'phases', 'philanthropy', 'philippines', 'philosophers', 'philosophical', 'philosophy', 'phobia', 'physical', 'physicist', 'physics', 'pia', 'piano', 'picked', 'pictureless', 'pilgrimage', 'pilgrims', 'pilots', 'pine', 'pir', 'pirs', 'pivotal', 'pixels', 'place', 'places', 'placing', 'plain', 'plains', 'plan', 'plane', 'planes', 'planned', 'planning', 'plans', 'plant', 'plants', 'plate', 'plateau', 'plateaus', 'play', 'played', 'players', 'playful', 'playing', 'plays', 'plebiscite', 'pml', 'poet', 'poetry', 'poets', 'point', 'points', 'police', 'policies', 'policy', 'political', 'politicians', 'politics', 'poll', 'pollution', 'polo', 'polynomial', 'poor', 'poorer', 'poorly', 'pop', 'populace', 'popular', 'popularity', 'popularly', 'population', 'populations', 'populist', 'populous', 'porcupines', 'port', 'portion', 'ports', 'poses', 'position', 'positions', 'possible', 'post', 'postgraduate', 'posthumous', 'potential', 'poverty', 'power', 'powerful', 'powers', 'ppp', 'practice', 'praise', 'pre', 'preamble', 'precarious', 'preceded', 'precision', 'predicted', 'predominant', 'predominantly', 'preemptive', 'prefer', 'preferable', 'preferred', 'premier', 'preparatory', 'preprocessing', 'prescribed', 'presence', 'present', 'presented', 'preserve', 'president', 'presidential', 'presidents', 'press', 'pressure', 'pressures', 'prevent', 'preventing', 'prevention', 'previous', 'previously', 'primarily', 'primary', 'prime', 'princely', 'principal', 'principally', 'principle', 'principles', 'print', 'prior', 'prison', 'private', 'privately', 'prize', 'pro', 'probabilistic', 'probability', 'probably', 'problem', 'problems', 'proceeds', 'process', 'processed', 'processing', 'produce', 'produced', 'producer', 'producers', 'produces', 'producing', 'product', 'production', 'productions', 'productivity', 'products', 'profess', 'professionally', 'professor', 'professors', 'profitable', 'program', 'programmes', 'programming', 'progress', 'progresses', 'project', 'projected', 'projection', 'projects', 'proliferation', 'prolonged', 'prominence', 'prominent', 'prominently', 'promise', 'promote', 'promoted', 'promotes', 'promotional', 'prompted', 'promulgating', 'prone', 'propagate', 'propensity', 'properly', 'prophet', 'proponent', 'proportion', 'proportional', 'proportionately', 'proposed', 'proprietary', 'prose', 'prospect', 'prospering', 'protect', 'protected', 'protection', 'protesters', 'prototype', 'prototypical', 'protracted', 'provable', 'proven', 'provenly', 'provide', 'provided', 'provides', 'providing', 'province', 'provinces', 'provincial', 'proximity', 'proxy', 'pspp', 'psy', 'pt', 'pti', 'ptv', 'public', 'publications', 'publicly', 'published', 'publishes', 'publishing', 'pulse', 'punishable', 'punjab', 'punjabi', 'punjabis', 'purchase', 'purchased', 'purchasing', 'pure', 'purer', 'purity', 'purpose', 'pursue', 'pursued', 'push', 'put', 'puts', 'putting', 'pāk', 'qadeer', 'qalander', 'qamar', 'qasim', 'qatar', 'qawwali', 'quaid', 'quality', 'quantities', 'quantization', 'queen', 'quetta', 'quick', 'quickly', 'quoted', 'quran', 'quraniyoon', 'ra', 'radial', 'radio', 'ragga', 'rah', 'rahim', 'rahmat', 'rai', 'rail', 'railroad', 'railway', 'railways', 'rainfall', 'rainy', 'raised', 'raj', 'rajasthani', 'ramadan', 'ramazan', 'random', 'randomly', 'range', 'rangers', 'ranges', 'ranging', 'ranked', 'ranking', 'ranks', 'rap', 'raped', 'rapid', 'rapidly', 'rapper', 'rapping', 'rare', 'rate', 'rated', 'rates', 'rather', 'ratings', 'raw', 'rawalpindi', 'raza', 'rbf', 're', 'reach', 'reached', 'reaches', 'reaching', 'reactor', 'reactors', 'readiness', 'real', 'realism', 'reality', 'realm', 'reasons', 'rebel', 'rebellion', 'recalculate', 'receive', 'received', 'recent', 'recently', 'reception', 'recipient', 'reciprocate', 'recognise', 'recognised', 'recognition', 'recommendations', 'recomposed', 'reconstruction', 'record', 'recorded', 'recounting', 'recover', 'recovery', 'recruitment', 'red', 'reduced', 'reducing', 'refer', 'references', 'referencing', 'referred', 'referring', 'refinement', 'reflect', 'reflecting', 'reflects', 'reformer', 'reforms', 'refuge', 'refugee', 'refugees', 'refused', 'regain', 'regarded', 'regarding', 'reggae', 'region', 'regional', 'regions', 'registered', 'regular', 'regularly', 'regulate', 'regulated', 'regulates', 'regulation', 'regulatory', 'rehman', 'reinstated', 'reintegration', 'reissue', 'related', 'relates', 'relation', 'relations', 'relationship', 'relatively', 'relaxation', 'relaxed', 'release', 'released', 'releasing', 'relevance', 'relevant', 'relief', 'religion', 'religions', 'religious', 'relocate', 'relocated', 'relocation', 'relocations', 'remain', 'remained', 'remaining', 'remains', 'reminiscent', 'remittances', 'remove', 'renaissance', 'repair', 'repeated', 'repeatedly', 'repelled', 'replaced', 'replacement', 'replaces', 'replied', 'report', 'reported', 'reportedly', 'reporters', 'reports', 'represent', 'representation', 'representative', 'represented', 'representing', 'reptiles', 'republic', 'repugnant', 'request', 'requested', 'require', 'required', 'requires', 'rescuing', 'research', 'researchers', 'researches', 'reserve', 'reserved', 'reserves', 'reservoir', 'reside', 'residence', 'resides', 'residing', 'resigned', 'resolution', 'resources', 'respected', 'respective', 'respectively', 'respects', 'response', 'responsibility', 'responsible', 'rest', 'restart', 'restarting', 'restoration', 'restricted', 'restricts', 'result', 'resulted', 'resulting', 'results', 'resumed', 'retaining', 'retaliation', 'retirement', 'retreating', 'retributive', 'return', 'returning', 'revealed', 'revenue', 'reverse', 'reviewing', 'revival', 'revivalist', 'revived', 'revolution', 'rice', 'richard', 'rift', 'right', 'rights', 'ringtone', 'riots', 'rise', 'risen', 'rising', 'rival', 'rivalry', 'rivals', 'river', 'riz', 'rizvi', 'rm', 'road', 'roads', 'robust', 'rocchio', 'rock', 'rocket', 'rocketry', 'rodents', 'rohingya', 'rohingyas', 'role', 'roles', 'roll', 'roman', 'roots', 'rose', 'roti', 'rough', 'roughly', 'rounaq', 'routinely', 'royal', 'ruchir', 'rudolph', 'ruins', 'rukn', 'rul8', 'rule', 'ruled', 'rulers', 'ruling', 'rummel', 'run', 'runner', 'running', 'rupees', 'rural', 'rushed', 'russia', 'rutskoy', 'rvert', 's1', 's2', 's_', 'saadat', 'saarc', 'sabotaged', 'sachs', 'sacred', 'saddar', 'sadequain', 'sadiq', 'safe', 'safety', 'sahiwal', 'said', 'saiful', 'saint', 'saints', 'sajid', 'salafi', 'salam', 'sales', 'salimuzzaman', 'same', 'sample', 'samples', 'sampling', 'samsung', 'san', 'sanctions', 'sanctuaries', 'sandy', 'sanghar', 'sara', 'saraiki', 'saraikis', 'sargodha', 'satellite', 'satisfy', 'sattar', 'saudi', 'saw', 'say', 'scala', 'scale', 'scatter', 'schemes', 'scholar', 'scholars', 'school', 'schools', 'science', 'sciences', 'scientific', 'scientist', 'scientists', 'scikit', 'scipy', 'score', 'scoring', 'scriptures', 'scrub', 'scrublands', 'sea', 'seaport', 'seaports', 'search', 'searchand', 'searchlight', 'season', 'seasoning', 'seasons', 'seats', 'secede', 'seceded', 'seceding', 'second', 'secondary', 'seconds', 'secretariat', 'secretary', 'secretive', 'sectarian', 'sections', 'sector', 'sectoral', 'sectors', 'secular', 'secularism', 'secured', 'security', 'see', 'seek', 'seem', 'seen', 'seesaw', 'segmentation', 'segregate', 'sehwan', 'seismicity', 'selection', 'seleucid', 'self', 'semi', 'semidefinite', 'seminar', 'senate', 'senator', 'senior', 'sent', 'sentiments', 'seoul', 'separable', 'separate', 'separated', 'separates', 'sepoy', 'september', 'serbs', 'serious', 'serpent', 'servants', 'serve', 'served', 'serves', 'service', 'services', 'serving', 'set', 'sets', 'settled', 'settlements', 'seven', 'seventh', 'several', 'severance', 'severe', 'sex', 'shabbir', 'shadow', 'shah', 'shahbaz', 'shaheen', 'shahi', 'shahid', 'shahis', 'shahjahan', 'shaikh', 'shaksgam', 'shalimar', 'shall', 'shalwar', 'shanghai', 'shape', 'shaped', 'shapes', 'share', 'shared', 'shares', 'sharia', 'shariah', 'shariat', 'sharif', 'sharma', 'sharply', 'shaykh', 'she', 'sheedis', 'sheep', 'shehbaz', 'sheikhupura', 'sher', 'shia', 'shias', 'shift', 'shifting', 'shina', 'shirts', 'shisham', 'shooky', 'shooting', 'short', 'shortages', 'shortly', 'shot', 'should', 'shoulder', 'show', 'showcases', 'shown', 'shrines', 'shrublands', 'shut', 'shuttle', 'si', 'sialkot', 'siddiqui', 'side', 'sides', 'siege', 'sighted', 'sigma', 'signal', 'signatory', 'signature', 'signed', 'significance', 'significant', 'significantly', 'sikh', 'sikhism', 'sikhs', 'silhouette', 'silk', 'similar', 'similarly', 'simla', 'simple', 'simply', 'since', 'sinchon', 'sind', 'sindh', 'sindhi', 'sindhis', 'singer', 'singers', 'singh', 'single', 'sino', 'sipri', 'sir', 'sirhindi', 'sisson', 'site', 'sites', 'situated', 'situation', 'six', 'sixth', 'sixty', 'size', 'sk', 'skirmishes', 'skool', 'skunk', 'sleepless', 'slightly', 'slow', 'slower', 'slowest', 'slowly', 'slums', 'small', 'smaller', 'smallsetminus', 'smile', 'smooth', 'smoothed', 'snow', 'so', 'soan', 'soanian', 'social', 'socialism', 'socialist', 'society', 'socio', 'soft', 'software', 'sohan', 'soldiers', 'sole', 'solely', 'solo', 'soloist', 'solution', 'solutions', 'solved', 'somalia', 'some', 'someone', 'sometimes', 'song', 'songs', 'songwriter', 'sons', 'soon', 'sophisticated', 'sort', 'sought', 'soul', 'sound', 'soundcloud', 'source', 'sources', 'soured', 'south', 'southern', 'southwest', 'sovereign', 'sovereignty', 'soviet', 'space', 'spaces', 'spanned', 'spanning', 'spans', 'spark', 'sparked', 'sparrows', 'sparse', 'sparsity', 'spatial', 'speaker', 'speakers', 'speaking', 'special', 'species', 'specific', 'specifically', 'specified', 'speed', 'spelled', 'spending', 'spends', 'spheres', 'spherical', 'spherically', 'spices', 'spinning', 'spiritual', 'split', 'splits', 'spoken', 'spokesperson', 'sponsored', 'sport', 'sporting', 'sports', 'spread', 'spring', 'spruce', 'spurred', 'sq', 'sqrt', 'squabbling', 'square', 'squared', 'squares', 'squash', 'sri', 'staff', 'stage', 'stagflation', 'stalled', 'stan', 'standard', 'standing', 'staple', 'star', 'started', 'starting', 'starts', 'state', 'stated', 'statehood', 'statement', 'states', 'stating', 'stations', 'statistics', 'status', 'stay', 'steadily', 'steinhaus', 'step', 'steps', 'still', 'stimulated', 'stone', 'stony', 'stood', 'stops', 'storytelling', 'straightforward', 'strained', 'strains', 'strata', 'strategic', 'strategies', 'strategy', 'streaming', 'strength', 'stretched', 'stretches', 'strikes', 'striped', 'strong', 'strongest', 'stronghold', 'structure', 'structures', 'struggle', 'struggles', 'stuart', 'student', 'students', 'studies', 'studio', 'study', 'style', 'styles', 'stylized', 'subalpine', 'subcontinent', 'subjacent', 'subject', 'suboptimal', 'subordinate', 'subsequent', 'subsidise', 'subspace', 'subspecies', 'substantial', 'substantially', 'subtropical', 'subway', 'succeeded', 'success', 'successful', 'successfully', 'successive', 'such', 'sudden', 'suffer', 'suffered', 'sufficient', 'suffix', 'suffrage', 'sufi', 'sufis', 'sufism', 'suga', 'sugar', 'sugarcane', 'sugarcoating', 'suggested', 'suhail', 'suitable', 'suits', 'sukkur', 'sulaiman', 'suleri', 'sultanate', 'sum', 'summer', 'sunnah', 'sunni', 'sunshine', 'suparco', 'super', 'supercomputing', 'superior', 'supermajority', 'superpolynomial', 'superpower', 'supervised', 'supplied', 'supply', 'support', 'supported', 'supporting', 'supportive', 'supremacy', 'supreme', 'suran', 'surged', 'surgery', 'surgical', 'surrender', 'surrendering', 'surrounding', 'survey', 'surveys', 'survive', 'suspended', 'sustainable', 'suture', 'svd', 'swapping', 'swaps', 'swat', 'sweet', 'switzerland', 'syed', 'syllables', 'symbol', 'syrian', 'system', 'systematically', 'systems', 'tablo', 'taejeon', 'tail', 'tailored', 'tajikistan', 'take', 'taken', 'takht', 'taking', 'takṣaśilā', 'taliban', 'talk', 'talpur', 'tamar', 'tamarisk', 'tamil', 'tando', 'tank', 'tara', 'target', 'task', 'taxicab', 'taxila', 'tea', 'teaching', 'teachings', 'team', 'tear', 'tears', 'technical', 'technique', 'techniques', 'technology', 'tectonic', 'teenage', 'tehrik', 'tehsil', 'tehsils', 'telecom', 'televised', 'television', 'temperate', 'temples', 'ten', 'tend', 'tended', 'tends', 'tension', 'tensions', 'tentative', 'term', 'termed', 'terminates', 'termination', 'terming', 'terms', 'territorial', 'territories', 'territory', 'terrorism', 'tessellation', 'test', 'tested', 'testings', 'tests', 'texas', 'textile', 'textiles', 'textual', 'than', 'thar', 'tharparkar', 'that', 'thatta', 'the', 'theatrical', 'their', 'them', 'themes', 'themselves', 'then', 'theologian', 'theoretical', 'theory', 'therapeutic', 'there', 'therefore', 'these', 'they', 'thin', 'thinking', 'third', 'thirty', 'this', 'those', 'though', 'thousand', 'thousanders', 'thousands', 'threatened', 'threats', 'three', 'threshold', 'thresholded', 'through', 'throughout', 'thunderstorm', 'thus', 'ticketing', 'tide', 'tier', 'ties', 'time', 'times', 'title', 'titled', 'to', 'today', 'together', 'tomb', 'tons', 'took', 'tool', 'tools', 'top', 'topics', 'torch', 'torn', 'total', 'tour', 'touring', 'tourism', 'tourist', 'tourists', 'tournaments', 'towards', 'town', 'towns', 'track', 'tracks', 'tractor', 'trade', 'trading', 'tradition', 'traditional', 'traffic', 'tragopan', 'trail', 'trailers', 'train', 'trained', 'training', 'tramway', 'transcend', 'transfer', 'transform', 'transformation', 'transforming', 'transit', 'transition', 'transliterated', 'transport', 'transportation', 'trapped', 'travel', 'travelled', 'treated', 'treaties', 'treatment', 'treaty', 'tree', 'trees', 'trend', 'trends', 'triangle', 'tribal', 'tributaries', 'trillion', 'tripping', 'trivia', 'troop', 'troops', 'trophy', 'tropical', 'trousers', 'truly', 'trust', 'try', 'tsangpo', 'tube', 'tuberculosis', 'tumours', 'tunisia', 'turk', 'turkey', 'turmeric', 'turmoil', 'turned', 'tv', 'twelve', 'twenty20', 'two', 'type', 'uae', 'ubiquity', 'uc', 'ucb', 'uk', 'ul', 'ulama', 'umayyad', 'umerkot', 'un', 'unable', 'unacceptable', 'unbalanced', 'unconditional', 'under', 'undergoing', 'underground', 'underpass', 'underpasses', 'underset', 'understanding', 'understood', 'undertaken', 'underwent', 'undocumented', 'unemployment', 'union', 'unique', 'unit', 'unite', 'united', 'units', 'unity', 'univariate', 'universal', 'universally', 'universe', 'universities', 'university', 'unlike', 'unnecessary', 'unpacked', 'unparalleled', 'unprecedented', 'unresolved', 'unsup', 'unsupervised', 'until', 'unveiled', 'up', 'update', 'updated', 'updates', 'updating', 'upon', 'upper', 'uprising', 'uranium', 'urban', 'urbanisation', 'urbanised', 'urdu', 'urial', 'us', 'usage', 'use', 'used', 'user', 'users', 'uses', 'ushering', 'using', 'usmani', 'ussr', 'usually', 'uyghur', 'vacate', 'vajpayee', 'validity', 'valley', 'valleys', 'valuable', 'value', 'values', 'vanishing', 'var', 'variable', 'variance', 'variances', 'variant', 'variants', 'variation', 'variations', 'varied', 'varies', 'varieties', 'variety', 'various', 'varphi', 'vary', 'vast', 'vector', 'vectors', 'vedas', 'vedic', 'vegetables', 'vegetative', 'vehicles', 'veneration', 'venture', 'ventures', 'version', 'very', 'vested', 'via', 'viceroy', 'vicinity', 'victims', 'victoria', 'victories', 'victory', 'video', 'videos', 'view', 'views', 'vigorous', 'vigorously', 'village', 'violence', 'violent', 'vip', 'virtue', 'visas', 'visible', 'vision', 'visit', 'visited', 'visualization', 'vital', 'vocational', 'voiced', 'voluntarily', 'voronoi', 'vote', 'votes', 'vulnerabilities', 'waging', 'wakhan', 'wali', 'want', 'wapda', 'war', 'warm', 'warplanes', 'wars', 'warsi', 'was', 'watching', 'water', 'way', 'wazir', 'wcss', 'we', 'wealthiest', 'weapons', 'wear', 'weather', 'weber', 'website', 'week', 'weeks', 'wef', 'weight', 'weighted', 'weights', 'weka', 'welfare', 'well', 'wells', 'went', 'were', 'west', 'western', 'wetlands', 'what', 'wheat', 'when', 'where', 'whereas', 'whether', 'which', 'while', 'whilst', 'whitening', 'who', 'whom', 'whose', 'why', 'wide', 'wikimedia', 'wild', 'wildcats', 'wildfires', 'wildlife', 'will', 'wine', 'wing', 'wings', 'winning', 'winter', 'wished', 'with', 'within', 'without', 'witnessed', 'women', 'won', 'wong', 'word', 'work', 'worked', 'workforce', 'working', 'works', 'world', 'worldwide', 'worn', 'worsened', 'worship', 'worst', 'would', 'wound', 'write', 'writes', 'writings', 'written', 'wrld', 'wrong', 'wrote', 'x1', 'x2', 'x_', 'xeric', 'xinjiang', 'xn', 'yahya', 'yale', 'yar', 'yarn', 'yatra', 'year', 'years', 'yet', 'yield', 'yielding', 'yields', 'yom', 'yoon', 'you', 'young', 'younger', 'youngest', 'yourself', 'yousaf', 'yousafzai', 'youth', 'zafar', 'zaheer', 'zarb', 'zardari', 'zayn', 'zee', 'zenith', 'zero', 'zesh', 'zia', 'ziaul', 'zimbabwean', 'zone', 'zones', 'zoroastrianism', 'zubairy', 'zulfikar', 'état', 'ømi', 'μi', 'ـستان', 'پاک', '가드', '민윤기', '슈가', '슈팅']\n",
      "(6, 4879)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b7828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4910772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                title  cluster\n",
      "1  k-means clustering        0\n",
      "3  k-means clustering        0\n",
      "4  k-means clustering        0\n",
      "5  k-means clustering        0\n",
      "0       Suga (rapper)        1\n",
      "2            Pakistan        2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devops\\AppData\\Local\\Temp/ipykernel_15260/845745049.py:3: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  model.fit(data)\n"
     ]
    }
   ],
   "source": [
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
    "model.fit(data)\n",
    "labels=model.labels_\n",
    "wiki_cl=pd.DataFrame(list(zip(title,labels)),columns=['title','cluster'])\n",
    "print(wiki_cl.sort_values(by=['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f73fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
